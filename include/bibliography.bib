
@article{Kitagawa2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.05579},
  title = {Convergence of a {{Newton}} Algorithm for Semi-Discrete Optimal Transport},
  url = {http://arxiv.org/abs/1603.05579},
  abstract = {Many problems in geometric optics or convex geometry can be recast as optimal transport problems: this includes the far-field reflector problem, Alexandrov's curvature prescription problem, etc. A popular way to solve these problems numerically is to assume that the source probability measure is absolutely continuous while the target measure is finitely supported. We refer to this setting as semi-discrete optimal transport. Among the several algorithms proposed to solve semi-discrete optimal transport problems, one currently needs to choose between algorithms that are slow but come with a convergence speed analysis (e.g. Oliker-Prussner) or algorithms that are much faster in practice but which come with no convergence guarantees Algorithms of the first kind rely on coordinate-wise increments and the number of iterations required to reach the solution up to an error of \$\textbackslash{}epsilon\$ is of order \$N\^3/\textbackslash{}epsilon\$, where \$N\$ is the number of Dirac masses in the target measure. On the other hand, algorithms of the second kind typically rely on the formulation of the semi-discrete optimal transport problem as an unconstrained convex optimization problem which is solved using a Newton or quasi-Newton method. The purpose of this article is to bridge this gap between theory and practice by introducing a damped Newton's algorithm which is experimentally efficient and by proving the global convergence of this algorithm with optimal rates. The main assumptions is that the cost function satisfies a condition that appears in the regularity theory for optimal transport (the Ma-Trudinger-Wang condition) and that the support of the source density is connected in a quantitative way (it must satisfy a weighted Poincar\textbackslash{}'e-Wirtinger inequality).},
  date = {2016},
  pages = {1-45},
  author = {Kitagawa, Jun and Mérigot, Quentin and Thibert, Boris},
  file = {/usr/stud/brechet/Zotero/storage/HX25QGCH/Kitagawa, Mérigot, Thibert - 2016 - Convergence of a Newton algorithm for semi-discrete optimal transport.pdf},
  arxivid = {1603.05579}
}

@article{Potash2016a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1669v3},
  title = {Recommender System Incorporating User Personality Profile through Analysis of Written Reviews},
  volume = {1680},
  issn = {16130073},
  url = {http://arxiv.org/abs/1704.00028},
  doi = {10.1016/j.aqpro.2013.07.003},
  abstract = {The cereal and oilseed trade is a necessary contingency for assuring world food security as we restore ecological health to the planet. In a more populous and resource stressed world, trade's role will grow – guiding producers and consumers in their choices and facilitating optimal use of natural resources. Greater water efficiency will be a fundamental part of this. The right policies, farm entrepreneurship, conscientious use of land and other resources, knowledge and infrastructure also are required.},
  journaltitle = {CEUR Workshop Proceedings},
  date = {2016},
  pages = {60-66},
  keywords = {Collaborative filtering,Human-centered computing,Recommender systems,Social networks},
  author = {Potash, Peter and Rumshisky, Anna},
  isbn = {0030-8870},
  arxivid = {arXiv:1011.1669v3},
  pmid = {24439530}
}

@article{Vaswani2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.03762},
  title = {Attention {{Is All You Need}}},
  issn = {0140-525X},
  url = {http://arxiv.org/abs/1706.03762},
  doi = {10.1017/S0140525X16001837},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  issue = {Nips},
  date = {2017},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  isbn = {9781577357384},
  arxivid = {1706.03762},
  pmid = {1000303116}
}

@article{Chan2011,
  title = {Math 172 : {{Lebesgue Integration}} and {{Fourier Analysis}}},
  date = {2011},
  author = {Chan, Charlotte}
}

@article{Matrices2000,
  title = {Relative {{Entropy}}},
  number = {x},
  date = {2000},
  pages = {13-55},
  author = {Matrices, Weight and Example, Simple Site}
}

@article{Martens2012,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  number = {2010},
  date = {2012},
  author = {Martens, James}
}

@article{B.Ash1990,
  title = {Information Theory‎},
  url = {http://books.google.com/books?id=yZ1JZA6Wo6YC\{&\}printsec=frontcover\{%\}5Cnfile:///Users/Brian\{_\}Caudle/Documents/Papers/1990/B. Ash/1990 B. Ash.pdf\{%\}5Cnpapers://75088281-f09c-4e77-b8c2-14041f4545f6/Paper/p344},
  abstract = {Excellent introduction treats 3 major areas: analysis of channel models and proof of coding theorems; study of specific coding systems; and study of statistical ...},
  date = {1990},
  pages = {339},
  keywords = {Computers},
  author = {B. Ash, R}
}

@article{Carlier,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1512.02783v2},
  title = {Convergence of {{Entropic Schemes}} For},
  pages = {1-28},
  author = {Carlier, Guillaume and Duval, Vincent and Schmitzer, Bernhard},
  arxivid = {arXiv:1512.02783v2}
}

@book{Gray2011,
  title = {Entropy and Information Theory},
  volume = {1},
  isbn = {978-1-4419-7969-8},
  abstract = {This book is devoted to the theory of probabilistic information measures and their application to coding theorems for information sources and noisy channels. The eventual goal is a general development of Shannon's mathematical theory of communication, but much of the space is devoted to the tools and methods required to prove the Shannon coding theorems. These tools form an area common to ergodic theory and information theory and comprise several quantitative notions of the information in random variables, random processes, and dynamical systems. Examples are entropy, mutual information, conditional entropy, conditional information, and discrimination or relative entropy, along with the limiting normalized versions of these quantities such as entropy rate and information rate. Much of the book is concerned with their properties, especially the long term asymptotic behavior of sample information and expected information. This is the only up-to-date treatment of traditional information theory emphasizing ergodic theory.},
  pagetotal = {1-409},
  date = {2011},
  author = {Gray, Robert M.},
  file = {/usr/stud/brechet/Zotero/storage/ALMBBP5C/Robert M. Gray (auth.)-Entropy and Information Theory-Springer US (2011).pdf},
  doi = {10.1007/978-1-4419-7970-4},
  issn = {0308518X},
  booktitle = {Entropy and {{Information Theory}}},
  arxivid = {arXiv:gr-qc/9809069v1},
  pmid = {19695087},
  eprinttype = {arxiv}
}

@article{Genevay2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.01807},
  title = {{{GAN}} and {{VAE}} from an {{Optimal Transport Point}} of {{View}}},
  url = {http://arxiv.org/abs/1706.01807},
  abstract = {This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).},
  date = {2017},
  pages = {1-6},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/usr/stud/brechet/Zotero/storage/E3EPRY6A/Genevay, Peyré, Cuturi - 2017 - GAN and VAE from an Optimal Transport Point of View.pdf},
  arxivid = {1706.01807}
}

@article{Mescheder2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1801.04406},
  title = {Which {{Training Methods}} for {{GANs}} Do Actually {{Converge}}?},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1801.04406},
  abstract = {Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
  date = {2018},
  author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  isbn = {1938-7228},
  arxivid = {1801.04406}
}

@article{Edition2018,
  title = {Inequalities In},
  issue = {March},
  date = {2018},
  pages = {657-687},
  author = {Edition, Second}
}

@article{Azim2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1669v3},
  title = {Abiotic Stresses Induce Total Phenolic, Total Flavonoid and Antioxidant Properties in {{Malaysian}} Indigenous Microalgae and Cyanobacterium},
  volume = {14},
  issn = {18238262},
  doi = {10.1017/CBO9781107415324.004},
  abstract = {This study investigated the ability of bacteriocins isolated from Bacillus spp. (Bacillus species) to inhibit four different yeast isolates obtained from common food products (nono, yoghurt, ogi and cheese) commonly consumed by Nigerians with minimal heat treatment. Forty-five Bacillus spp. was isolated and identified from common food products using cultural, morphological, physiological and biochemical characteristics. These isolates were tested for antimicrobial activity against Salmonella enteritidis (3), Micrococcus luteus (1) and Staphylococcus aureus (2). Eight bacteriocin producing strains were identified from an over- night broth culture centrifugated at 3500 revolutions for five minutes. Fungicidal effects of these bacteriocins were tested against four yeast strains using the Agar Well Diffusion method. The bacteriocins produced wide zones of inhibition ranging from 5.9±0.000 to 24.00±0.000 mm against the 4 yeast strains tested. There was a significant difference (at p0.05) between the yeast organisms and the bacteriocins from the Bacillus spp. The study reveals the antifungal property of bacteriocins from Bacillus spp. and serves therefore as a base for further studies in its use in the control of diseases and extension of shelf-life of products prone to fungi contamination.},
  number = {1},
  journaltitle = {Malaysian Journal of Microbiology},
  date = {2018},
  pages = {25-33},
  keywords = {Antioxidant,Cyanobacteria,Microalgae,Stress},
  author = {Azim, Nur Husna and Subki, Atiqah and Yusof, Zetty Norhana Balia},
  isbn = {9788578110796},
  arxivid = {arXiv:1011.1669v3},
  pmid = {25246403}
}

@article{Courty2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08848},
  title = {Joint {{Distribution Optimal Transportation}} for {{Domain Adaptation}}},
  issn = {0162-8828},
  url = {http://arxiv.org/abs/1705.08848},
  doi = {10.1109/TPAMI.2016.2615921},
  abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \$\textbackslash{}mathcal\{P\}\_s\$ and \$\textbackslash{}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \$\textbackslash{}mathcal\{P\}\^f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
  issue = {Nips},
  date = {2017},
  author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
  isbn = {9782875870148},
  arxivid = {1705.08848}
}

@article{Konur2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980v9},
  title = {The Scientometric Evaluation of the Institutional Research: {{The Marmara Universities}}-{{Part}} 4},
  volume = {5},
  issn = {13087711},
  doi = {10.1063/1.4902458},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  number = {2},
  journaltitle = {Energy Education Science and Technology Part B: Social and Educational Studies},
  date = {2013},
  pages = {365-380},
  keywords = {AandHCI,Higher education spending,Incentive structures,Marmara Universities,Research evaluation,Research productivity,SCIE,Scientometrics,SSCI,Turkey,Web of Knowledge},
  author = {Konur, Ozcan},
  isbn = {9780735412705},
  arxivid = {1412.6980v9}
}

@article{Larochelle2011,
  title = {The {{Neural Autoregressive Distribution Estimator}}},
  volume = {15},
  date = {2011},
  pages = {29-37},
  author = {Larochelle, Hugo and Murray, Iain}
}

@article{Rubner2000b,
  title = {Earth Mover's Distance as a Metric for Image Retrieval},
  volume = {40},
  issn = {09205691},
  url = {http://www.springerlink.com/index/W5515K817681125H.pdf},
  doi = {10.1023/A:1026543900054},
  abstract = {We investigate the properties of a metric between two distributions, the Earth Mover's Distance (EMD), for content-based image retrieval. The EMD is based on the minimal cost that must be paid to transform one dis-tribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The EMD is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length represen-tations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the EMD is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the EMD with that of other distances.},
  number = {2},
  journaltitle = {International Journal of Computer Vision},
  date = {2000},
  pages = {99-121},
  keywords = {color,earth mover,image retrieval,perceptual metrics,s distance,texture},
  author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
  isbn = {0920-5691},
  arxivid = {arXiv:astro-ph/0005074v1},
  pmid = {1284},
  eprinttype = {arxiv}
}

@article{Montavon2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.01972},
  title = {Wasserstein {{Training}} of {{Boltzmann Machines}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1507.01972},
  abstract = {The Boltzmann machine provides a useful framework to learn highly complex, multimodal and multiscale data distributions that occur in the real world. The default method to learn its parameters consists of minimizing the Kullback-Leibler (KL) divergence from training samples to the Boltzmann model. We propose in this work a novel approach for Boltzmann training which assumes that a meaningful metric between observations is given. This metric can be represented by the Wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters. Minimization of this new Wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure. We demonstrate the practical potential of these models for data completion and denoising, for which the metric between observations plays a crucial role.},
  number = {1},
  date = {2015},
  pages = {1-9},
  author = {Montavon, Grégoire and Müller, Klaus-Robert and Cuturi, Marco},
  arxivid = {1507.01972}
}

@article{Bertsimas1997,
  title = {Introduction to Linear Optimization},
  url = {http://scholar.google.com/scholar?hl=en\{&\}btnG=Search\{&\}q=intitle:Introduction+to+Linear+Optimization\{#\}0},
  date = {1997},
  author = {Bertsimas, D and Tsitsiklis, J N},
  booktitle = {Imamu.{{Edu}}.{{Sa}}}
}

@article{Aude2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.08527},
  title = {Stochastic {{Optimization}} for {{Large}}-Scale {{Optimal Transport}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1605.08527},
  abstract = {Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal OT problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.},
  issue = {Nips},
  journaltitle = {Nips},
  date = {2016},
  pages = {1-12},
  author = {Genevay, Aude and Cuturi, Marco and Peyré, Gabriel and Bach, Francis},
  file = {/usr/stud/brechet/Zotero/storage/W49LBC5I/Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport.pdf},
  arxivid = {1605.08527}
}

@article{Loshchilov2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1608.03983},
  title = {{{SGDR}}: {{Stochastic Gradient Descent}} with {{Warm Restarts}}},
  issn = {15826163},
  url = {http://arxiv.org/abs/1608.03983},
  doi = {10.1002/fut},
  abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
  date = {2016},
  pages = {1-16},
  author = {Loshchilov, Ilya and Hutter, Frank},
  isbn = {978-0-674-02343-7},
  arxivid = {1608.03983},
  pmid = {87370679}
}

@article{Lim2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.02894},
  title = {Geometric {{GAN}}},
  url = {http://arxiv.org/abs/1705.02894},
  abstract = {Generative Adversarial Nets (GANs) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in GAN and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric GAN using SVM separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric GAN converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric GAN.},
  issue = {Mmd},
  date = {2017},
  pages = {1-17},
  author = {Lim, Jae Hyun and Ye, Jong Chul},
  arxivid = {1705.02894}
}

@article{Sjolund2013,
  title = {Gaussian Channel},
  issue = {May},
  date = {2013},
  pages = {1-26},
  author = {Sjölund, Jens}
}

@book{Edition2006,
  title = {Kolmogorov Complexity},
  isbn = {0-01-001111-0},
  pagetotal = {463-508},
  date = {2006},
  author = {Edition, Second}
}

@article{Hardt2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.01240v2},
  title = {Stability of Stochastic Gradient Descent},
  date = {2016},
  pages = {1-32},
  author = {Hardt, Moritz and Recht, Benjamin},
  arxivid = {arXiv:1509.01240v2}
}

@article{Capacity2012,
  title = {Channel {{Capacity}}},
  issue = {November},
  date = {2012},
  pages = {1-132},
  author = {Capacity, Channel},
  isbn = {9780470825617}
}

@book{Kurdila2005,
  location = {{Basel ; Boston}},
  title = {Convex Functional Analysis},
  isbn = {978-0-8176-2198-8 978-3-7643-2198-7},
  pagetotal = {228},
  series = {Systems \& Control},
  publisher = {{Birkhauser Verlag}},
  date = {2005},
  keywords = {Convex functions,Existence theorems,Functional analysis,Mathematical optimization},
  author = {Kurdila, Andrew and Zabarankin, Michael}
}

@article{Goodfellow2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1406.2661},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Generative {{Adversarial Networks}}},
  url = {http://arxiv.org/abs/1406.2661},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  urldate = {2019-01-08},
  date = {2014-06-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  file = {/usr/stud/brechet/Zotero/storage/UH5DPN9T/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf}
}

@article{Mescheder2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.04722},
  primaryClass = {cs},
  langid = {english},
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1701.04722},
  shorttitle = {Adversarial {{Variational Bayes}}},
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihoodproblem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justiﬁcation, retains most advantages of standard Variational Autoencoders and is easy to implement.},
  urldate = {2019-01-08},
  date = {2017-01-17},
  keywords = {Computer Science - Machine Learning},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  file = {/usr/stud/brechet/Zotero/storage/TW9X9W9H/Mescheder et al. - 2017 - Adversarial Variational Bayes Unifying Variationa.pdf}
}

@article{Bell2015a,
  title = {Weak Symplectic Forms and Differential Calculus in {{Banach}} Spaces},
  date = {2015},
  pages = {1-15},
  author = {Bell, Jordan}
}

@article{Carlier2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1411.3602},
  title = {Numerical Methods for Matching for Teams and {{Wasserstein}} Barycenters},
  issn = {12903841},
  url = {http://arxiv.org/abs/1411.3602},
  doi = {10.1051/m2an/2015033},
  abstract = {Equilibrium multi-population matching (matching for teams) is a problem from mathematical economics which is related to multi-marginal optimal transport. A special but important case is the Wasserstein barycenter problem, which has applications in image processing and statistics. Two algorithms are presented: a linear programming algorithm and an efficient nonsmooth optimization algorithm, which applies in the case of the Wasserstein barycenters. The measures are approximated by discrete measures: convergence of the approximation is proved. Numerical results are presented which illustrate the efficiency of the algorithms.},
  date = {2014},
  pages = {1-29},
  keywords = {convex minimization,duality,linear,matching for teams,numerical methods for nonsmooth,programming,wasserstein barycenters},
  author = {Carlier, Guillaume and Oberman, Adam and Oudet, Edouard},
  file = {/usr/stud/brechet/Zotero/storage/5NBDNJ3Q/carlieretal2015.pdf},
  arxivid = {1411.3602}
}

@article{Berger2003,
  title = {Rate-{{Distortion Theory}}},
  url = {http://doi.wiley.com/10.1002/0471219282.eot142},
  doi = {10.1002/0471219282.eot142},
  journaltitle = {Wiley Encyclopedia of Telecommunications},
  date = {2003},
  pages = {301-346},
  author = {Berger, Toby},
  isbn = {9780471219286}
}

@article{Schmitzer2017,
  title = {Optimal {{Transport}} for {{Data Analysis}}},
  volume = {1},
  date = {2017},
  pages = {1-38},
  author = {Schmitzer, Bernhard}
}

@incollection{Montavon2015a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1507.01972},
  title = {Wasserstein {{Training}} of {{Boltzmann Machines}}},
  url = {http://arxiv.org/abs/1507.01972},
  abstract = {The Boltzmann machine provides a useful framework to learn highly complex, multimodal and multiscale data distributions that occur in the real world. The default method to learn its parameters consists of minimizing the Kullback-Leibler (KL) divergence from training samples to the Boltzmann model. We propose in this work a novel approach for Boltzmann training which assumes that a meaningful metric between observations is given. This metric can be represented by the Wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters. Minimization of this new Wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure. We demonstrate the practical potential of these models for data completion and denoising, for which the metric between observations plays a crucial role.},
  number = {1},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  date = {2015},
  pages = {3718-3726},
  author = {Montavon, Grégoire and Müller, Klaus-Robert and Cuturi, Marco},
  file = {/usr/stud/brechet/Zotero/storage/GAX3R9S5/Montavon, Cuturi, Paris-saclay - 2016 - Wasserstein Training of Restricted Boltzmann Machines.pdf},
  issn = {10495258},
  arxivid = {1507.01972}
}

@article{Xu2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.03044v3},
  title = {Show , {{Attend}} and {{Tell}} : {{Neural Image Caption Generation}} with {{Visual Attention}}},
  date = {2014},
  author = {Xu, Kelvin and Courville, Aaron and Zemel, Richard S and Bengio, Yoshua},
  arxivid = {arXiv:1502.03044v3}
}

@book{Peypouquet2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1669v3},
  title = {Convex {{Optimization}} in {{Normed Spaces}}},
  isbn = {978-3-319-13709-4},
  url = {http://link.springer.com/10.1007/978-3-319-13710-0},
  abstract = {This work is intended to serve as a guide for graduate students and researchers who wish to get acquainted with the main theoretical and practical tools for the numerical minimization of convex functions on Hilbert spaces. Therefore, it contains the main tools that are necessary to conduct independent research on the topic. It is also a concise, easy-to-follow and self-contained textbook, which may be useful for any researcher working on related fields, as well as teachers giving graduate-level courses on the topic. It will contain a thorough revision of the extant literature including both classical and state-of-the-art references.},
  date = {2015},
  author = {Peypouquet, Juan},
  doi = {10.1007/978-3-319-13710-0},
  issn = {1098-6596},
  arxivid = {arXiv:1011.1669v3},
  pmid = {25246403}
}

@article{Muandet2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09522},
  title = {Kernel {{Mean Embedding}} of {{Distributions}}: {{A Review}} and {{Beyond}}},
  issn = {1935-8237},
  url = {http://arxiv.org/abs/1605.09522\{%\}0Ahttp://dx.doi.org/10.1561/2200000060},
  doi = {10.1561/2200000060},
  abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
  date = {2016},
  author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
  file = {/usr/stud/brechet/Zotero/storage/45XLTGH2/muandet2017trim.pdf;/usr/stud/brechet/Zotero/storage/9G3NMF6X/muandet2017.pdf;/usr/stud/brechet/Zotero/storage/JQLBFBVF/muandet2017trim2.pdf},
  isbn = {9781680832884},
  arxivid = {1605.09522}
}

@article{ElGamal2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1001.3404},
  title = {Network {{Information Theory}}},
  url = {http://ebooks.cambridge.org/ref/id/CBO9781139030687},
  doi = {10.1017/CBO9781139030687},
  abstract = {This comprehensive treatment of network information theory and its applications pro-vides the first unified coverage of both classical and recent results. With an approach that balances the introduction of new models and new coding techniques, readers are guided through Shannon's point-to-point information theory, single-hop networks, multihop networks, and extensions to distributed computing, secrecy, wireless communication, and networking. Elementary mathematical tools and techniques are used throughout, requiring only basic knowledge of probability, whilst unified proofs of coding theorems are based on a few simple lemmas, making the text accessible to newcomers. Key topics covered include successive cancellation and superposition coding, MIMO wireless com-munication, network coding, and cooperative relaying. Also covered are feedback and interactive communication, capacity approximations and scaling laws, and asynchronous and random access channels. This book is ideal for use in the classroom, for self-study, and as a reference for researchers and engineers in industry and academia. In the field of network information theory, he is best known for his seminal contributions to the relay, broadcast, and interference chan-nels; multiple description coding; coding for noisy networks; and energy-efficient packet scheduling and throughput–delay tradeoffs in wireless networks. He is a Fellow of IEEE and the winner of the 2012 Claude E. Shannon Award, the highest honor in the field of information theory.},
  date = {2011},
  pages = {509-611},
  author = {El Gamal, Abbas and Kim, Young-Han},
  isbn = {9781139030687},
  arxivid = {1001.3404}
}

@article{Seguy2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.02283},
  title = {Large-{{Scale Optimal Transport}} and {{Mapping Estimation}}},
  url = {http://arxiv.org/abs/1711.02283},
  abstract = {This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport (OT) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized OT, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a \textbackslash{}textit\{Monge map\} as a deep neural network learned by approximating the barycentric projection of the previously-obtained OT plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized OT which show that our estimations converge to the OT plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.},
  number = {1781},
  date = {2017},
  pages = {1-15},
  author = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, Rémi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
  arxivid = {1711.02283}
}

@article{Potash2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1011.1669v3},
  title = {Recommender System Incorporating User Personality Profile through Analysis of Written Reviews},
  volume = {1680},
  issn = {16130073},
  url = {http://arxiv.org/abs/1701.04722},
  doi = {10.1016/j.aqpro.2013.07.003},
  abstract = {The cereal and oilseed trade is a necessary contingency for assuring world food security as we restore ecological health to the planet. In a more populous and resource stressed world, trade's role will grow – guiding producers and consumers in their choices and facilitating optimal use of natural resources. Greater water efficiency will be a fundamental part of this. The right policies, farm entrepreneurship, conscientious use of land and other resources, knowledge and infrastructure also are required.},
  journaltitle = {CEUR Workshop Proceedings},
  date = {2016},
  pages = {60-66},
  keywords = {Collaborative filtering,Human-centered computing,Recommender systems,Social networks},
  author = {Potash, Peter and Rumshisky, Anna},
  file = {/usr/stud/brechet/Zotero/storage/3XP8K4MR/Mescheder et al. - 2017 - Unifying Variational Autoencoders and Generative Adversarial Networks.pdf},
  isbn = {0030-8870},
  arxivid = {arXiv:1011.1669v3},
  pmid = {24439530}
}

@article{Ramdas2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1509.02237},
  title = {On Wasserstein Two-Sample Testing and Related Families of Nonparametric Tests},
  volume = {19},
  issn = {10994300},
  doi = {10.3390/e19020047},
  abstract = {Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, PP/QQ plots and ROC/ODC curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a \textbackslash{}textit\{smoothed\} Wasserstein distance, and others through the pursuit of a "distribution-free" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others.},
  number = {2},
  journaltitle = {Entropy},
  date = {2017},
  pages = {1-18},
  keywords = {Energy distance,Entropic smoothing,Maximum mean discrepancy,QQ and PP plots,ROC and ODC curves,Two-sample testing,Wasserstein distance},
  author = {Ramdas, Aaditya and Trillos, Nicolás Garćia and Cuturi, Marco},
  isbn = {978-0-471-86725-8},
  arxivid = {1509.02237}
}

@report{Cohen2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.01507},
  title = {{{ON THE CONVERGENCE OF ADAM AND BEYOND}}},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  date = {2018},
  pages = {1-23},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  doi = {10.1134/S0001434607010294},
  issn = {08695652},
  isbn = {9781538610329},
  arxivid = {1709.01507},
  pmid = {23766329}
}

@incollection{Monge1781,
  title = {Mémoire Sur La Théorie Des Déblais et de Remblais},
  isbn = {0959-3780},
  abstract = {Rural and regional hinterlands provide the ecosystem service needs for increasingly urbanised communities across the globe. These inter-related ecosystem services provide key opportunities in securing climate change mitigation and adaptation. Their integrated management in the face of climate change, however, can be confounded by fragmentation within the complex institutional arrangements concerned with natural resource management. This suggests the need for a more systemic approach to continuous improvement in the integrated and adaptive governance of natural resources. This paper explores the theoretical foundations for integrated natural resource management and reviews positive systemic improvements that have been emerging in the Australian context. In setting clear theoretical foundations, the paper explores both functional and structural aspects of natural resource governance systems. Functional considerations include issues of connectivity, knowledge use and capacity within the natural resource decision making environment. Structural considerations refer to the institutions and processes that undertake planning through to implementation, monitoring and evaluation. From this foundation, we review the last decade of emerging initiatives in governance regarding the integration of agriculture and forests across the entire Australian landscape. This includes the shift towards more devolved regional approaches to integrated natural resource management and recent progress towards the use of terrestrial carbon at landscape scale to assist in climate change mitigation and adaptation. These developments, however, have also been tempered by a significant raft of new landscape-scale regulations that have tended to be based on a more centralist philosophy that landowners should be providing ecosystem services for the wider public good without substantive reward. Given this background, we explore a case study of efforts taken to integrate the management of landscape-scale agro-ecological services in the Wet Tropics of tropical Queensland. This is being achieved primarily through the integration of regional natural resource management planning and the development of aggregated terrestrial carbon offset products at a whole of landscape scale via the Degree Celsius initiative. Finally, the paper teases out the barriers and opportunities being experienced, leading to discussion about the global implications for managing climate change, income generation and poverty reduction.},
  booktitle = {Histoire de l'{{Académie Royale}} Des {{Sciences}} de {{Paris}}, Avec Les {{Mémoires}} de {{Mathématique}} et de {{Physique}} Pour La Même Année},
  date = {1781},
  author = {Monge, Gaspard},
  doi = {http://dx.doi.org/10.1016/j.gloenvcha.2013.10.003}
}

@book{Pochet2006,
  title = {Production {{Planning}} by {{Mixed Integer Programming}}},
  isbn = {978-0-387-29959-4},
  url = {http://link.springer.com/10.1007/0-387-33477-7},
  abstract = {Annotation},
  pagetotal = {524},
  date = {2006},
  author = {Pochet, Yves and a. Wolsey, Laurence},
  file = {/usr/stud/brechet/Zotero/storage/CREGTDTD/Pochet, Wolsey - 2006 - Springer Series in Operations Research and Financial Engineering.pdf},
  doi = {10.1007/0-387-33477-7}
}

@article{Rolet2016,
  title = {Fast {{Dictionary Learning}} with a {{Smoothed Wasserstein Loss}}},
  volume = {41},
  issn = {1938-7228},
  url = {http://proceedings.mlr.press/v51/rolet16.pdf},
  abstract = {We consider in this paper the dictionary learning problem when the observations are normalized histograms of features. This problem can be tackled using non-negative matrix factorization approaches, using typi-cally Euclidean or Kullback-Leibler fitting er-rors. Because these fitting errors are separa-ble and treat each feature on equal footing, they are blind to any similarity the features may share. We assume in this work that we have prior knowledge on these features. To leverage this side-information, we propose to use the Wasserstein (a.k.a. earth mover's or optimal transport) distance as the fitting er-ror between each original point and its re-construction, and we propose scalable algo-rithms to to so. Our methods build upon Fenchel duality and entropic regularization of Wasserstein distances, which improves not only speed but also computational stability. We apply these techniques on face images and text documents. We show in particular that we can learn dictionaries (topics) for bag-of-word representations of texts using words that may not have appeared in the original texts, or even words that come from a differ-ent language than that used in the texts.},
  number = {3},
  journaltitle = {International Conference on Artificial Intelligence and Statistics},
  date = {2016},
  pages = {1-9},
  author = {Rolet, Antoine and Cuturi, Marco and Peyré, Gabriel}
}

@article{Cuturi2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1310.4375},
  title = {Fast {{Computation}} of {{Wasserstein Barycenters}}},
  volume = {32},
  url = {http://arxiv.org/abs/1310.4375},
  abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
  number = {c},
  date = {2013},
  keywords = {Ma,optimal transportation,Wasserstein barycenter},
  author = {Cuturi, Marco and Doucet, Arnaud},
  isbn = {9781634393973},
  arxivid = {1310.4375}
}

@article{Duchi2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1103.4296v1},
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  volume = {12},
  issn = {15324435},
  url = {http://portal.acm.org/ft\{_\}gateway.cfm?id=2021068\{&\}ftid=1013134\{&\}coll=DL\{&\}dl=GUIDE\{&\}CFID=346380592\{&\}CFTOKEN=89823863\{%\}0Ahttp://portal.acm.org/citation.cfm?id=1953048.2021068\{&\}coll=DL\{&\}dl=GUIDE\{&\}CFID=346380592\{&\}CFTOKEN=89823863\{%\}0Ahttp://dl.acm.org/citation.cfm?id=2021},
  doi = {10.1109/CDC.2012.6426698},
  abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
  journaltitle = {The Journal of Machine Learning Research},
  date = {2011},
  pages = {2121-2159},
  keywords = {adaptivity,online learning,stochastic convex opti-,subgradient methods},
  author = {Duchi, J and Hazan, E and Singer, Y},
  isbn = {9780982252925},
  arxivid = {arXiv:1103.4296v1},
  pmid = {2868127}
}

@article{Wilson2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08292v2},
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning arXiv}} : 1705 . 08292v2 [ Stat . {{ML}} ] 22 {{May}} 2018},
  issue = {Nips},
  date = {2017},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
  arxivid = {arXiv:1705.08292v2}
}

@article{Bell2015,
  title = {Gradients and {{Hessians}} in {{Hilbert}} Spaces},
  number = {1},
  date = {2015},
  pages = {1-11},
  author = {Bell, Jordan}
}

@article{Makhzani2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.05644v2},
  title = {Adversarial {{Autoencoders}}},
  date = {2014},
  author = {Makhzani, Alireza and Frey, Brendan and Goodfellow, Ian},
  arxivid = {arXiv:1511.05644v2}
}

@article{Race2006,
  title = {Gambling and {{Data}}},
  date = {2006},
  pages = {159-182},
  author = {Race, T H E Horse}
}

@article{Genevay2017a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.00292},
  title = {Learning {{Generative Models}} with {{Sinkhorn Divergences}}},
  issn = {1938-7228},
  url = {http://arxiv.org/abs/1706.00292},
  abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using OT raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating OT losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original OT loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the OT loss with streamlined GPU execution. Entropic smoothing generates a family of losses interpolating between Wasserstein (OT) and Maximum Mean Discrepancy (MMD), thus allowing to find a sweet spot leveraging the geometry of OT and the favorable high-dimensional sample complexity of MMD which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
  date = {2017},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  file = {/usr/stud/brechet/Zotero/storage/XHE66C2I/Genevay, Peyré, Cuturi - 2018 - Learning Generative Models with Sinkhorn Divergences.pdf},
  arxivid = {1706.00292}
}

@article{Feydy2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.08278},
  title = {Interpolating between {{Optimal Transport}} and {{MMD}} Using {{Sinkhorn Divergences}}},
  url = {http://arxiv.org/abs/1810.08278},
  abstract = {Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies (MMD) and Optimal Transport distances (OT) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law. This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between MMD and OT. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the GPU, gradients of the Sinkhorn loss can be computed for batches of a million samples.},
  number = {1},
  date = {2018},
  author = {Feydy, Jean and Séjourné, Thibault and Vialard, François-Xavier and Amari, Shun-ichi and Trouvé, Alain and Peyré, Gabriel},
  file = {/usr/stud/brechet/Zotero/storage/9PUNRXCM/Feydy et al. - 2018 - Interpolating between Optimal Transport and MMD us.pdf},
  isbn = {1810.08278v1},
  arxivid = {1810.08278}
}

@article{Karlsen2006,
  title = {Notes on Weak Convergence},
  date = {2006},
  pages = {1-14},
  author = {Karlsen, Kenneth H},
  file = {/usr/stud/brechet/Zotero/storage/5C3S3UIW/Weakconvergence.pdf}
}

@article{Cover2006,
  title = {Entropy {{Rates}}},
  number = {X},
  journaltitle = {Elements of Information Theory2},
  date = {2006},
  pages = {71-101},
  author = {Cover, Thomas M. and Thomas, Joy A.}
}

@article{Goodfellow,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1805.08318v1},
  title = {Self-{{Attention Generative Adversarial Networks}}},
  author = {Goodfellow, Ian and Odena, Augustus},
  arxivid = {arXiv:1805.08318v1}
}

@article{Rockafellar1967,
  title = {Duality and {{Stability}} in {{Extremum Problems}}},
  volume = {21},
  number = {1},
  date = {1967},
  author = {Rockafellar, R Tyrrell}
}

@article{Defazio2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1407.0202},
  title = {{{SAGA}}: {{A Fast Incremental Gradient Method With Support}} for {{Non}}-{{Strongly Convex Composite Objectives}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1407.0202},
  doi = {10.1080/0958315021000054359},
  abstract = {In this work we introduce a new optimisation method called SAGA in the spirit of SAG, SDCA, MISO and SVRG, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. SAGA improves on the theory behind SAG and SVRG, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike SDCA, SAGA supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
  date = {2014},
  pages = {1-9},
  author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
  file = {/usr/stud/brechet/Zotero/storage/BLIJUEZ3/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf},
  isbn = {0631191666},
  arxivid = {1407.0202}
}

@article{Sanjabi2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.08249},
  title = {On the {{Convergence}} and {{Robustness}} of {{Training GANs}} with {{Regularized Optimal Transport}}},
  url = {http://arxiv.org/abs/1802.08249},
  abstract = {Generative Adversarial Networks (GANs) are one of the most practical methods for learning data distributions. A popular GAN formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein GAN formulation, which is based on regularized Optimal Transport (OT), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of GAN optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning MNIST digits as well as CIFAR-10images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.},
  date = {2018},
  author = {Sanjabi, Maziar and Ba, Jimmy and Razaviyayn, Meisam and Lee, Jason D.},
  file = {/usr/stud/brechet/Zotero/storage/8NXWJUIC/Sanjabi et al. - 2018 - On the Convergence and Robustness of Training GANs.pdf},
  arxivid = {1802.08249}
}

@article{Lindstrom2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.01783},
  title = {Racial {{Bias Shapes Social Reinforcement Learning}}},
  volume = {25},
  issn = {14679280},
  url = {http://arxiv.org/abs/1602.01783},
  doi = {10.1177/0956797613514093},
  abstract = {Both emotional facial expressions and markers of racial-group belonging are ubiquitous signals in social interaction, but little is known about how these signals together affect future behavior through learning. To address this issue, we investigated how emotional (threatening or friendly) in-group and out-group faces reinforced behavior in a reinforcement-learning task. We asked whether reinforcement learning would be modulated by intergroup attitudes (i.e., racial bias). The results showed that individual differences in racial bias critically modulated reinforcement learning. As predicted, racial bias was associated with more efficiently learned avoidance of threatening out-group individuals. We used computational modeling analysis to quantitatively delimit the underlying processes affected by social reinforcement. These analyses showed that racial bias modulates the rate at which exposure to threatening out-group individuals is transformed into future avoidance behavior. In concert, these results shed new light on the learning processes underlying social interaction with racial-in-group and out-group individuals.},
  number = {3},
  journaltitle = {Psychological Science},
  date = {2014},
  pages = {711-719},
  keywords = {emotions,learning,racial and ethnic attitudes and relations,social influences},
  author = {Lindström, Björn and Selbing, Ida and Molapour, Tanaz and Olsson, Andreas},
  isbn = {0956-7976},
  arxivid = {1602.01783},
  pmid = {24458270}
}

@article{Schmitzer2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.06519},
  title = {Stabilized {{Sparse Scaling Algorithms}} for {{Entropy Regularized Transport Problems}}},
  url = {http://arxiv.org/abs/1610.06519},
  abstract = {Scaling algorithms for entropic transport-type problems have become a very popular numerical method, encompassing Wasserstein barycenters, multi-marginal problems, gradient flows and unbalanced transport. However, a standard implementation of the scaling algorithm has several numerical limitations: the scaling factors diverge and convergence becomes impractically slow as the entropy regularization approaches zero. Moreover, handling the dense kernel matrix becomes unfeasible for large problems. To address this, we propose several modifications: A log-domain stabilized formulation, the well-known epsilon-scaling heuristic, an adaptive truncation of the kernel and a coarse-to-fine scheme. This allows to solve larger problems with smaller regularization and negligible truncation error. A new convergence analysis of the Sinkhorn algorithm is developed, working towards a better understanding of epsilon-scaling. Numerical examples illustrate efficiency and versatility of the modified algorithm.},
  date = {2016},
  pages = {1-49},
  author = {Schmitzer, Bernhard},
  arxivid = {1610.06519}
}

@article{Samal2018,
  title = {Introduction and Preview},
  issn = {21915318},
  url = {http://books.google.com.hk/books?id=VWq5GG6ycxMC\{&\}printsec=frontcover\{&\}dq=Elements+of+Information+Theory\{&\}hl=\{&\}cd=1\{&\}source=gbs\{_\}api\{%\}5Cnpapers3://publication/uuid/E1320BD9-B443-434F-8D83-8198955806BD},
  doi = {10.1007/978-3-319-70733-4_1},
  abstract = {© Springer-Verlag Berlin Heidelberg 2006. The classical state-space model for a proper system, corresponding to a set of ordinary di.erential equations (ODEs), can be obtained by selecting a minimal set of variables (known as state variables). However, there are practical situations where physical variables (referred to as descriptor variables) cannot be chosen as state variables in a natural way to provide a mathematical model in a state-space form. Usually, descriptor variables chosen naturally are not minimal in the sense that they may be related algebraically. This may result in a singular system model since some of the relationships of these variables are dynamic while others purely static.},
  number = {9783319707327},
  journaltitle = {SpringerBriefs in Applied Sciences and Technology},
  date = {2018},
  pages = {1-18},
  author = {Samal, Sneha},
  isbn = {9780198275435}
}

@article{Sriperumbudur2009,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0901.2698},
  title = {On Integral Probability Metrics, \textbackslash{}phi-Divergences and Binary Classification},
  url = {http://arxiv.org/abs/0901.2698},
  abstract = {A class of distance measures on probabilities -- the integral probability metrics (IPMs) -- is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. IPMs have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures defined on a metric space. Practical applications of IPMs are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of IPMs, which should contribute to making IPMs more widely used in practice, for instance in areas where \$\textbackslash{}phi\$-divergences are currently popular. First, to understand the relation between IPMs and \$\textbackslash{}phi\$-divergences, the necessary and sufficient conditions under which these classes intersect are derived: the total variation distance is shown to be the only non-trivial \$\textbackslash{}phi\$-divergence that is also an IPM. This shows that IPMs are essentially different from \$\textbackslash{}phi\$-divergences. Second, empirical estimates of several IPMs from finite i.i.d. samples are obtained, and their consistency and convergence rates are analyzed. These estimators are shown to be easily computable, with better rates of convergence than estimators of \$\textbackslash{}phi\$-divergences. Third, a novel interpretation is provided for IPMs by relating them to binary classification, where it is shown that the IPM between class-conditional distributions is the negative of the optimal risk associated with a binary classifier. In addition, the smoothness of an appropriate binary classifier is proved to be inversely related to the distance between the class-conditional distributions, measured in terms of an IPM.},
  number = {1},
  date = {2009},
  pages = {1-18},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
  file = {/usr/stud/brechet/Zotero/storage/ERF8PXHV/sriperumbudur2009.pdf},
  arxivid = {0901.2698}
}

@article{Sriperumbudur2012,
  title = {On the Empirical Estimation of Integral Probability Metrics},
  volume = {6},
  issn = {19357524},
  doi = {10.1214/12-EJS722},
  abstract = {In this paper, we develop and analyze a nonparametric method for estimating the class of integral probability metrics (IPMs), examples of which include the Wasserstein distance, Dudley metric, and maximum mean discrepancy (MMD). We show that these distances can be estimated efficiently by solving a linear program in the case of Wasserstein distance and Dudley metric, while MMD is computable in a closed form. All these estimators are shown to be strongly consistent and their convergence rates are analyzed. Based on these results, we show that IPMs are simple to estimate and the estimators exhibit good convergence behavior compared to \&\#x00F8;-divergence estimators.},
  journaltitle = {Electronic Journal of Statistics},
  date = {2012},
  pages = {1550-1599},
  keywords = {Dual-bounded Lipschitz distance (Dudley metric),Empirical estimation,Integral probability metrics,Kantorovich metric,Kernel distance,Lipschitz classifier,Rademacher average,Reproducing kernel Hilbert space,Support vector machine},
  author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R.G.},
  file = {/usr/stud/brechet/Zotero/storage/E9J93V5S/sriperumbudur2012.pdf},
  isbn = {9781424469604}
}

@article{Mattar2012,
  title = {Differential {{Entropy}}},
  url = {http://mathworld.wolfram.com/DifferentialEntropy.html},
  number = {X},
  journaltitle = {MathWorld--A Wolfram Web Resource},
  date = {2012},
  pages = {243-259},
  author = {Mattar, Marwan and Rudary, Matthew Weisstein, Eric},
  isbn = {9781466583177}
}

@book{Santambrogio2015b,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  isbn = {978-3-319-20827-5},
  abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
  pagetotal = {1-376},
  date = {2015},
  author = {Santambrogio, F.},
  file = {/usr/stud/brechet/Zotero/storage/IKNCMGFK/Santambrogio - 2015 - Optimal Transport for Applied Mathematicians.pdf},
  doi = {10.1007/978-3-319-20828-2},
  booktitle = {Springer {{International Publishing Switzerland}}}
}

@article{To2019,
  title = {E {{NTROPIC GAN S MEET VAE S}} : {{A S TATISTICAL A PPROACH TO C OMPUTE S AMPLE}}},
  date = {2019},
  author = {To, Pproach}
}

@article{Bellemare2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10743},
  title = {The {{Cramer Distance}} as a {{Solution}} to {{Biased Wasserstein Gradients}}},
  issn = {0022-0302},
  url = {http://arxiv.org/abs/1705.10743},
  doi = {10.1111/j.1574-0862.2012.00588.x},
  abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram\textbackslash{}'er distance. We show that the Cram\textbackslash{}'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram\textbackslash{}'er distance in practice we design a new algorithm, the Cram\textbackslash{}'er Generative Adversarial Network (GAN), and show that it performs significantly better than the related Wasserstein GAN.},
  date = {2017},
  pages = {1-20},
  author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
  isbn = {1112300600},
  arxivid = {1705.10743}
}

@article{Heusel2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1706.08500v6},
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  issue = {Nips},
  date = {2017},
  author = {Heusel, Martin and Jan, L G and Hochreiter, Sepp},
  arxivid = {arXiv:1706.08500v6}
}

@article{Nye2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.06399},
  title = {Are {{Efficient Deep Representations Learnable}}?},
  issn = {0004-6361},
  url = {http://arxiv.org/abs/1807.06399},
  doi = {10.1051/0004-6361/201527329},
  abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
  date = {2018},
  pages = {1-16},
  keywords = {TOREAD},
  author = {Nye, Maxwell and Saxe, Andrew},
  isbn = {2004012439},
  arxivid = {1807.06399},
  mendeley-tags = {TOREAD},
  pmid = {23459267}
}

@article{Chen2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03657},
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  issn = {978-3-319-16807-4},
  url = {http://arxiv.org/abs/1606.03657},
  doi = {10.1007/978-3-319-16817-3},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
  date = {2016},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  file = {/Users/Pierre/Documents/ss18/mt/literature/infogan_interpretable_representation_learning_by_information_maximizing_generative_adversarial_nets.pdf;/usr/stud/brechet/Zotero/storage/2CEWKHMB/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf},
  isbn = {978-3-319-16816-6},
  arxivid = {1606.03657},
  pmid = {23459267}
}

@book{Santambrogio2015a,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  isbn = {978-3-319-20827-5},
  abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
  pagetotal = {1-376},
  date = {2015},
  author = {Santambrogio, F.},
  doi = {10.1007/978-3-319-20828-2},
  booktitle = {Springer {{International Publishing Switzerland}}}
}

@article{Cover2006a,
  title = {Universal Source Coding},
  journaltitle = {Elements of Information Theory},
  date = {2006},
  pages = {427-462},
  author = {Cover, Thomas and Thomas, Joy}
}

@article{Levy2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.02634},
  title = {Notions of Optimal Transport Theory and How to Implement Them on a Computer},
  volume = {72},
  issn = {00978493},
  doi = {10.1016/j.cag.2018.01.009},
  abstract = {This article gives an introduction to optimal transport, a mathematical theory that makes it possible to measure distances between functions (or distances between more general objects), to interpolate between objects or to enforce mass/volume conservation in certain computational physics simulations. Optimal transport is a rich scientific domain, with active research communities, both on its theoretical aspects and on more applicative considerations, such as geometry processing and machine learning. This article aims at explaining the main principles behind the theory of optimal transport, introduce the different involved notions, and more importantly, how they relate, to let the reader grasp an intuition of the elegant theory that structures them. Then we will consider a specific setting, called semi-discrete, where a continuous function is transported to a discrete sum of Dirac masses. Studying this specific setting naturally leads to an efficient computational algorithm, that uses classical notions of computational geometry, such as a generalization of Voronoi diagrams called Laguerre diagrams.},
  journaltitle = {Computers and Graphics (Pergamon)},
  date = {2018},
  pages = {135-148},
  keywords = {Mathematics,Numerical optimization,Optimal transport,Physics},
  author = {Lévy, Bruno and Schwindt, Erica L.},
  file = {/usr/stud/brechet/Zotero/storage/6WUL6TTE/Lévy, Schwindt - 2018 - Notions of optimal transport theory and how to implement them on a computer.pdf},
  arxivid = {1710.02634}
}

@article{Durrett2010,
  title = {Probability},
  issn = {0006341X},
  url = {http://ebooks.cambridge.org/ref/id/CBO9780511779398},
  doi = {10.1017/CBO9780511779398},
  abstract = {"This book is an introduction to probability theory covering laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. It is a comprehensive treatment concentrating on the results that are the most useful for applications. Its philosophy is that the best way to learn probability is to see it in action, so there are 200 examples and 450 problems"--},
  date = {2010},
  pages = {438},
  author = {Durrett, Rick},
  isbn = {9780511779398},
  pmid = {16156020}
}

@article{Courty2017b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08848},
  title = {Joint {{Distribution Optimal Transportation}} for {{Domain Adaptation}}},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2016.2615921},
  abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \$\textbackslash{}mathcal\{P\}\_s\$ and \$\textbackslash{}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \$\textbackslash{}mathcal\{P\}\^f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
  date = {2017},
  author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
  isbn = {9782875870148},
  arxivid = {1705.08848}
}

@article{Aronszajn1950,
  eprinttype = {jstor},
  eprint = {1990404?origin=crossref},
  title = {Theory of {{Reproducing Kernels}}},
  volume = {68},
  issn = {00029947},
  doi = {10.2307/1990404},
  abstract = {Preface The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie générale de noyaux reproduisants—Première partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we ...},
  number = {3},
  journaltitle = {Transactions of the American Mathematical Society},
  date = {1950},
  pages = {337},
  author = {Aronszajn, N.},
  isbn = {047021211X},
  pmid = {17746742}
}

@article{Loshchilov2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05101},
  title = {Fixing {{Weight Decay Regularization}} in {{Adam}}},
  volume = {100},
  issn = {1567-7257},
  url = {http://arxiv.org/abs/1711.05101},
  doi = {10.1016/j.meegid.2012.02.004},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash{}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard SGD and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with SGD with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts (AdamWR) that has strong anytime performance while achieving state-of-the-art results on CIFAR-10 and ImageNet32x32. Our source code is available at https://github.com/loshchil/AdamW-and-SGDW},
  date = {2017},
  author = {Loshchilov, Ilya and Hutter, Frank},
  isbn = {1711.05101v2},
  arxivid = {1711.05101},
  pmid = {22365971}
}

@article{Oizumi,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1709.10219v1},
  title = {Information {{Geometry Connecting Wasserstein Distance}} and {{Kullback}}-{{Leibler Divergence}} via the {{Entropy}}-{{Relaxed Transportation Problem}}},
  pages = {1-24},
  author = {Oizumi, Masafumi},
  file = {/usr/stud/brechet/Zotero/storage/HIG6UQEA/Information-Geometry-Connecting-Wasserstein-Distance-and-Kullback-Leibler-Divergence-via-the-Entropy-Relaxed-Transportation-Problem.pdf},
  arxivid = {arXiv:1709.10219v1}
}

@article{Xiang2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.03971},
  title = {On the {{Effects}} of {{Batch}} and {{Weight Normalization}} in {{Generative Adversarial Networks}}},
  issn = {0098-6445},
  url = {http://arxiv.org/abs/1704.03971},
  doi = {10.1080/00986449008911427},
  abstract = {Generative adversarial networks (GANs) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However GANs are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization (BN) techniques have been introduced to address the training. Though BN accelerates the training in the beginning, our experiments show that the use of BN can be unstable and negatively impact the quality of the trained model. The evaluation of BN and numerous other recent schemes for improving GAN training is hindered by the lack of an effective objective quality measure for GAN models. To address these issues, we first introduce a weight normalization (WN) approach for GAN training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard DCGAN architecture on commonly used datasets (CelebA, LSUN bedroom, and CIFAR-10) indicate that training using WN is generally superior to BN for GANs, achieving 10\% lower mean squared loss for reconstruction and significantly better qualitative results than BN. We further demonstrate the stability of WN on a 21-layer ResNet trained with the CelebA data set. The code for this paper is available at https://github.com/stormraiser/gan-weightnorm-resnet},
  date = {2017},
  author = {Xiang, Sitao and Li, Hao},
  arxivid = {1704.03971}
}

@article{Nye2018a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.06399},
  title = {Are {{Efficient Deep Representations Learnable}}?},
  issn = {0004-6361},
  url = {http://arxiv.org/abs/1807.06399},
  doi = {10.1051/0004-6361/201527329},
  abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
  issue = {Ml},
  date = {2018},
  pages = {1-14},
  author = {Nye, Maxwell and Saxe, Andrew},
  file = {/usr/stud/brechet/Zotero/storage/848MS7XW/Welling - Unknown - Auto-Encoding Variational Bayes arXiv 1312 . 6114v10 stat . ML 1 May 2014.pdf},
  isbn = {2004012439},
  arxivid = {1807.06399},
  pmid = {23459267}
}

@article{Cover2005a,
  title = {Elements of {{Information Theory}}},
  issn = {15579654},
  doi = {10.1002/047174882X},
  abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
  number = {x},
  journaltitle = {Elements of Information Theory},
  date = {2005},
  pages = {1-748},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  isbn = {9780471241959},
  arxivid = {ISBN 0-471-06259-6},
  pmid = {20660925},
  eprinttype = {arxiv}
}

@article{Uilbart1979,
  title = {'i. h. p.,},
  volume = {4},
  date = {1979},
  pages = {333-354},
  author = {Uilbart, C G}
}

@article{Lecun2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6184v5},
  title = {Deep Learning},
  volume = {521},
  issn = {14764687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  number = {7553},
  date = {2015},
  pages = {436-444},
  author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  booktitle = {Nature},
  isbn = {9780521835688},
  arxivid = {arXiv:1312.6184v5},
  pmid = {10463930}
}

@article{Cover2005,
  title = {Elements of {{Information Theory}}},
  issn = {15579654},
  doi = {10.1002/047174882X},
  abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, AEP, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
  journaltitle = {Elements of Information Theory},
  date = {2005},
  pages = {1-748},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  isbn = {9780471241959},
  arxivid = {ISBN 0-471-06259-6},
  pmid = {20660925},
  eprinttype = {arxiv}
}

@article{Blondel2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.06276v2},
  title = {Smooth and {{Sparse Optimal Transport}}},
  volume = {84},
  date = {2018},
  author = {Blondel, Mathieu and Seguy, Vivien and Rolet, Antoine},
  arxivid = {arXiv:1710.06276v2}
}

@article{Green2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.00160},
  title = {Modeling Contagion through Social Networks to Explain and Predict Gunshot Violence in {{Chicago}}, 2006 to 2014},
  volume = {177},
  issn = {21686106},
  url = {http://arxiv.org/abs/1406.2661},
  doi = {10.1001/jamainternmed.2016.8245},
  abstract = {Importance Every day in the United States, more than 200 people are murdered or assaulted with a firearm. Little research has considered the role of interpersonal ties in the pathways through which gun violence spreads. Objective To evaluate the extent to which the people who will become subjects of gun violence can be predicted by modeling gun violence as an epidemic that is transmitted between individuals through social interactions. Design, Setting, and Participants This study was an epidemiological analysis of a social network of individuals who were arrested during an 8-year period in Chicago, Illinois, with connections between people who were arrested together for the same offense. Modeling of the spread of gunshot violence over the network was assessed using a probabilistic contagion model that assumed individuals were subject to risks associated with being arrested together, in addition to demographic factors, such as age, sex, and neighborhood residence. Participants represented a network of 138 163 individuals who were arrested between January 1, 2006, and March 31, 2014 (29.9\% of all individuals arrested in Chicago during this period), 9773 of whom were subjects of gun violence. Individuals were on average 27 years old at the midpoint of the study, predominantly male (82.0\%) and black (75.6\%), and often members of a gang (26.2\%). Main Outcomes and Measures Explanation and prediction of becoming a subject of gun violence (fatal or nonfatal) using epidemic models based on person-to-person transmission through a social network. Results Social contagion accounted for 63.1\% of the 11 123 gunshot violence episodes; subjects of gun violence were shot on average 125 days after their infector (the person most responsible for exposing the subject to gunshot violence). Some subjects of gun violence were shot more than once. Models based on both social contagion and demographics performed best; when determining the 1.0\% of people (n = 1382) considered at highest risk to be shot each day, the combined model identified 728 subjects of gun violence (6.5\%) compared with 475 subjects of gun violence (4.3\%) for the demographics model (53.3\% increase) and 589 subjects of gun violence (5.3\%) for the social contagion model (23.6\% increase). Conclusions and Relevance Gunshot violence follows an epidemic-like process of social contagion that is transmitted through networks of people by social interactions. Violence prevention efforts that account for social contagion, in addition to demographics, have the potential to prevent more shootings than efforts that focus on only demographics.},
  number = {3},
  journaltitle = {JAMA Internal Medicine},
  date = {2017},
  pages = {326-333},
  author = {Green, Ben and Horel, Thibaut and Papachristos, Andrew V.},
  file = {/usr/stud/brechet/Zotero/storage/IIUJITFN/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf},
  isbn = {1581138285},
  arxivid = {1701.00160},
  pmid = {15040217}
}

@article{Coppersmith2006,
  title = {Bibliography [1]},
  date = {2006},
  pages = {145-171},
  author = {Coppersmith, D}
}

@article{Wang,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.07971v3},
  title = {Non-Local {{Neural Networks}}},
  author = {Wang, Xiaolong and Girshick, Ross},
  arxivid = {arXiv:1711.07971v3}
}

@article{DaSilva2008,
  title = {Variational Analysis of the Phenyl + {{O2and}} Phenoxy + {{O}} Reactions},
  volume = {112},
  issn = {10895639},
  doi = {10.1021/jp7118845},
  abstract = {Variational transition state analysis was performed on the barrierless phenyl + O2 and phenoxy + O association reactions. In addition, we also calculated rate constants for the related vinyl radical (C2H3) + O2 and vinoxy radical (C2H3O) + O reactions and provided rate constant estimates for analogous reactions in substituted aromatic systems. Potential energy scans along the dissociating C-OO and CO-O bonds (with consideration of C-OO internal rotation) were obtained at the O3LYP/6-31G(d) density functional theory level. The CO-O and C-OO bond scission reactions were observed to be barrierless, in both phenyl and vinyl systems. Potential energy wells were scaled by G3B3 reaction enthalpies to obtain accurate activation enthalpies. Frequency calculations were performed for all reactants and products and at points along the potential energy surfaces, allowing us to evaluate thermochemical properties as a function of temperature according to the principles of statistical mechanics and the rigid rotor harmonic oscillator (RRHO) approximation. The low-frequency vibrational modes corresponding to R-OO internal rotation were omitted from the RRHO analysis and replaced with a hindered internal rotor analysis using O3LYP/6-31G(d) rotor potentials. Rate constants were calculated as a function of temperature (300-2000 K) and position from activation entropies and enthalpies, according to canonical transition state theory; these rate constants were minimized with respect to position to obtain variational rate constants as a function of temperature. For the phenyl + O2 reaction, we identified the transition state to be located at a C-OO bond length of between 2.56 and 2.16 A (300-2000 K), while for the phenoxy + O reaction, the transition state was located at a CO-O bond length of 2.00-1.90 A. Variational rate constants were fit to a three-parameter form of the Arrhenius equation, and for the phenyl + O2 association reaction, we found k(T) = 1.860 x 1013T-0.217 exp(0.358/T) (with k in cm3 mol-1 s-1 and T in K); this rate equation provides good agreement with low-temperature experimental measurements of the phenyl + O2 rate constant. Preliminary results were presented for a correlation between activation energy (or reaction enthalpy) and pre-exponential factor for heterolytic O-O bond scission reactions.},
  number = {16},
  journaltitle = {Journal of Physical Chemistry A},
  date = {2008},
  pages = {3566-3575},
  author = {Da Silva, Gabriel and Bozzelli, Joseph W.},
  isbn = {1089-5639},
  pmid = {18348555}
}

@article{Salimans,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1602.07868v3},
  title = {Weight {{Normalization}} : {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  author = {Salimans, Tim},
  arxivid = {arXiv:1602.07868v3}
}

@article{Frogner2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1506.05439},
  title = {Learning with a {{Wasserstein Loss}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1506.05439},
  doi = {10.1016/S0891-5849(98)00315-3},
  abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
  date = {2015},
  pages = {1-17},
  keywords = {confusing dogs with cats,for example,in what follows,lationships between the different,might,or semantic similarity,output dimensions,performance in a way,re-,relationships,similarity,space the ground metric,structure on the label,that is sensitive to,that likewise have semantic,using the ground metric,we can measure prediction,we will call the},
  author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
  isbn = {1506.05439},
  arxivid = {1506.05439},
  pmid = {10381194}
}

@article{Gregor2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1502.04623v2},
  title = {{{DRAW}} : {{A Recurrent Neural Network For Image Generation arXiv}} : 1502 . 04623v2 [ Cs . {{CV}} ] 20 {{May}} 2015},
  date = {2014},
  author = {Gregor, Karol and Graves, Alex and Com, Wierstra Google},
  arxivid = {arXiv:1502.04623v2}
}

@article{Gretton2008,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {0805.2368},
  title = {A {{Kernel Method}} for the {{Two}}-{{Sample Problem}}},
  volume = {1},
  issn = {1049-5258},
  url = {http://arxiv.org/abs/0805.2368},
  abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  date = {2008},
  pages = {1-10},
  author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J. and Scholkopf, Bernhard and Smola, Alexander J.},
  file = {/usr/stud/brechet/Zotero/storage/6H9T4VCU/gretton2008.pdf},
  isbn = {0-262-19568-2},
  arxivid = {0805.2368}
}

@article{Salomon2007,
  title = {Data {{Compression}}},
  issn = {2155-5222},
  url = {http://www.csa.com/partners/viewrecord.php?requester=gs\{&\}collection=TRD\{&\}recid=1892920CI},
  doi = {10.1002/9781118256053.ch13},
  number = {x},
  journaltitle = {Online},
  date = {2007},
  pages = {1008},
  author = {Salomon, David},
  isbn = {1-84628-602-6},
  pmid = {20556846}
}

@article{Feizi,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.10793v2},
  title = {Understanding {{GANs}} : The {{LQG Setting}}},
  pages = {1-22},
  author = {Feizi, Soheil and Farnia, Farzan and Ginart, Tony and Tse, David},
  arxivid = {arXiv:1710.10793v2}
}

@article{Type2006,
  title = {Maximum {{Entropy}} 모델을 이용한 나열 및 병렬형 인식},
  volume = {1},
  number = {x},
  date = {2006},
  pages = {409-425},
  author = {Type, Parallel and Maximum, Using and Model, Entropy}
}

@book{Santambrogio2015,
  title = {Optimal {{Transport}} for {{Applied Mathematicians}}},
  isbn = {978-3-319-20827-5},
  abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
  pagetotal = {1-376},
  date = {2015},
  author = {Santambrogio, F.},
  doi = {10.1007/978-3-319-20828-2},
  booktitle = {Springer {{International Publishing Switzerland}}}
}

@article{Courty2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08848},
  title = {Joint {{Distribution Optimal Transportation}} for {{Domain Adaptation}}},
  issn = {0162-8828},
  url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123838360000187 http://arxiv.org/abs/1705.08848},
  doi = {10.1109/TPAMI.2016.2615921},
  abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \$\textbackslash{}mathcal\{P\}\_s\$ and \$\textbackslash{}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \$\textbackslash{}mathcal\{P\}\^f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
  journaltitle = {MATLAB for Neuroscientists},
  date = {2017},
  pages = {317-327},
  author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
  isbn = {9782875870148},
  arxivid = {1705.08848},
  pmid = {21492013}
}

@article{Restarts2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.13243v1},
  title = {{{TION LOOK AT D EEP L EARNING HEURISTICS}} :},
  date = {2018},
  author = {Restarts, Rate},
  arxivid = {arXiv:1810.13243v1}
}

@article{Cuturi2013a,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1306.0895},
  title = {Sinkhorn {{Distances}}: {{Lightspeed Computation}} of {{Optimal Transportation Distances}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1306.0895},
  abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the MNIST benchmark problem.},
  date = {2013},
  pages = {1-9},
  author = {Cuturi, Marco},
  file = {/usr/stud/brechet/Zotero/storage/5RZ26V8E/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Optimal Transportation Distances.pdf},
  isbn = {9781510810587},
  arxivid = {1306.0895},
  pmid = {1714571}
}

@book{Garreau1987,
  eprinttype = {jstor},
  eprint = {2348852?origin=crossref},
  title = {Real and {{Complex Analysis}}.},
  volume = {36},
  isbn = {0-07-054234-1},
  abstract = {This is an advanced text for the one- or two-semester course in analysis taught primarily to math, science, computer science, and electrical engineering majors at the junior, senior or graduate level. The basic techniques and theorems of analysis are presented in such a way that the intimate connections between its various branches are strongly emphasized. The traditionally separate subjects of 'real analysis' and 'complex analysis' are thus united in one volume. Some of the basic ideas from functional analysis are also included. This is the only book to take this unique approach. The third edition includes a new chapter on differentiation. Proofs of theorems presented in the book are concise and complete and many challenging exercises appear at the end of each chapter. The book is arranged so that each chapter builds upon the other, giving students a gradual understanding of the subject.},
  pagetotal = {423},
  number = {4},
  date = {1987},
  author = {Garreau, G. A. and Rudin, Walter},
  doi = {10.2307/2348852},
  issn = {00390526},
  booktitle = {The {{Statistician}}},
  pmid = {21273021}
}

@article{Bousquet2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.07642},
  title = {From Optimal Transport to Generative Modeling: The {{VEGAN}} Cookbook},
  url = {http://arxiv.org/abs/1705.07642},
  abstract = {We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution \$P\_X\$ and the latent variable model distribution \$P\_G\$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from \$P\_X\$ and \$P\_G\$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.},
  date = {2017},
  pages = {1-15},
  author = {Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Simon-Gabriel, Carl-Johann and Schoelkopf, Bernhard},
  file = {/usr/stud/brechet/Zotero/storage/YPCSVXSH/Bousquet et al. - Unknown - From optimal transport to generative modeling the VEGAN cookbook.pdf},
  arxivid = {1705.07642}
}

@article{Nye2018b,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1807.06399},
  title = {Are {{Efficient Deep Representations Learnable}}?},
  issn = {0004-6361},
  url = {http://arxiv.org/abs/1807.06399},
  doi = {10.1051/0004-6361/201527329},
  abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
  date = {2018},
  author = {Nye, Maxwell and Saxe, Andrew},
  isbn = {2004012439},
  arxivid = {1807.06399},
  pmid = {23459267}
}

@article{Spectroscopy1975,
  title = {List of {{Symbols}}},
  issn = {1473-6691},
  url = {http://linkinghub.elsevier.com/retrieve/pii/B9780122120503500074},
  doi = {10.1016/B978-0-12-212050-3.50007-4},
  abstract = {00000},
  number = {x},
  journaltitle = {Feedback Systems: Input–Output Properties},
  date = {1975},
  pages = {xv-xvi},
  author = {Spectroscopy, Electrochemical Impedance},
  isbn = {9780511615092},
  pmid = {24170943}
}

@article{Ba2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1607.06450},
  title = {Layer {{Normalization}}},
  issn = {10450823},
  url = {http://arxiv.org/abs/1607.06450},
  doi = {10.1038/nature14236},
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  date = {2016},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
  isbn = {978-3-642-04273-7},
  arxivid = {1607.06450}
}

@article{Zeiler2012,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1311.2901v3},
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  date = {2012},
  author = {Zeiler, Matthew D and Fergus, Rob},
  arxivid = {arXiv:1311.2901v3}
}

@article{Li2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.08584},
  title = {{{MMD GAN}}: {{Towards Deeper Understanding}} of {{Moment Matching Network}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1705.08584},
  abstract = {Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD). Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN significantly outperforms GMMN, and is competitive with other representative GAN works.},
  number = {MMD},
  date = {2017},
  pages = {1-14},
  author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and Póczos, Barnabás},
  arxivid = {1705.08584}
}

@article{Peyre2011,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1104.4631},
  title = {Comparison between \${{W}}\_2\$ Distance and \$\textbackslash{}dot\{\vphantom\}{{H}}\vphantom\{\}\^\{-1\}\$ Norm, and Localisation of {{Wasserstein}} Distance},
  volume = {2},
  url = {http://arxiv.org/abs/1104.4631},
  doi = {10.1051/cocv/2017050},
  abstract = {It is well known that the quadratic Wasserstein distance \$W\_2 (\textbackslash{}mathord\{\textbackslash{}boldsymbol\{\textbackslash{}cdot\}\}, \textbackslash{}mathord\{\textbackslash{}boldsymbol\{\textbackslash{}cdot\}\})\$ is formally equivalent, for infinitesimally small perturbations, to some weighted \$H\^\{-1\}\$ homogeneous Sobolev norm. In this article I show that this equivalence can be integrated to get non-asymptotic comparison results between these distances. Then I give an application of these results to prove that the \$W\_2\$ distance exhibits some localisation phenomenon: if \$\textbackslash{}mu\$ and \$\textbackslash{}nu\$ are measures on \$\textbackslash{}mathbf\{R\}\^n\$ and \$\textbackslash{}varphi \textbackslash{}colon \textbackslash{}mathbf\{R\}\^n \textbackslash{}to \textbackslash{}mathbf\{R\}\_+\$ is some bump function with compact support, then under mild hypotheses, you can bound above the Wasserstein distance between \$\textbackslash{}varphi \textbackslash{}cdot \textbackslash{}mu\$ and \$\textbackslash{}varphi \textbackslash{}cdot \textbackslash{}nu\$ by an explicit multiple of \$W\_2 (\textbackslash{}mu, \textbackslash{}nu)\$.},
  number = {2},
  date = {2011},
  keywords = {homogeneous sobolev norm,localisation,wasserstein distance},
  author = {Peyre, Rémi},
  file = {/usr/stud/brechet/Zotero/storage/MEGAFLJF/peyre2011.pdf},
  arxivid = {1104.4631}
}

@article{Bottou2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.04838},
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  issn = {0036-1445},
  url = {http://arxiv.org/abs/1606.04838},
  doi = {10.1137/16M1080173},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  date = {2016},
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  file = {/usr/stud/brechet/Zotero/storage/8ZS9Q24I/Bottou, Curtis, Nocedal - 2016 - Optimization Methods for Large-Scale Machine Learning.pdf},
  isbn = {9781538634288},
  arxivid = {1606.04838}
}

@article{Sommerfeld2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.03287},
  title = {Inference for Empirical {{Wasserstein}} Distances on Finite Spaces},
  volume = {80},
  issn = {14679868},
  doi = {10.1111/rssb.12236},
  abstract = {The Wasserstein distance is an attractive tool for data analysis but statistical inference is hindered by the lack of distributional limits. To overcome this obstacle, for probability measures supported on finitely many points, we derive the asymptotic distribution of empirical Wasserstein distances as the optimal value of a linear program with random objective function. This facilitates statistical inference (e.g. confidence intervals for sample based Wasserstein distances) in large generality. Our proof is based on directional Hadamard differentiability. Failure of the classical bootstrap and alternatives are discussed. The utility of the distributional results is illustrated on two data sets.},
  number = {1},
  journaltitle = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
  date = {2018},
  pages = {219-238},
  keywords = {Wasserstein distance,Optimal transport,Bootstrap,Central limit theorem,Directional Hadamard derivative,Hypothesis testing},
  author = {Sommerfeld, Max and Munk, Axel},
  arxivid = {1610.03287}
}

@article{Gehring2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.03122v3},
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  date = {2016},
  author = {Gehring, Jonas and Dauphin, Yann N},
  arxivid = {arXiv:1705.03122v3}
}

@article{Rockafellar1970,
  title = {Convex {{Analysis}}},
  issn = {0036-1445},
  url = {https://www.degruyter.com/view/books/9781400873173/9781400873173/9781400873173.xml},
  doi = {10.1515/9781400873173},
  abstract = {Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.},
  date = {1970},
  author = {Rockafellar, Ralph Tyrell},
  booktitle = {Analysis},
  isbn = {9781400873173}
}

@article{Gotmare2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1810.13243},
  title = {A {{Closer Look}} at {{Deep Learning Heuristics}}: {{Learning}} Rate Restarts, {{Warmup}} and {{Distillation}}},
  url = {https://arxiv.org/abs/1810.13243},
  date = {2018-10},
  author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  arxivid = {1810.13243}
}

@article{Mescheder2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1705.10461},
  title = {The {{Numerics}} of {{GANs}}},
  issn = {10495258},
  url = {http://arxiv.org/abs/1705.10461},
  abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.},
  issue = {Nips},
  date = {2017},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  file = {/usr/stud/brechet/Zotero/storage/2ISNZ59G/Mescheder et al. - 2017 - The Numerics of GANs.pdf},
  arxivid = {1705.10461}
}

@book{Bertsimas1997a,
  title = {Introduction to {{Linear Optimization}}},
  volume = {30},
  isbn = {1-886529-19-1},
  abstract = {This book provides a unified, insightful, and modern treatment of linear optimization, that is, linear programming, network flow problems, and discrete optimization. It includes classical topics as well as the state of the art, in both theory and practice.},
  pagetotal = {15-21},
  date = {1997},
  author = {Bertsimas, Dimitris J. and Tsitsiklis, J.N. John N. J.N.},
  booktitle = {Introduction to {{Linear Optimization}}},
  pmid = {3528304}
}

@article{Salimans2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.05573},
  primaryClass = {cs, stat},
  title = {Improving {{GANs Using Optimal Transport}}},
  url = {http://arxiv.org/abs/1803.05573},
  abstract = {We present Optimal Transport GAN (OT-GAN), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show OT-GAN to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.},
  urldate = {2019-03-21},
  date = {2018-03-14},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Salimans, Tim and Zhang, Han and Radford, Alec and Metaxas, Dimitris},
  file = {/usr/stud/brechet/Zotero/storage/F75GS5TE/Salimans et al. - 2018 - Improving GANs Using Optimal Transport.pdf;/usr/stud/brechet/Zotero/storage/SF9ZH5PB/1803.html}
}

@article{Salimans2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.03498},
  primaryClass = {cs},
  title = {Improved {{Techniques}} for {{Training GANs}}},
  url = {http://arxiv.org/abs/1606.03498},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. We focus on two applications of GANs: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  urldate = {2019-03-21},
  date = {2016-06-10},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
  file = {/usr/stud/brechet/Zotero/storage/A3JZM648/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf;/usr/stud/brechet/Zotero/storage/DEWV8MVB/1606.html}
}

@article{Gulrajani2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.00028},
  primaryClass = {cs, stat},
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  url = {http://arxiv.org/abs/1704.00028},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  urldate = {2019-03-21},
  date = {2017-03-31},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
  file = {/usr/stud/brechet/Zotero/storage/XUFXPCWH/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf;/usr/stud/brechet/Zotero/storage/HI93EMPA/1704.html}
}

@article{Higgins2016,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  url = {https://openreview.net/forum?id=Sy2fzU9gl},
  shorttitle = {Beta-{{VAE}}},
  abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial...},
  urldate = {2019-03-23},
  date = {2016-11-04},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
  file = {/usr/stud/brechet/Zotero/storage/E6TI4BM5/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf;/usr/stud/brechet/Zotero/storage/GSMTT4ZK/forum.html}
}

@article{Karras2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1812.04948},
  primaryClass = {cs, stat},
  title = {A {{Style}}-{{Based Generator Architecture}} for {{Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1812.04948},
  abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
  urldate = {2019-03-23},
  date = {2018-12-12},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  author = {Karras, Tero and Laine, Samuli and Aila, Timo},
  file = {/usr/stud/brechet/Zotero/storage/PAYISA8S/Karras et al. - 2018 - A Style-Based Generator Architecture for Generativ.pdf;/usr/stud/brechet/Zotero/storage/JNRUH8NN/1812.html}
}

@article{Ridgeway2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1612.05299},
  primaryClass = {cs},
  title = {A {{Survey}} of {{Inductive Biases}} for {{Factorial Representation}}-{{Learning}}},
  url = {http://arxiv.org/abs/1612.05299},
  abstract = {With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning. This article surveys various constraints that encourage a learning algorithm to discover factorial representations. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to. This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.},
  urldate = {2019-03-23},
  date = {2016-12-15},
  keywords = {Computer Science - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Ridgeway, Karl},
  file = {/usr/stud/brechet/Zotero/storage/PC59NR2J/Ridgeway - 2016 - A Survey of Inductive Biases for Factorial Represe.pdf;/usr/stud/brechet/Zotero/storage/3JJD59JD/1612.html}
}

@article{Houthooft2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09674},
  primaryClass = {cs, stat},
  title = {{{VIME}}: {{Variational Information Maximizing Exploration}}},
  url = {http://arxiv.org/abs/1605.09674},
  shorttitle = {{{VIME}}},
  abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
  urldate = {2019-03-23},
  date = {2016-05-31},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence,Computer Science - Robotics},
  author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
  file = {/usr/stud/brechet/Zotero/storage/DW8ZTSL2/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf;/usr/stud/brechet/Zotero/storage/ZPBR8Y44/1605.html}
}

@article{Arjovsky2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.07875},
  primaryClass = {cs, stat},
  title = {Wasserstein {{GAN}}},
  url = {http://arxiv.org/abs/1701.07875},
  abstract = {We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
  urldate = {2019-03-25},
  date = {2017-01-26},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
  file = {/usr/stud/brechet/Zotero/storage/3EJBR6TS/Arjovsky et al. - 2017 - Wasserstein GAN.pdf;/usr/stud/brechet/Zotero/storage/EGA28UQ7/1701.html}
}

@article{Kingma2014,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1412.6980},
  primaryClass = {cs},
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  url = {http://arxiv.org/abs/1412.6980},
  shorttitle = {Adam},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  urldate = {2019-03-27},
  date = {2014-12-22},
  keywords = {Computer Science - Machine Learning},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  file = {/usr/stud/brechet/Zotero/storage/LJWIXXKQ/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf;/usr/stud/brechet/Zotero/storage/B9WVYY8L/1412.html}
}

@article{Balaji2018,
  title = {Entropic {{GANs}} Meet {{VAEs}}: {{A Statistical Approach}} to {{Compute Sample Likelihoods}} in {{GANs}}},
  url = {https://openreview.net/forum?id=BygMAiRqK7},
  shorttitle = {Entropic {{GANs}} Meet {{VAEs}}},
  abstract = {Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks (GANs) and Variational AutoEncoders (VAEs)....},
  urldate = {2019-03-27},
  date = {2018-09-27},
  author = {Balaji, Yogesh and Hasani, Hamed and Chellappa, Rama and Feizi, Soheil},
  file = {/usr/stud/brechet/Zotero/storage/G4STG9BB/Balaji et al. - 2018 - Entropic GANs meet VAEs A Statistical Approach to.pdf;/usr/stud/brechet/Zotero/storage/C2JGQKP3/forum.html}
}

@book{Gelles2017,
  langid = {english},
  location = {{Boston Delft}},
  title = {Coding for Interactive Communication: A Survey},
  isbn = {978-0-521-83378-3 978-1-68083-346-1},
  shorttitle = {Coding for Interactive Communication},
  pagetotal = {168},
  number = {13:1-2},
  series = {Foundations and Trends in Theoretical Computer Science},
  publisher = {{now Publishers}},
  date = {2017},
  author = {Gelles, Ran},
  file = {/usr/stud/brechet/Zotero/storage/DBMRLPYB/Gelles - 2017 - Coding for interactive communication a survey.pdf},
  note = {OCLC: on1035831877}
}

@book{Gelles2017a,
  langid = {english},
  location = {{Boston Delft}},
  title = {Coding for Interactive Communication: A Survey},
  isbn = {978-0-521-83378-3 978-1-68083-346-1},
  shorttitle = {Coding for Interactive Communication},
  pagetotal = {168},
  number = {13:1-2},
  series = {Foundations and Trends in Theoretical Computer Science},
  publisher = {{now Publishers}},
  date = {2017},
  author = {Gelles, Ran},
  file = {/usr/stud/brechet/Zotero/storage/B2PDYB5L/Gelles - 2017 - Coding for interactive communication a survey.pdf},
  note = {OCLC: on1035831877}
}

@book{Boyd2010,
  location = {{Cambridge [u.a.]}},
  title = {Convex Optimization},
  edition = {8. print.},
  isbn = {978-0-521-83378-3},
  pagetotal = {XIII, 716 S. :},
  publisher = {{Cambridge Univ. Press}},
  date = {2010},
  author = {Boyd, Stephen P.},
  file = {/usr/stud/brechet/Zotero/storage/4T5KU53A/singleHit.html}
}

@article{Burgess2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.03599},
  primaryClass = {cs, stat},
  title = {Understanding Disentangling in \$\textbackslash{}beta\$-{{VAE}}},
  url = {http://arxiv.org/abs/1804.03599},
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in \$\textbackslash{}beta\$-VAE, as training progresses. From these insights, we propose a modification to the training regime of \$\textbackslash{}beta\$-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \$\textbackslash{}beta\$-VAE, without the previous trade-off in reconstruction accuracy.},
  urldate = {2019-04-05},
  date = {2018-04-10},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Artificial Intelligence},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  file = {/usr/stud/brechet/Zotero/storage/H5LUGGVM/Burgess et al. - 2018 - Understanding disentangling in $beta$-VAE.pdf;/usr/stud/brechet/Zotero/storage/YLLPC42G/1804.html}
}

@article{Esmaeili2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1804.02086},
  primaryClass = {cs, stat},
  title = {Structured {{Disentangled Representations}}},
  url = {http://arxiv.org/abs/1804.02086},
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  urldate = {2019-04-05},
  date = {2018-04-05},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and van de Meent, Jan-Willem},
  options = {useprefix=true},
  file = {/usr/stud/brechet/Zotero/storage/MIN84HFJ/Esmaeili et al. - 2018 - Structured Disentangled Representations.pdf;/usr/stud/brechet/Zotero/storage/JB4WL7ZH/1804.html}
}

@article{Hong2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1711.05914},
  title = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}: {{An Overview}}},
  volume = {52},
  issn = {03600300},
  url = {http://arxiv.org/abs/1711.05914},
  doi = {10.1145/3301282},
  shorttitle = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}},
  abstract = {Generative Adversarial Networks (GAN) have received wide attention in the machine learning field for their potential to learn high-dimensional, complex real data distribution. Specifically, they do not rely on any assumptions about the distribution and can generate real-like samples from latent space in a simple manner. This powerful property leads GAN to be applied to various applications such as image synthesis, image attribute editing, image translation, domain adaptation and other academic fields. In this paper, we aim to discuss the details of GAN for those readers who are familiar with, but do not comprehend GAN deeply or who wish to view GAN from various perspectives. In addition, we explain how GAN operates and the fundamental meaning of various objective functions that have been suggested recently. We then focus on how the GAN can be combined with an autoencoder framework. Finally, we enumerate the GAN variants that are applied to various tasks and other fields for those who are interested in exploiting GAN for their research.},
  number = {1},
  journaltitle = {ACM Computing Surveys},
  urldate = {2019-04-08},
  date = {2019-02-13},
  pages = {1-43},
  keywords = {Computer Science - Machine Learning},
  author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
  file = {/usr/stud/brechet/Zotero/storage/T3FBC695/Hong et al. - 2019 - How Generative Adversarial Networks and Their Vari.pdf;/usr/stud/brechet/Zotero/storage/UE7WE8I4/1711.html}
}

@book{Billingsley1995,
  location = {{New York [u.a.]}},
  title = {Probability and Measure},
  edition = {3. ed.},
  isbn = {978-0-471-00710-4},
  pagetotal = {XII, 593 S. :},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {{Wiley}},
  date = {1995},
  author = {Billingsley, Patrick},
  file = {/usr/stud/brechet/Zotero/storage/HFWPJB6G/singleHit.html}
}

@book{Hansen2009,
  location = {{Copenhagen}},
  title = {Measure Theory},
  edition = {Fourth edition},
  isbn = {978-87-91927-44-7},
  pagetotal = {xiv, 600 Seiten :},
  publisher = {{Department of Mathematical Sciences, University of Copenhagen}},
  date = {2009},
  author = {Hansen, Ernst},
  file = {/usr/stud/brechet/Zotero/storage/ABCMQPGM/singleHit.html}
}

@article{Peyre2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1803.00567},
  primaryClass = {stat},
  langid = {english},
  title = {Computational {{Optimal Transport}}},
  url = {http://arxiv.org/abs/1803.00567},
  abstract = {Optimal transport (OT) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in OT cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of OT in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, OT is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews OT with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of OT that make it particularly useful for some of these applications.},
  urldate = {2019-04-11},
  date = {2018-03-01},
  keywords = {Statistics - Machine Learning},
  author = {Peyré, Gabriel and Cuturi, Marco},
  file = {/usr/stud/brechet/Zotero/storage/D2ARJUIW/Peyré and Cuturi - 2018 - Computational Optimal Transport.pdf}
}

@book{Villani2009,
  location = {{Berlin [u.a.]}},
  title = {Optimal Transport},
  isbn = {978-3-540-71049-3},
  pagetotal = {XXII, 973 S. :},
  series = {Grundlehren Der Mathematischen {{Wissenschaften}}},
  publisher = {{Springer}},
  date = {2009},
  author = {Villani, Cédric},
  file = {/usr/stud/brechet/Zotero/storage/ESVBLR5V/singleHit.html}
}

@article{Kingma2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6114},
  primaryClass = {cs, stat},
  title = {Auto-{{Encoding Variational Bayes}}},
  url = {http://arxiv.org/abs/1312.6114},
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  urldate = {2019-04-14},
  date = {2013-12-20},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Kingma, Diederik P. and Welling, Max},
  file = {/usr/stud/brechet/Zotero/storage/ETHUBA6Z/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf;/usr/stud/brechet/Zotero/storage/UILUFME6/1312.html}
}

@article{Radford2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1511.06434},
  primaryClass = {cs},
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1511.06434},
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  urldate = {2019-04-14},
  date = {2015-11-19},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  file = {/usr/stud/brechet/Zotero/storage/MN4DJZDB/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf;/usr/stud/brechet/Zotero/storage/QM82I8MU/1511.html}
}

@article{Rubenstein2018,
  title = {Learning {{Disentangled Representations}} with {{Wasserstein Auto}}-{{Encoders}}},
  url = {https://openreview.net/forum?id=Hy79-UJPM},
  abstract = {We apply Wasserstein auto-encoders (WAEs) to the problem of disentangled representation learning. We highlight the potential of WAEs with promising results on a benchmark disentanglement task.},
  urldate = {2019-04-17},
  date = {2018-02-12},
  author = {Rubenstein, Paul K. and Schoelkopf, Bernhard and Tolstikhin, Ilya},
  file = {/usr/stud/brechet/Zotero/storage/KREA7ACJ/Rubenstein et al. - 2018 - Learning Disentangled Representations with Wassers.pdf;/usr/stud/brechet/Zotero/storage/WEBF9CRP/forum.html}
}

@article{Loshchilov2018,
  title = {Decoupled {{Weight Decay Regularization}}},
  url = {https://openreview.net/forum?id=Bkg6RiCqY7},
  abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is \textbackslash{}emph\{not\} the case...},
  urldate = {2019-04-17},
  date = {2018-09-27},
  author = {Loshchilov, Ilya and Hutter, Frank},
  file = {/usr/stud/brechet/Zotero/storage/LHLP5KEG/Loshchilov and Hutter - 2018 - Decoupled Weight Decay Regularization.pdf;/usr/stud/brechet/Zotero/storage/5RKGW2L7/forum.html}
}

@article{Kulkarni2015,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1503.03167},
  primaryClass = {cs},
  title = {Deep {{Convolutional Inverse Graphics Network}}},
  url = {http://arxiv.org/abs/1503.03167},
  abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
  urldate = {2019-04-17},
  date = {2015-03-11},
  keywords = {Computer Science - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Graphics},
  author = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  file = {/usr/stud/brechet/Zotero/storage/IVEQLKPY/Kulkarni et al. - 2015 - Deep Convolutional Inverse Graphics Network.pdf;/usr/stud/brechet/Zotero/storage/5YK62PB8/1503.html}
}

@article{Kim2018,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1802.05983},
  primaryClass = {cs, stat},
  title = {Disentangling by {{Factorising}}},
  url = {http://arxiv.org/abs/1802.05983},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon \$\textbackslash{}beta\$-VAE by providing a better trade-off between disentanglement and reconstruction quality. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  urldate = {2019-04-17},
  date = {2018-02-16},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Kim, Hyunjik and Mnih, Andriy},
  file = {/usr/stud/brechet/Zotero/storage/ZA7XUCZ6/Kim and Mnih - 2018 - Disentangling by Factorising.pdf;/usr/stud/brechet/Zotero/storage/NUQTLPCL/1802.html}
}

@article{Ghadimi2013,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1309.5549},
  primaryClass = {cs, math, stat},
  title = {Stochastic {{First}}- and {{Zeroth}}-Order {{Methods}} for {{Nonconvex Stochastic Programming}}},
  url = {http://arxiv.org/abs/1309.5549},
  abstract = {In this paper, we introduce a new stochastic approximation (SA) type algorithm, namely the randomized stochastic gradient (RSG) method, for solving an important class of nonlinear (possibly nonconvex) stochastic programming (SP) problems. We establish the complexity of this method for computing an approximate stationary point of a nonlinear programming problem. We also show that this method possesses a nearly optimal rate of convergence if the problem is convex. We discuss a variant of the algorithm which consists of applying a post-optimization phase to evaluate a short list of solutions generated by several independent runs of the RSG method, and show that such modification allows to improve significantly the large-deviation properties of the algorithm. These methods are then specialized for solving a class of simulation-based optimization problems in which only stochastic zeroth-order information is available.},
  urldate = {2019-04-17},
  date = {2013-09-21},
  keywords = {Statistics - Machine Learning,Computer Science - Computational Complexity,Mathematics - Optimization and Control},
  author = {Ghadimi, Saeed and Lan, Guanghui},
  file = {/usr/stud/brechet/Zotero/storage/AC3TQWQ8/Ghadimi and Lan - 2013 - Stochastic First- and Zeroth-order Methods for Non.pdf;/usr/stud/brechet/Zotero/storage/K4XPUVCT/1309.html}
}

@article{Redko,
  langid = {english},
  title = {Optimal {{Transport}} for {{Multi}}-Source {{Domain Adaptation}} under {{Target Shift}}},
  pages = {10},
  author = {Redko, Ievgen and Courty, Nicolas and Flamary, Rémi and Tuia, Devis},
  file = {/usr/stud/brechet/Zotero/storage/9HBIXMZ6/Redko et al. - Optimal Transport for Multi-source Domain Adaptati.pdf}
}

@article{Janati,
  langid = {english},
  title = {Wasserstein Regularization for Sparse Multi-Task Regression},
  pages = {30},
  author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
  file = {/usr/stud/brechet/Zotero/storage/A8CEK9QL/Janati et al. - Wasserstein regularization for sparse multi-task r.pdf}
}

@article{Rowland,
  langid = {english},
  title = {Orthogonal {{Estimation}} of {{Wasserstein Distances}}},
  abstract = {Wasserstein distances are increasingly used in a wide variety of applications in machine learning. Sliced Wasserstein distances form an important subclass which may be estimated eﬃciently through one-dimensional sorting operations. In this paper, we propose a new variant of sliced Wasserstein distance, study the use of orthogonal coupling in Monte Carlo estimation of Wasserstein distances and draw connections with stratiﬁed sampling, and evaluate our approaches experimentally in a range of large-scale experiments in generative modelling and reinforcement learning.},
  pages = {17},
  author = {Rowland, Mark and Hron, Jiri and Tang, Yunhao and Choromanski, Krzysztof and Sarlos, Tamas and Weller, Adrian},
  file = {/usr/stud/brechet/Zotero/storage/799FXLQQ/Rowland et al. - Orthogonal Estimation of Wasserstein Distances.pdf}
}

@article{Genevay,
  langid = {english},
  title = {Sample {{Complexity}} of {{Sinkhorn Divergences}}},
  pages = {25},
  author = {Genevay, Aude and Chizat, Lénaic and Bach, Francis and Cuturi, Marco and Peyré, Gabriel},
  file = {/usr/stud/brechet/Zotero/storage/HHG5WRC5/Genevay et al. - Sample Complexity of Sinkhorn Divergences.pdf}
}

@article{Alvarez-Melis,
  langid = {english},
  title = {Towards {{Optimal Transport}} with {{Global Invariances}}},
  abstract = {Many problems in machine learning involve calculating correspondences between sets of objects, such as point clouds or images. Discrete optimal transport provides a natural and successful approach to such tasks whenever the two sets of objects can be represented in the same space, or at least distances between them can be directly evaluated. Unfortunately neither requirement is likely to hold when object representations are learned from data. Indeed, automatically derived representations such as word embeddings are typically ﬁxed only up to some global transformations, for example, reﬂection or rotation. As a result, pairwise distances across two such instances are ill-deﬁned without specifying their relative transformation. In this work, we propose a general framework for optimal transport in the presence of latent global transformations. We cast the problem as a joint optimization over transport couplings and transformations chosen from a ﬂexible class of invariances, propose algorithms to solve it, and show promising results in various tasks, including a popular unsupervised word translation benchmark.},
  pages = {13},
  author = {Alvarez-Melis, David and Jegelka, Stefanie and Jaakkola, Tommi S},
  file = {/usr/stud/brechet/Zotero/storage/774S6VQ9/Alvarez-Melis et al. - Towards Optimal Transport with Global Invariances.pdf}
}

@article{Forrow,
  langid = {english},
  title = {Statistical {{Optimal Transport}} via {{Factored Couplings}}},
  abstract = {We propose a new method to estimate Wasserstein distances and optimal transport plans between two probability distributions from samples in high dimension. Unlike plugin rules that simply replace the true distributions by their empirical counterparts, our method promotes couplings with low transport rank, a new structural assumption that is similar to the nonnegative rank of a matrix. Regularizing based on this assumption leads to drastic improvements on highdimensional data for various tasks, including domain adaptation in single-cell RNA sequencing data. These ﬁndings are supported by a theoretical analysis that indicates that the transport rank is key in overcoming the curse of dimensionality inherent to datadriven optimal transport.},
  pages = {19},
  author = {Forrow, Aden and Hutter, Jan-Christian and Nitzan, Mor and Rigollet, Philippe and Schiebinger, Geoﬀrey},
  file = {/usr/stud/brechet/Zotero/storage/F6FBGFC5/Forrow et al. - Statistical Optimal Transport via Factored Couplin.pdf}
}

@article{Dessein2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1610.06447},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Regularized {{Optimal Transport}} and the {{Rot Mover}}'s {{Distance}}},
  url = {http://arxiv.org/abs/1610.06447},
  abstract = {This paper presents a uniﬁed framework for smooth convex regularization of discrete optimal transport problems. In this context, the regularized optimal transport turns out to be equivalent to a matrix nearness problem with respect to Bregman divergences. Our framework thus naturally generalizes a previously proposed regularization based on the Boltzmann-Shannon entropy related to the Kullback-Leibler divergence, and solved with the Sinkhorn-Knopp algorithm. We call the regularized optimal transport distance the rot mover’s distance in reference to the classical earth mover’s distance. By exploiting alternate Bregman projections, we develop the alternate scaling algorithm and non-negative alternate scaling algorithm, to compute eﬃciently the regularized optimal plans depending on whether the domain of the regularizer lies within the non-negative orthant or not. We further enhance the separable case with a sparse extension to deal with high data dimensions. We also instantiate our framework and discuss the inherent speciﬁcities for well-known regularizers and statistical divergences in the machine learning and information geometry communities. Finally, we demonstrate the merits of our methods with experiments using synthetic data to illustrate the eﬀect of diﬀerent regularizers, penalties and dimensions, as well as real-world data for a pattern recognition application to audio scene classiﬁcation.},
  urldate = {2019-04-29},
  date = {2016-10-20},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Dessein, Arnaud and Papadakis, Nicolas and Rouas, Jean-Luc},
  file = {/usr/stud/brechet/Zotero/storage/WJ8HZJAB/Dessein et al. - 2016 - Regularized Optimal Transport and the Rot Mover's .pdf}
}

@article{Essid2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1704.08200},
  primaryClass = {cs, math},
  langid = {english},
  title = {Quadratically-{{Regularized Optimal Transport}} on {{Graphs}}},
  url = {http://arxiv.org/abs/1704.08200},
  abstract = {Optimal transportation provides a means of lifting distances between points on a geometric domain to distances between signals over the domain, expressed as probability distributions. On a graph, transportation problems can be used to express challenging tasks involving matching supply to demand with minimal shipment expense; in discrete language, these become minimumcost network ﬂow problems. Regularization typically is needed to ensure uniqueness for the linear ground distance case and to improve optimization convergence; state-of-the-art techniques employ entropic regularization on the transportation matrix. In this paper, we explore a quadratic alternative to entropic regularization for transport over a graph. We theoretically analyze the behavior of quadratically-regularized graph transport, characterizing how regularization aﬀects the structure of ﬂows in the regime of small but nonzero regularization. We further exploit elegant second-order structure in the dual of this problem to derive an easily-implemented Newton-type optimization algorithm.},
  urldate = {2019-04-29},
  date = {2017-04-26},
  keywords = {Mathematics - Optimization and Control,Computer Science - Numerical Analysis,Mathematics - Numerical Analysis},
  author = {Essid, Montacer and Solomon, Justin},
  file = {/usr/stud/brechet/Zotero/storage/GLFA2HN3/Essid and Solomon - 2017 - Quadratically-Regularized Optimal Transport on Gra.pdf}
}

@article{lei2019,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1902.02934},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Mode {{Collapse}} and {{Regularity}} of {{Optimal Transportation Maps}}},
  url = {http://arxiv.org/abs/1902.02934},
  abstract = {This work builds the connection between the regularity theory of optimal transportation map, MongeAmpère equation and GANs, which gives a theoretic understanding of the major drawbacks of GANs: convergence difﬁculty and mode collapse.},
  urldate = {2019-05-03},
  date = {2019-02-07},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {{lei}, Na and Guo, Yang and An, Dongsheng and Qi, Xin and Luo, Zhongxuan and Yau, Shing-Tung and Gu, Xianfeng},
  file = {/usr/stud/brechet/Zotero/storage/CZBNPM37/lei et al. - 2019 - Mode Collapse and Regularity of Optimal Transporta.pdf}
}

@article{Huemer2013,
  langid = {english},
  title = {{{AN ONTOLOGICAL PROOF OF MORAL REALISM}}},
  volume = {30},
  issn = {0265-0525, 1471-6437},
  url = {https://www.cambridge.org/core/product/identifier/S0265052513000125/type/journal_article},
  doi = {10.1017/S0265052513000125},
  number = {1-2},
  journaltitle = {Social Philosophy and Policy},
  urldate = {2019-05-04},
  date = {2013-01},
  pages = {259-279},
  author = {Huemer, Michael},
  file = {/usr/stud/brechet/Zotero/storage/YEYL774N/Huemer - 2013 - AN ONTOLOGICAL PROOF OF MORAL REALISM.pdf}
}

@book{Huemer2019,
  langid = {english},
  title = {Dialogues on {{Ethical Vegetarianism}}},
  edition = {1},
  isbn = {978-1-138-32830-3},
  url = {https://www.taylorfrancis.com/books/9780429641176},
  publisher = {{Routledge}},
  urldate = {2019-05-04},
  date = {2019-03-27},
  author = {Huemer, Michael},
  file = {/usr/stud/brechet/Zotero/storage/LWX66BA4/Huemer - 2019 - Dialogues on Ethical Vegetarianism.pdf},
  doi = {10.4324/9781138328303}
}

@article{Lin2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1712.04086},
  primaryClass = {cs, math, stat},
  title = {{{PacGAN}}: {{The}} Power of Two Samples in Generative Adversarial Networks},
  url = {http://arxiv.org/abs/1712.04086},
  shorttitle = {{{PacGAN}}},
  abstract = {Generative adversarial networks (GANs) are innovative techniques for learning generative models of complex data distributions from samples. Despite remarkable recent improvements in generating realistic images, one of their major shortcomings is the fact that in practice, they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the main focus of several recent advances in GANs. Yet there is little understanding of why mode collapse happens and why existing approaches are able to mitigate mode collapse. We propose a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We borrow analysis tools from binary hypothesis testing---in particular the seminal result of Blackwell [Bla53]---to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggests that packing provides significant improvements in practice as well.},
  urldate = {2019-05-05},
  date = {2017-12-11},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Information Theory},
  author = {Lin, Zinan and Khetan, Ashish and Fanti, Giulia and Oh, Sewoong},
  file = {/usr/stud/brechet/Zotero/storage/ZN9B4MDU/Lin et al. - 2017 - PacGAN The power of two samples in generative adv.pdf;/usr/stud/brechet/Zotero/storage/DPSHMDEF/1712.html}
}

@article{Dumoulin2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1606.00704},
  primaryClass = {cs, stat},
  title = {Adversarially {{Learned Inference}}},
  url = {http://arxiv.org/abs/1606.00704},
  abstract = {We introduce the adversarially learned inference (ALI) model, which jointly learns a generation network and an inference network using an adversarial process. The generation network maps samples from stochastic latent variables to the data space while the inference network maps training examples in data space to the space of latent variables. An adversarial game is cast between these two networks and a discriminative network is trained to distinguish between joint latent/data-space samples from the generative network and joint samples from the inference network. We illustrate the ability of the model to learn mutually coherent inference and generation networks through the inspections of model samples and reconstructions and confirm the usefulness of the learned representations by obtaining a performance competitive with state-of-the-art on the semi-supervised SVHN and CIFAR10 tasks.},
  urldate = {2019-05-05},
  date = {2016-06-02},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Dumoulin, Vincent and Belghazi, Ishmael and Poole, Ben and Mastropietro, Olivier and Lamb, Alex and Arjovsky, Martin and Courville, Aaron},
  file = {/usr/stud/brechet/Zotero/storage/JQAXHB48/Dumoulin et al. - 2016 - Adversarially Learned Inference.pdf;/usr/stud/brechet/Zotero/storage/P59YSJWV/1606.html}
}

@article{Donahue2016,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1605.09782},
  primaryClass = {cs, stat},
  title = {Adversarial {{Feature Learning}}},
  url = {http://arxiv.org/abs/1605.09782},
  abstract = {The ability of the Generative Adversarial Networks (GANs) framework to learn generative models mapping from simple latent distributions to arbitrarily complex data distributions has been demonstrated empirically, with compelling results showing that the latent space of such generators captures semantic variation in the data distribution. Intuitively, models trained to predict these semantic latent representations given data may serve as useful feature representations for auxiliary problems where semantics are relevant. However, in their existing form, GANs have no means of learning the inverse mapping -- projecting data back into the latent space. We propose Bidirectional Generative Adversarial Networks (BiGANs) as a means of learning this inverse mapping, and demonstrate that the resulting learned feature representation is useful for auxiliary supervised discrimination tasks, competitive with contemporary approaches to unsupervised and self-supervised feature learning.},
  urldate = {2019-05-08},
  date = {2016-05-31},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Computer Science - Artificial Intelligence},
  author = {Donahue, Jeff and Krähenbühl, Philipp and Darrell, Trevor},
  file = {/usr/stud/brechet/Zotero/storage/PA9Q77D5/Donahue et al. - 2016 - Adversarial Feature Learning.pdf;/usr/stud/brechet/Zotero/storage/ALHGXSME/1605.html}
}

@article{Warde-Farley2017,
  langid = {english},
  title = {{{IMPROVING GENERATIVE ADVERSARIAL NETWORKS WITH DENOISING FEATURE MATCHING}}},
  abstract = {We propose an augmented training procedure for generative adversarial networks designed to address shortcomings of the original by directing the generator towards probable conﬁgurations of abstract discriminator features. We estimate and track the distribution of these features, as computed from data, with a denoising auto-encoder, and use it to propose high-level targets for the generator. We combine this new loss with the original and evaluate the hybrid criterion on the task of unsupervised image synthesis from datasets comprising a diverse set of visual categories, noting a qualitative and quantitative improvement in the “objectness” of the resulting samples.},
  date = {2017},
  pages = {11},
  author = {Warde-Farley, David and Bengio, Yoshua},
  file = {/usr/stud/brechet/Zotero/storage/G9SAZJBI/Warde-Farley and Bengio - 2017 - IMPROVING GENERATIVE ADVERSARIAL NETWORKS WITH DEN.pdf}
}

@article{Berthelot2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1703.10717},
  primaryClass = {cs, stat},
  langid = {english},
  title = {{{BEGAN}}: {{Boundary Equilibrium Generative Adversarial Networks}}},
  url = {http://arxiv.org/abs/1703.10717},
  shorttitle = {{{BEGAN}}},
  abstract = {We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure.},
  urldate = {2019-05-08},
  date = {2017-03-30},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Berthelot, David and Schumm, Thomas and Metz, Luke},
  file = {/usr/stud/brechet/Zotero/storage/NJU4EI87/Berthelot et al. - 2017 - BEGAN Boundary Equilibrium Generative Adversarial.pdf}
}

@article{Shen2017,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1707.01217},
  primaryClass = {cs, stat},
  langid = {english},
  title = {Wasserstein {{Distance Guided Representation Learning}} for {{Domain Adaptation}}},
  url = {http://arxiv.org/abs/1707.01217},
  abstract = {Domain adaptation aims at generalizing a high-performance learner on a target domain via utilizing the knowledge distilled from a source domain which has a different but related data distribution. One solution to domain adaptation is to learn domain invariant feature representations while the learned representations should also be discriminative in prediction. To learn such representations, domain adaptation frameworks usually include a domain invariant representation learning approach to measure and reduce the domain discrepancy, as well as a discriminator for classiﬁcation. Inspired by Wasserstein GAN, in this paper we propose a novel approach to learn domain invariant feature representations, namely Wasserstein Distance Guided Representation Learning (WDGRL). WDGRL utilizes a neural network, denoted by the domain critic, to estimate empirical Wasserstein distance between the source and target samples and optimizes the feature extractor network to minimize the estimated Wasserstein distance in an adversarial manner. The theoretical advantages of Wasserstein distance for domain adaptation lie in its gradient property and promising generalization bound. Empirical studies on common sentiment and image classiﬁcation adaptation datasets demonstrate that our proposed WDGRL outperforms the state-of-the-art domain invariant representation learning approaches.},
  urldate = {2019-06-03},
  date = {2017-07-05},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  author = {Shen, Jian and Qu, Yanru and Zhang, Weinan and Yu, Yong},
  file = {/usr/stud/brechet/Zotero/storage/A6JYP32H/Shen et al. - 2017 - Wasserstein Distance Guided Representation Learnin.pdf}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

