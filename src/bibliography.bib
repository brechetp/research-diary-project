@article{Kitagawa2016,
	title = {Convergence of a Newton algorithm for semi-discrete optimal transport},
	url = {http://arxiv.org/abs/1603.05579},
	abstract = {Many problems in geometric optics or convex geometry can be recast as optimal transport problems: this includes the far-field reflector problem, Alexandrov's curvature prescription problem, etc. A popular way to solve these problems numerically is to assume that the source probability measure is absolutely continuous while the target measure is finitely supported. We refer to this setting as semi-discrete optimal transport. Among the several algorithms proposed to solve semi-discrete optimal transport problems, one currently needs to choose between algorithms that are slow but come with a convergence speed analysis (e.g. Oliker-Prussner) or algorithms that are much faster in practice but which come with no convergence guarantees Algorithms of the first kind rely on coordinate-wise increments and the number of iterations required to reach the solution up to an error of \${\textbackslash}epsilon\$ is of order \$N{\textasciicircum}3/{\textbackslash}epsilon\$, where \$N\$ is the number of Dirac masses in the target measure. On the other hand, algorithms of the second kind typically rely on the formulation of the semi-discrete optimal transport problem as an unconstrained convex optimization problem which is solved using a Newton or quasi-Newton method. The purpose of this article is to bridge this gap between theory and practice by introducing a damped Newton's algorithm which is experimentally efficient and by proving the global convergence of this algorithm with optimal rates. The main assumptions is that the cost function satisfies a condition that appears in the regularity theory for optimal transport (the Ma-Trudinger-Wang condition) and that the support of the source density is connected in a quantitative way (it must satisfy a weighted Poincar{\textbackslash}'e-Wirtinger inequality).},
	pages = {1--45},
	author = {Kitagawa, Jun and Mérigot, Quentin and Thibert, Boris},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1603.05579},
	note = {bibtex*[arxivid=1603.05579] },
	file = {Kitagawa, Mérigot, Thibert - 2016 - Convergence of a Newton algorithm for semi-discrete optimal transport:/usr/stud/brechet/Zotero/storage/HX25QGCH/Kitagawa, Mérigot, Thibert - 2016 - Convergence of a Newton algorithm for semi-discrete optimal transport.pdf:application/pdf}
}

@article{Potash2016a,
	title = {Recommender system incorporating user personality profile through analysis of written reviews},
	volume = {1680},
	issn = {16130073},
	url = {http://arxiv.org/abs/1704.00028},
	doi = {10.1016/j.aqpro.2013.07.003},
	abstract = {The cereal and oilseed trade is a necessary contingency for assuring world food security as we restore ecological health to the planet. In a more populous and resource stressed world, trade's role will grow – guiding producers and consumers in their choices and facilitating optimal use of natural resources. Greater water efficiency will be a fundamental part of this. The right policies, farm entrepreneurship, conscientious use of land and other resources, knowledge and infrastructure also are required.},
	pages = {60--66},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Potash, Peter and Rumshisky, Anna},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {arXiv:1011.1669v3},
	note = {bibtex*[isbn=0030-8870;arxivid={arXiv}:1011.1669v3;pmid=24439530] },
	keywords = {Collaborative filtering, Human-centered computing, Recommender systems, Social networks}
}

@article{Vaswani2017,
	title = {Attention Is All You Need},
	issn = {0140-525X},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.1017/S0140525X16001837},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.8 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	issue = {Nips},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1706.03762},
	note = {bibtex*[isbn=9781577357384;arxivid=1706.03762;pmid=1000303116] }
}

@article{Rubner2000a,
	title = {Earth mover's distance as a metric for image retrieval},
	volume = {40},
	issn = {09205691},
	doi = {10.1023/A:1026543900054},
	abstract = {We investigate the properties of a metric between two distributions, the Earth Mover's Distance ({EMD}), for content-based image retrieval. The {EMD} is based on the minimal cost that must be paid to transform one dis-tribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The {EMD} is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length represen-tations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the {EMD} is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the {EMD} with that of other distances.},
	pages = {99--121},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
	date = {2000},
	eprinttype = {arxiv},
	eprint = {0005074v1 [arXiv:astro-ph]},
	note = {bibtex*[isbn=0920-5691;arxivid={arXiv}:astro-ph/0005074v1;pmid=1284] },
	keywords = {color, earth mover, image retrieval, perceptual metrics, s distance, texture}
}

@article{Chan2011,
	title = {Math 172 : Lebesgue Integration and Fourier Analysis},
	author = {Chan, Charlotte},
	date = {2011}
}

@article{Matrices2000,
	title = {Relative Entropy},
	pages = {13--55},
	issue = {x},
	author = {Matrices, Weight and Example, Simple Site},
	date = {2000}
}

@article{Martens2012,
	title = {On the importance of initialization and momentum in deep learning},
	number = {2010},
	author = {Martens, James},
	date = {2012}
}

@article{B.Ash1990,
	title = {Information theory‎},
	url = {http://books.google.com/books?id=yZ1JZA6Wo6YC{&}printsec=frontcover{%}5Cnfile:///Users/Brian{_}Caudle/Documents/Papers/1990/B. Ash/1990 B. Ash.pdf{%}5Cnpapers://75088281-f09c-4e77-b8c2-14041f4545f6/Paper/p344},
	abstract = {Excellent introduction treats 3 major areas: analysis of channel models and proof of coding theorems; study of specific coding systems; and study of statistical ...},
	pages = {339},
	author = {B. Ash, R},
	date = {1990},
	keywords = {Computers}
}

@article{Carlier,
	title = {Convergence of Entropic Schemes for},
	pages = {1--28},
	author = {Carlier, Guillaume and Duval, Vincent and Schmitzer, Bernhard},
	eprinttype = {arxiv},
	eprint = {arXiv:1512.02783v2},
	note = {bibtex*[arxivid={arXiv}:1512.02783v2] }
}

@book{Gray2011,
	title = {Entropy and information theory},
	volume = {1},
	isbn = {978-1-4419-7969-8},
	abstract = {This book is devoted to the theory of probabilistic information measures and their application to coding theorems for information sources and noisy channels. The eventual goal is a general development of Shannon's mathematical theory of communication, but much of the space is devoted to the tools and methods required to prove the Shannon coding theorems. These tools form an area common to ergodic theory and information theory and comprise several quantitative notions of the information in random variables, random processes, and dynamical systems. Examples are entropy, mutual information, conditional entropy, conditional information, and discrimination or relative entropy, along with the limiting normalized versions of these quantities such as entropy rate and information rate. Much of the book is concerned with their properties, especially the long term asymptotic behavior of sample information and expected information. This is the only up-to-date treatment of traditional information theory emphasizing ergodic theory.},
	pagetotal = {1-409},
	author = {Gray, Robert M.},
	date = {2011},
	doi = {10.1007/978-1-4419-7970-4},
	eprinttype = {arxiv},
	eprint = {9809069v1 [arXiv:gr-qc]},
	note = {bibtex*[booktitle=Entropy and Information Theory;arxivid={arXiv}:gr-qc/9809069v1;pmid=19695087]
{ISSN}: 0308518X },
	file = {Robert M. Gray (auth.)-Entropy and Information Theory-Springer US (2011):/usr/stud/brechet/Zotero/storage/ALMBBP5C/Robert M. Gray (auth.)-Entropy and Information Theory-Springer US (2011).pdf:application/pdf}
}

@article{Rubner2000,
	title = {Earth mover's distance as a metric for image retrieval},
	volume = {40},
	issn = {09205691},
	url = {http://www.springerlink.com/index/W5515K817681125H.pdf},
	doi = {10.1023/A:1026543900054},
	abstract = {We investigate the properties of a metric between two distributions, the Earth Mover's Distance ({EMD}), for content-based image retrieval. The {EMD} is based on the minimal cost that must be paid to transform one dis-tribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The {EMD} is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length represen-tations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the {EMD} is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the {EMD} with that of other distances.},
	pages = {99--121},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
	date = {2000},
	eprinttype = {arxiv},
	eprint = {0005074v1 [arXiv:astro-ph]},
	note = {bibtex*[isbn=0920-5691;arxivid={arXiv}:astro-ph/0005074v1;pmid=1284] },
	keywords = {color, earth mover, image retrieval, perceptual metrics, s distance, texture}
}

@article{Genevay2017,
	title = {{GAN} and {VAE} from an Optimal Transport Point of View},
	url = {http://arxiv.org/abs/1706.01807},
	abstract = {This short article revisits some of the ideas introduced in {arXiv}:1701.07875 and {arXiv}:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders ({VAE}), Generative Adversarial Networks ({GAN}) and Minimum Kantorovitch Estimators ({MKE}).},
	pages = {1--6},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1706.01807},
	note = {bibtex*[arxivid=1706.01807] },
	file = {Genevay, Peyré, Cuturi - 2017 - GAN and VAE from an Optimal Transport Point of View:/usr/stud/brechet/Zotero/storage/E3EPRY6A/Genevay, Peyré, Cuturi - 2017 - GAN and VAE from an Optimal Transport Point of View.pdf:application/pdf}
}

@article{Mescheder2018,
	title = {Which Training Methods for {GANs} do actually Converge?},
	issn = {1938-7228},
	url = {http://arxiv.org/abs/1801.04406},
	abstract = {Recent work has shown local convergence of {GAN} training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized {GAN} training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize {GAN} training. Our analysis shows that {GAN} training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-{GANs} and {WGAN}-{GP} with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of {GAN} training. Based on our analysis, we extend our convergence results to more general {GANs} and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning.},
	author = {Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1801.04406},
	note = {bibtex*[isbn=1938-7228;arxivid=1801.04406] }
}

@article{Edition2018,
	title = {Inequalities in},
	pages = {657--687},
	issue = {March},
	author = {Edition, Second},
	date = {2018}
}

@article{Azim2018,
	title = {Abiotic stresses induce total phenolic, total flavonoid and antioxidant properties in Malaysian indigenous microalgae and cyanobacterium},
	volume = {14},
	issn = {18238262},
	doi = {10.1017/CBO9781107415324.004},
	abstract = {This study investigated the ability of bacteriocins isolated from Bacillus spp. (Bacillus species) to inhibit four different yeast isolates obtained from common food products (nono, yoghurt, ogi and cheese) commonly consumed by Nigerians with minimal heat treatment. Forty-five Bacillus spp. was isolated and identified from common food products using cultural, morphological, physiological and biochemical characteristics. These isolates were tested for antimicrobial activity against Salmonella enteritidis (3), Micrococcus luteus (1) and Staphylococcus aureus (2). Eight bacteriocin producing strains were identified from an over- night broth culture centrifugated at 3500 revolutions for five minutes. Fungicidal effects of these bacteriocins were tested against four yeast strains using the Agar Well Diffusion method. The bacteriocins produced wide zones of inhibition ranging from 5.9±0.000 to 24.00±0.000 mm against the 4 yeast strains tested. There was a significant difference (at p0.05) between the yeast organisms and the bacteriocins from the Bacillus spp. The study reveals the antifungal property of bacteriocins from Bacillus spp. and serves therefore as a base for further studies in its use in the control of diseases and extension of shelf-life of products prone to fungi contamination.},
	pages = {25--33},
	number = {1},
	journaltitle = {Malaysian Journal of Microbiology},
	author = {Azim, Nur Husna and Subki, Atiqah and Yusof, Zetty Norhana Balia},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {arXiv:1011.1669v3},
	note = {bibtex*[isbn=9788578110796;arxivid={arXiv}:1011.1669v3;pmid=25246403] },
	keywords = {Antioxidant, Cyanobacteria, Microalgae, Stress}
}

@article{Courty2017a,
	title = {Joint Distribution Optimal Transportation for Domain Adaptation},
	issn = {0162-8828},
	url = {http://arxiv.org/abs/1705.08848},
	doi = {10.1109/TPAMI.2016.2615921},
	abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \${\textbackslash}mathcal\{P\}\_s\$ and \${\textbackslash}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \${\textbackslash}mathcal\{P\}{\textasciicircum}f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
	issue = {Nips},
	author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.08848},
	note = {bibtex*[isbn=9782875870148;arxivid=1705.08848] }
}

@article{Konur2013,
	title = {The scientometric evaluation of the institutional research: The Marmara Universities-Part 4},
	volume = {5},
	issn = {13087711},
	doi = {10.1063/1.4902458},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order mo-ments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpre-tations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical con-vergence properties of the algorithm and provide a regret bound on the conver-gence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	pages = {365--380},
	number = {2},
	journaltitle = {Energy Education Science and Technology Part B: Social and Educational Studies},
	author = {Konur, Ozcan},
	date = {2013},
	eprinttype = {arxiv},
	eprint = {1412.6980v9},
	note = {bibtex*[isbn=9780735412705;arxivid=1412.6980v9] },
	keywords = {{AandHCI}, Higher education spending, Incentive structures, Marmara Universities, Research evaluation, Research productivity, {SCIE}, Scientometrics, {SSCI}, Turkey, Web of Knowledge}
}

@article{Larochelle2011,
	title = {The Neural Autoregressive Distribution Estimator},
	volume = {15},
	pages = {29--37},
	author = {Larochelle, Hugo and Murray, Iain},
	date = {2011}
}

@article{Rubner2000b,
	title = {Earth mover's distance as a metric for image retrieval},
	volume = {40},
	issn = {09205691},
	url = {http://www.springerlink.com/index/W5515K817681125H.pdf},
	doi = {10.1023/A:1026543900054},
	abstract = {We investigate the properties of a metric between two distributions, the Earth Mover's Distance ({EMD}), for content-based image retrieval. The {EMD} is based on the minimal cost that must be paid to transform one dis-tribution into the other, in a precise sense, and was first proposed for certain vision problems by Peleg, Werman, and Rom. For image retrieval, we combine this idea with a representation scheme for distributions that is based on vector quantization. This combination leads to an image comparison framework that often accounts for perceptual similarity better than other previously proposed methods. The {EMD} is based on a solution to the transportation problem from linear optimization, for which efficient algorithms are available, and also allows naturally for partial matching. It is more robust than histogram matching techniques, in that it can operate on variable-length represen-tations of the distributions that avoid quantization and other binning problems typical of histograms. When used to compare distributions with the same overall mass, the {EMD} is a true metric. In this paper we focus on applications to color and texture, and we compare the retrieval performance of the {EMD} with that of other distances.},
	pages = {99--121},
	number = {2},
	journaltitle = {International Journal of Computer Vision},
	author = {Rubner, Yossi and Tomasi, Carlo and Guibas, Leonidas J.},
	date = {2000},
	eprinttype = {arxiv},
	eprint = {0005074v1 [arXiv:astro-ph]},
	note = {bibtex*[isbn=0920-5691;arxivid={arXiv}:astro-ph/0005074v1;pmid=1284] },
	keywords = {color, earth mover, image retrieval, perceptual metrics, s distance, texture}
}

@article{Montavon2015,
	title = {Wasserstein Training of Boltzmann Machines},
	issn = {10495258},
	url = {http://arxiv.org/abs/1507.01972},
	abstract = {The Boltzmann machine provides a useful framework to learn highly complex, multimodal and multiscale data distributions that occur in the real world. The default method to learn its parameters consists of minimizing the Kullback-Leibler ({KL}) divergence from training samples to the Boltzmann model. We propose in this work a novel approach for Boltzmann training which assumes that a meaningful metric between observations is given. This metric can be represented by the Wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters. Minimization of this new Wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure. We demonstrate the practical potential of these models for data completion and denoising, for which the metric between observations plays a crucial role.},
	pages = {1--9},
	number = {1},
	author = {Montavon, Grégoire and Müller, Klaus-Robert and Cuturi, Marco},
	date = {2015},
	eprinttype = {arxiv},
	eprint = {1507.01972},
	note = {bibtex*[arxivid=1507.01972] }
}

@article{Bertsimas1997,
	title = {Introduction to linear optimization},
	url = {http://scholar.google.com/scholar?hl=en{&}btnG=Search{&}q=intitle:Introduction+to+Linear+Optimization{#}0},
	author = {Bertsimas, D and Tsitsiklis, J N},
	date = {1997},
	note = {bibtex*[booktitle=Imamu.Edu.Sa]}
}

@article{Aude2016,
	title = {Stochastic Optimization for Large-scale Optimal Transport},
	issn = {10495258},
	url = {http://arxiv.org/abs/1605.08527},
	abstract = {Optimal transport ({OT}) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of {OT} is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale problems routinely encountered in machine learning applications. These methods are able to manipulate arbitrary distributions (either discrete or continuous) by simply requiring to be able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual {OT} problem can be re-cast as the maximization of an expectation ; (b) entropic regularization of the primal {OT} problem results in a smooth dual optimization optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional {OT} solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization ; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space ({RKHS}). This is currently the only known method to solve this problem, apart from computing {OT} on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.},
	pages = {1--12},
	issue = {Nips},
	journaltitle = {Nips},
	author = {Genevay, Aude and Cuturi, Marco and Peyré, Gabriel and Bach, Francis},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1605.08527},
	note = {bibtex*[arxivid=1605.08527] },
	file = {Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport:/usr/stud/brechet/Zotero/storage/W49LBC5I/Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport.pdf:application/pdf;Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport(2):/usr/stud/brechet/Zotero/storage/I5DEUTTL/Genevay et al. - 2016 - Stochastic Optimization for Large-scale Optimal Transport(2).pdf:application/pdf}
}

@article{Loshchilov2017,
	title = {{SGDR}: Stochastic Gradient Descent with Warm Restarts},
	issn = {15826163},
	url = {http://arxiv.org/abs/1608.03983},
	doi = {10.1002/fut},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the {CIFAR}-10 and {CIFAR}-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of {EEG} recordings and on a downsampled version of the {ImageNet} dataset. Our source code is available at https://github.com/loshchil/{SGDR}},
	pages = {1--16},
	author = {Loshchilov, Ilya and Hutter, Frank},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1608.03983},
	note = {bibtex*[isbn=978-0-674-02343-7;arxivid=1608.03983;pmid=87370679] }
}

@article{Lim2017,
	title = {Geometric {GAN}},
	url = {http://arxiv.org/abs/1705.02894},
	abstract = {Generative Adversarial Nets ({GANs}) represent an important milestone for effective generative models, which has inspired numerous variants seemingly different from each other. One of the main contributions of this paper is to reveal a unified geometric structure in {GAN} and its variants. Specifically, we show that the adversarial generative model training can be decomposed into three geometric steps: separating hyperplane search, discriminator parameter update away from the separating hyperplane, and the generator update along the normal vector direction of the separating hyperplane. This geometric intuition reveals the limitations of the existing approaches and leads us to propose a new formulation called geometric {GAN} using {SVM} separating hyperplane that maximizes the margin. Our theoretical analysis shows that the geometric {GAN} converges to a Nash equilibrium between the discriminator and generator. In addition, extensive numerical results show that the superior performance of geometric {GAN}.},
	pages = {1--17},
	issue = {Mmd},
	author = {Lim, Jae Hyun and Ye, Jong Chul},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.02894},
	note = {bibtex*[arxivid=1705.02894] }
}

@article{Sjolund2013,
	title = {Gaussian channel},
	pages = {1--26},
	issue = {May},
	author = {Sjölund, Jens},
	date = {2013}
}

@book{Edition2006,
	title = {Kolmogorov complexity},
	isbn = {0-01-001111-0},
	pagetotal = {463-508},
	author = {Edition, Second},
	date = {2006}
}

@article{Hardt2016,
	title = {Stability of stochastic gradient descent},
	pages = {1--32},
	author = {Hardt, Moritz and Recht, Benjamin},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {arXiv:1509.01240v2},
	note = {bibtex*[arxivid={arXiv}:1509.01240v2] }
}

@article{Capacity2012,
	title = {Channel Capacity},
	pages = {1--132},
	issue = {November},
	author = {Capacity, Channel},
	date = {2012},
	note = {bibtex*[isbn=9780470825617]}
}

@book{Kurdila2005,
	location = {Basel ; Boston},
	title = {Convex functional analysis},
	isbn = {978-0-8176-2198-8 978-3-7643-2198-7},
	series = {Systems \& control},
	pagetotal = {228},
	publisher = {Birkhauser Verlag},
	author = {Kurdila, Andrew and Zabarankin, Michael},
	date = {2005},
	keywords = {Convex functions, Existence theorems, Functional analysis, Mathematical optimization}
}

@article{Goodfellow2014,
	title = {Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1406.2661},
	abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
	journaltitle = {{arXiv}:1406.2661 [cs, stat]},
	author = {Goodfellow, Ian J. and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	urldate = {2019-01-08},
	date = {2014-06-10},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1406.2661},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:/usr/stud/brechet/Zotero/storage/UH5DPN9T/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf}
}

@article{Mescheder2017a,
	title = {Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1701.04722},
	shorttitle = {Adversarial Variational Bayes},
	abstract = {Variational Autoencoders ({VAEs}) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes ({AVB}), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihoodproblem as a two-player game, hence establishing a principled connection between {VAEs} and Generative Adversarial Networks ({GANs}). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine {VAEs} with {GANs}, our approach has a clear theoretical justiﬁcation, retains most advantages of standard Variational Autoencoders and is easy to implement.},
	journaltitle = {{arXiv}:1701.04722 [cs]},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	urldate = {2019-01-08},
	date = {2017-01-17},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1701.04722},
	keywords = {Computer Science - Machine Learning},
	file = {Mescheder et al. - 2017 - Adversarial Variational Bayes Unifying Variationa.pdf:/usr/stud/brechet/Zotero/storage/TW9X9W9H/Mescheder et al. - 2017 - Adversarial Variational Bayes Unifying Variationa.pdf:application/pdf}
}

@article{Bell2015a,
	title = {Weak symplectic forms and differential calculus in Banach spaces},
	pages = {1--15},
	author = {Bell, Jordan},
	date = {2015}
}

@article{Carlier2014,
	title = {Numerical methods for matching for teams and Wasserstein barycenters},
	issn = {12903841},
	url = {http://arxiv.org/abs/1411.3602},
	doi = {10.1051/m2an/2015033},
	abstract = {Equilibrium multi-population matching (matching for teams) is a problem from mathematical economics which is related to multi-marginal optimal transport. A special but important case is the Wasserstein barycenter problem, which has applications in image processing and statistics. Two algorithms are presented: a linear programming algorithm and an efficient nonsmooth optimization algorithm, which applies in the case of the Wasserstein barycenters. The measures are approximated by discrete measures: convergence of the approximation is proved. Numerical results are presented which illustrate the efficiency of the algorithms.},
	pages = {1--29},
	author = {Carlier, Guillaume and Oberman, Adam and Oudet, Edouard},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {1411.3602},
	note = {bibtex*[arxivid=1411.3602] },
	keywords = {convex minimization, duality, linear, matching for teams, numerical methods for nonsmooth, programming, wasserstein barycenters},
	file = {carlieretal2015:/usr/stud/brechet/Zotero/storage/5NBDNJ3Q/carlieretal2015.pdf:application/pdf}
}

@article{Berger2003,
	title = {Rate-Distortion Theory},
	url = {http://doi.wiley.com/10.1002/0471219282.eot142},
	doi = {10.1002/0471219282.eot142},
	pages = {301--346},
	journaltitle = {Wiley Encyclopedia of Telecommunications},
	author = {Berger, Toby},
	date = {2003},
	note = {bibtex*[isbn=9780471219286]}
}

@article{Schmitzer2017,
	title = {Optimal Transport for Data Analysis},
	volume = {1},
	pages = {1--38},
	author = {Schmitzer, Bernhard},
	date = {2017}
}

@incollection{Montavon2015a,
	title = {Wasserstein Training of Boltzmann Machines},
	url = {http://arxiv.org/abs/1507.01972},
	abstract = {The Boltzmann machine provides a useful framework to learn highly complex, multimodal and multiscale data distributions that occur in the real world. The default method to learn its parameters consists of minimizing the Kullback-Leibler ({KL}) divergence from training samples to the Boltzmann model. We propose in this work a novel approach for Boltzmann training which assumes that a meaningful metric between observations is given. This metric can be represented by the Wasserstein distance between distributions, for which we derive a gradient with respect to the model parameters. Minimization of this new Wasserstein objective leads to generative models that are better when considering the metric and that have a cluster-like structure. We demonstrate the practical potential of these models for data completion and denoising, for which the metric between observations plays a crucial role.},
	pages = {3718--3726},
	number = {1},
	booktitle = {Advances in Neural Information Processing Systems 29},
	author = {Montavon, Grégoire and Müller, Klaus-Robert and Cuturi, Marco},
	date = {2015},
	eprinttype = {arxiv},
	eprint = {1507.01972},
	note = {bibtex*[arxivid=1507.01972]
{ISSN}: 10495258 },
	file = {Montavon, Cuturi, Paris-saclay - 2016 - Wasserstein Training of Restricted Boltzmann Machines:/usr/stud/brechet/Zotero/storage/GAX3R9S5/Montavon, Cuturi, Paris-saclay - 2016 - Wasserstein Training of Restricted Boltzmann Machines.pdf:application/pdf}
}

@article{Xu2014,
	title = {Show , Attend and Tell : Neural Image Caption Generation with Visual Attention},
	author = {Xu, Kelvin and Courville, Aaron and Zemel, Richard S and Bengio, Yoshua},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {arXiv:1502.03044v3},
	note = {bibtex*[arxivid={arXiv}:1502.03044v3] }
}

@book{Peypouquet2015,
	title = {Convex Optimization in Normed Spaces},
	isbn = {978-3-319-13709-4},
	url = {http://link.springer.com/10.1007/978-3-319-13710-0},
	abstract = {This work is intended to serve as a guide for graduate students and researchers who wish to get acquainted with the main theoretical and practical tools for the numerical minimization of convex functions on Hilbert spaces. Therefore, it contains the main tools that are necessary to conduct independent research on the topic. It is also a concise, easy-to-follow and self-contained textbook, which may be useful for any researcher working on related fields, as well as teachers giving graduate-level courses on the topic. It will contain a thorough revision of the extant literature including both classical and state-of-the-art references.},
	author = {Peypouquet, Juan},
	date = {2015},
	doi = {10.1007/978-3-319-13710-0},
	eprinttype = {arxiv},
	eprint = {arXiv:1011.1669v3},
	note = {bibtex*[arxivid={arXiv}:1011.1669v3;pmid=25246403]
{ISSN}: 1098-6596 }
}

@article{Muandet2016,
	title = {Kernel Mean Embedding of Distributions: A Review and Beyond},
	issn = {1935-8237},
	url = {http://arxiv.org/abs/1605.09522{%}0Ahttp://dx.doi.org/10.1561/2200000060},
	doi = {10.1561/2200000060},
	abstract = {A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space ({RKHS}) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original "feature map" common to support vector machines ({SVMs}) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the {RKHS} and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply {RKHS} methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.},
	author = {Muandet, Krikamol and Fukumizu, Kenji and Sriperumbudur, Bharath and Schölkopf, Bernhard},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1605.09522},
	note = {bibtex*[isbn=9781680832884;arxivid=1605.09522] },
	file = {muandet2017:/usr/stud/brechet/Zotero/storage/9G3NMF6X/muandet2017.pdf:application/pdf;muandet2017trim:/usr/stud/brechet/Zotero/storage/45XLTGH2/muandet2017trim.pdf:application/pdf;muandet2017trim2:/usr/stud/brechet/Zotero/storage/JQLBFBVF/muandet2017trim2.pdf:application/pdf}
}

@article{ElGamal2011,
	title = {Network Information Theory},
	url = {http://ebooks.cambridge.org/ref/id/CBO9781139030687},
	doi = {10.1017/CBO9781139030687},
	abstract = {This comprehensive treatment of network information theory and its applications pro-vides the first unified coverage of both classical and recent results. With an approach that balances the introduction of new models and new coding techniques, readers are guided through Shannon's point-to-point information theory, single-hop networks, multihop networks, and extensions to distributed computing, secrecy, wireless communication, and networking. Elementary mathematical tools and techniques are used throughout, requiring only basic knowledge of probability, whilst unified proofs of coding theorems are based on a few simple lemmas, making the text accessible to newcomers. Key topics covered include successive cancellation and superposition coding, {MIMO} wireless com-munication, network coding, and cooperative relaying. Also covered are feedback and interactive communication, capacity approximations and scaling laws, and asynchronous and random access channels. This book is ideal for use in the classroom, for self-study, and as a reference for researchers and engineers in industry and academia. In the field of network information theory, he is best known for his seminal contributions to the relay, broadcast, and interference chan-nels; multiple description coding; coding for noisy networks; and energy-efficient packet scheduling and throughput–delay tradeoffs in wireless networks. He is a Fellow of {IEEE} and the winner of the 2012 Claude E. Shannon Award, the highest honor in the field of information theory.},
	pages = {509--611},
	author = {El Gamal, Abbas and Kim, Young-Han},
	date = {2011},
	eprinttype = {arxiv},
	eprint = {1001.3404},
	note = {bibtex*[isbn=9781139030687;arxivid=1001.3404] }
}

@article{Seguy2017,
	title = {Large-Scale Optimal Transport and Mapping Estimation},
	url = {http://arxiv.org/abs/1711.02283},
	abstract = {This paper presents a novel two-step approach for the fundamental problem of learning an optimal map from one distribution to another. First, we learn an optimal transport ({OT}) plan, which can be thought as a one-to-many map between the two distributions. To that end, we propose a stochastic dual approach of regularized {OT}, and show empirically that it scales better than a recent related approach when the amount of samples is very large. Second, we estimate a {\textbackslash}textit\{Monge map\} as a deep neural network learned by approximating the barycentric projection of the previously-obtained {OT} plan. This parameterization allows generalization of the mapping outside the support of the input measure. We prove two theoretical stability results of regularized {OT} which show that our estimations converge to the {OT} plan and Monge map between the underlying continuous measures. We showcase our proposed approach on two applications: domain adaptation and generative modeling.},
	pages = {1--15},
	number = {1781},
	author = {Seguy, Vivien and Damodaran, Bharath Bhushan and Flamary, Rémi and Courty, Nicolas and Rolet, Antoine and Blondel, Mathieu},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1711.02283},
	note = {bibtex*[arxivid=1711.02283] }
}

@article{Potash2016,
	title = {Recommender system incorporating user personality profile through analysis of written reviews},
	volume = {1680},
	issn = {16130073},
	url = {http://arxiv.org/abs/1701.04722},
	doi = {10.1016/j.aqpro.2013.07.003},
	abstract = {The cereal and oilseed trade is a necessary contingency for assuring world food security as we restore ecological health to the planet. In a more populous and resource stressed world, trade's role will grow – guiding producers and consumers in their choices and facilitating optimal use of natural resources. Greater water efficiency will be a fundamental part of this. The right policies, farm entrepreneurship, conscientious use of land and other resources, knowledge and infrastructure also are required.},
	pages = {60--66},
	journaltitle = {{CEUR} Workshop Proceedings},
	author = {Potash, Peter and Rumshisky, Anna},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {arXiv:1011.1669v3},
	note = {bibtex*[isbn=0030-8870;arxivid={arXiv}:1011.1669v3;pmid=24439530] },
	keywords = {Collaborative filtering, Human-centered computing, Recommender systems, Social networks},
	file = {Mescheder et al. - 2017 - Unifying Variational Autoencoders and Generative Adversarial Networks:/usr/stud/brechet/Zotero/storage/3XP8K4MR/Mescheder et al. - 2017 - Unifying Variational Autoencoders and Generative Adversarial Networks.pdf:application/pdf}
}

@article{Ramdas2017,
	title = {On wasserstein two-sample testing and related families of nonparametric tests},
	volume = {19},
	issn = {10994300},
	doi = {10.3390/e19020047},
	abstract = {Nonparametric two sample or homogeneity testing is a decision theoretic problem that involves identifying differences between two random variables without making parametric assumptions about their underlying distributions. The literature is old and rich, with a wide variety of statistics having being intelligently designed and analyzed, both for the unidimensional and the multivariate setting. Our contribution is to tie together many of these tests, drawing connections between seemingly very different statistics. In this work, our central object is the Wasserstein distance, as we form a chain of connections from univariate methods like the Kolmogorov-Smirnov test, {PP}/{QQ} plots and {ROC}/{ODC} curves, to multivariate tests involving energy statistics and kernel based maximum mean discrepancy. Some connections proceed through the construction of a {\textbackslash}textit\{smoothed\} Wasserstein distance, and others through the pursuit of a "distribution-free" Wasserstein test. Some observations in this chain are implicit in the literature, while others seem to have not been noticed thus far. Given nonparametric two sample testing's classical and continued importance, we aim to provide useful connections for theorists and practitioners familiar with one subset of methods but not others.},
	pages = {1--18},
	number = {2},
	journaltitle = {Entropy},
	author = {Ramdas, Aaditya and Trillos, Nicolás Garćia and Cuturi, Marco},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1509.02237},
	note = {bibtex*[isbn=978-0-471-86725-8;arxivid=1509.02237] },
	keywords = {Energy distance, Entropic smoothing, Maximum mean discrepancy, {QQ} and {PP} plots, {ROC} and {ODC} curves, Two-sample testing, Wasserstein distance}
}

@report{Cohen2018,
	title = {{ON} {THE} {CONVERGENCE} {OF} {ADAM} {AND} {BEYOND}},
	abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as {RMSPROP}, {ADAM}, {ADADELTA}, {NADAM} are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where {ADAM} does not converge to the optimal solution, and describe the precise problems with the previous analysis of {ADAM} algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with "long-term memory" of past gradients, and propose new variants of the {ADAM} algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
	pages = {1--23},
	author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
	date = {2018},
	doi = {10.1134/S0001434607010294},
	eprinttype = {arxiv},
	eprint = {1709.01507},
	note = {bibtex*[isbn=9781538610329;arxivid=1709.01507;pmid=23766329]
{ISSN}: 08695652 }
}

@incollection{Monge1781,
	title = {Mémoire sur la théorie des déblais et de remblais},
	isbn = {0959-3780},
	abstract = {Rural and regional hinterlands provide the ecosystem service needs for increasingly urbanised communities across the globe. These inter-related ecosystem services provide key opportunities in securing climate change mitigation and adaptation. Their integrated management in the face of climate change, however, can be confounded by fragmentation within the complex institutional arrangements concerned with natural resource management. This suggests the need for a more systemic approach to continuous improvement in the integrated and adaptive governance of natural resources. This paper explores the theoretical foundations for integrated natural resource management and reviews positive systemic improvements that have been emerging in the Australian context. In setting clear theoretical foundations, the paper explores both functional and structural aspects of natural resource governance systems. Functional considerations include issues of connectivity, knowledge use and capacity within the natural resource decision making environment. Structural considerations refer to the institutions and processes that undertake planning through to implementation, monitoring and evaluation. From this foundation, we review the last decade of emerging initiatives in governance regarding the integration of agriculture and forests across the entire Australian landscape. This includes the shift towards more devolved regional approaches to integrated natural resource management and recent progress towards the use of terrestrial carbon at landscape scale to assist in climate change mitigation and adaptation. These developments, however, have also been tempered by a significant raft of new landscape-scale regulations that have tended to be based on a more centralist philosophy that landowners should be providing ecosystem services for the wider public good without substantive reward. Given this background, we explore a case study of efforts taken to integrate the management of landscape-scale agro-ecological services in the Wet Tropics of tropical Queensland. This is being achieved primarily through the integration of regional natural resource management planning and the development of aggregated terrestrial carbon offset products at a whole of landscape scale via the Degree Celsius initiative. Finally, the paper teases out the barriers and opportunities being experienced, leading to discussion about the global implications for managing climate change, income generation and poverty reduction.},
	booktitle = {Histoire de l'Académie Royale des Sciences de Paris, avec les Mémoires de Mathématique et de Physique pour la même année},
	author = {Monge, Gaspard},
	date = {1781},
	doi = {http://dx.doi.org/10.1016/j.gloenvcha.2013.10.003}
}

@book{Pochet2006,
	title = {Production Planning by Mixed Integer Programming},
	isbn = {978-0-387-29959-4},
	url = {http://link.springer.com/10.1007/0-387-33477-7},
	abstract = {Annotation},
	pagetotal = {524},
	author = {Pochet, Yves and Wolsey, Laurence a.},
	date = {2006},
	doi = {10.1007/0-387-33477-7},
	file = {Pochet, Wolsey - 2006 - Springer Series in Operations Research and Financial Engineering:/usr/stud/brechet/Zotero/storage/CREGTDTD/Pochet, Wolsey - 2006 - Springer Series in Operations Research and Financial Engineering.pdf:application/pdf}
}

@article{Rolet2016,
	title = {Fast Dictionary Learning with a Smoothed Wasserstein Loss},
	volume = {41},
	issn = {1938-7228},
	url = {http://proceedings.mlr.press/v51/rolet16.pdf},
	abstract = {We consider in this paper the dictionary learning problem when the observations are normalized histograms of features. This problem can be tackled using non-negative matrix factorization approaches, using typi-cally Euclidean or Kullback-Leibler fitting er-rors. Because these fitting errors are separa-ble and treat each feature on equal footing, they are blind to any similarity the features may share. We assume in this work that we have prior knowledge on these features. To leverage this side-information, we propose to use the Wasserstein (a.k.a. earth mover's or optimal transport) distance as the fitting er-ror between each original point and its re-construction, and we propose scalable algo-rithms to to so. Our methods build upon Fenchel duality and entropic regularization of Wasserstein distances, which improves not only speed but also computational stability. We apply these techniques on face images and text documents. We show in particular that we can learn dictionaries (topics) for bag-of-word representations of texts using words that may not have appeared in the original texts, or even words that come from a differ-ent language than that used in the texts.},
	pages = {1--9},
	number = {3},
	journaltitle = {International Conference on Artificial Intelligence and Statistics},
	author = {Rolet, Antoine and Cuturi, Marco and Peyré, Gabriel},
	date = {2016}
}

@article{Cuturi2013,
	title = {Fast Computation of Wasserstein Barycenters},
	volume = {32},
	url = {http://arxiv.org/abs/1310.4375},
	abstract = {We present new algorithms to compute the mean of a set of empirical probability measures under the optimal transport metric. This mean, known as the Wasserstein barycenter, is the measure that minimizes the sum of its Wasserstein distances to each element in that set. We propose two original algorithms to compute Wasserstein barycenters that build upon the subgradient method. A direct implementation of these algorithms is, however, too costly because it would require the repeated resolution of large primal and dual optimal transport problems to compute subgradients. Extending the work of Cuturi (2013), we propose to smooth the Wasserstein distance used in the definition of Wasserstein barycenters with an entropic regularizer and recover in doing so a strictly convex objective whose gradients can be computed for a considerably cheaper computational cost using matrix scaling algorithms. We use these algorithms to visualize a large family of images and to solve a constrained clustering problem.},
	issue = {c},
	author = {Cuturi, Marco and Doucet, Arnaud},
	date = {2013},
	eprinttype = {arxiv},
	eprint = {1310.4375},
	note = {bibtex*[isbn=9781634393973;arxivid=1310.4375] },
	keywords = {Ma, optimal transportation, Wasserstein barycenter}
}

@article{Duchi2011,
	title = {Adaptive subgradient methods for online learning and stochastic optimization},
	volume = {12},
	issn = {15324435},
	url = {http://portal.acm.org/ft{_}gateway.cfm?id=2021068{&}ftid=1013134{&}coll=DL{&}dl=GUIDE{&}CFID=346380592{&}CFTOKEN=89823863{%}0Ahttp://portal.acm.org/citation.cfm?id=1953048.2021068{&}coll=DL{&}dl=GUIDE{&}CFID=346380592{&}CFTOKEN=89823863{%}0Ahttp://dl.acm.org/citation.cfm?id=2021},
	doi = {10.1109/CDC.2012.6426698},
	abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
	pages = {2121--2159},
	journaltitle = {The Journal of Machine Learning Research},
	author = {Duchi, J and Hazan, E and Singer, Y},
	date = {2011},
	eprinttype = {arxiv},
	eprint = {arXiv:1103.4296v1},
	note = {bibtex*[isbn=9780982252925;arxivid={arXiv}:1103.4296v1;pmid=2868127] },
	keywords = {adaptivity, online learning, stochastic convex opti-, subgradient methods}
}

@article{Wilson2017,
	title = {The Marginal Value of Adaptive Gradient Methods in Machine Learning {arXiv} : 1705 . 08292v2 [ stat . {ML} ] 22 May 2018},
	issue = {Nips},
	author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nathan and Recht, Benjamin},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {arXiv:1705.08292v2},
	note = {bibtex*[arxivid={arXiv}:1705.08292v2] }
}

@article{Bell2015,
	title = {Gradients and Hessians in Hilbert spaces},
	pages = {1--11},
	number = {1},
	author = {Bell, Jordan},
	date = {2015}
}

@article{Makhzani2014,
	title = {Adversarial Autoencoders},
	author = {Makhzani, Alireza and Frey, Brendan and Goodfellow, Ian},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {arXiv:1511.05644v2},
	note = {bibtex*[arxivid={arXiv}:1511.05644v2] }
}

@article{Race2006,
	title = {Gambling and Data},
	pages = {159--182},
	author = {Race, T H E Horse},
	date = {2006}
}

@article{Genevay2017a,
	title = {Learning Generative Models with Sinkhorn Divergences},
	issn = {1938-7228},
	url = {http://arxiv.org/abs/1706.00292},
	abstract = {The ability to compare two degenerate probability distributions (i.e. two probability distributions supported on two distinct low-dimensional manifolds living in a much higher-dimensional space) is a crucial problem arising in the estimation of generative models for high-dimensional observations such as those arising in computer vision or natural language. It is known that optimal transport metrics can represent a cure for this problem, since they were specifically designed as an alternative to information divergences to handle such problematic scenarios. Unfortunately, training generative machines using {OT} raises formidable computational and statistical challenges, because of (i) the computational burden of evaluating {OT} losses, (ii) the instability and lack of smoothness of these losses, (iii) the difficulty to estimate robustly these losses and their gradients in high dimension. This paper presents the first tractable computational method to train large scale generative models using an optimal transport loss, and tackles these three issues by relying on two key ideas: (a) entropic smoothing, which turns the original {OT} loss into one that can be computed using Sinkhorn fixed point iterations; (b) algorithmic (automatic) differentiation of these iterations. These two approximations result in a robust and differentiable approximation of the {OT} loss with streamlined {GPU} execution. Entropic smoothing generates a family of losses interpolating between Wasserstein ({OT}) and Maximum Mean Discrepancy ({MMD}), thus allowing to find a sweet spot leveraging the geometry of {OT} and the favorable high-dimensional sample complexity of {MMD} which comes with unbiased gradient estimates. The resulting computational architecture complements nicely standard deep network generative models by a stack of extra layers implementing the loss function.},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1706.00292},
	note = {bibtex*[arxivid=1706.00292] },
	file = {Genevay, Peyré, Cuturi - 2018 - Learning Generative Models with Sinkhorn Divergences:/usr/stud/brechet/Zotero/storage/XHE66C2I/Genevay, Peyré, Cuturi - 2018 - Learning Generative Models with Sinkhorn Divergences.pdf:application/pdf}
}

@article{Feydy2018,
	title = {Interpolating between Optimal Transport and {MMD} using Sinkhorn Divergences},
	url = {http://arxiv.org/abs/1810.08278},
	abstract = {Comparing probability distributions is a fundamental problem in data sciences. Simple norms and divergences such as the total variation and the relative entropy only compare densities in a point-wise manner and fail to capture the geometric nature of the problem. In sharp contrast, Maximum Mean Discrepancies ({MMD}) and Optimal Transport distances ({OT}) are two classes of distances between measures that take into account the geometry of the underlying space and metrize the convergence in law. This paper studies the Sinkhorn divergences, a family of geometric divergences that interpolates between {MMD} and {OT}. Relying on a new notion of geometric entropy, we provide theoretical guarantees for these divergences: positivity, convexity and metrization of the convergence in law. On the practical side, we detail a numerical scheme that enables the large scale application of these divergences for machine learning: on the {GPU}, gradients of the Sinkhorn loss can be computed for batches of a million samples.},
	number = {1},
	author = {Feydy, Jean and Séjourné, Thibault and Vialard, François-Xavier and Amari, Shun-ichi and Trouvé, Alain and Peyré, Gabriel},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1810.08278},
	note = {bibtex*[isbn=1810.08278v1;arxivid=1810.08278] }
}

@article{Karlsen2006,
	title = {Notes on weak convergence},
	pages = {1--14},
	author = {Karlsen, Kenneth H},
	date = {2006},
	file = {Weakconvergence:/usr/stud/brechet/Zotero/storage/5C3S3UIW/Weakconvergence.pdf:application/pdf}
}

@article{Cover2006,
	title = {Entropy Rates},
	pages = {71--101},
	issue = {X},
	journaltitle = {Elements of Information Theory2},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	date = {2006}
}

@article{Goodfellow,
	title = {Self-Attention Generative Adversarial Networks},
	author = {Goodfellow, Ian and Odena, Augustus},
	eprinttype = {arxiv},
	eprint = {arXiv:1805.08318v1},
	note = {bibtex*[arxivid={arXiv}:1805.08318v1] }
}

@article{Rockafellar1967,
	title = {Duality and Stability in Extremum Problems},
	volume = {21},
	number = {1},
	author = {Rockafellar, R Tyrrell},
	date = {1967}
}

@article{Defazio2014,
	title = {{SAGA}: A Fast Incremental Gradient Method With Support for Non-Strongly Convex Composite Objectives},
	issn = {10495258},
	url = {http://arxiv.org/abs/1407.0202},
	doi = {10.1080/0958315021000054359},
	abstract = {In this work we introduce a new optimisation method called {SAGA} in the spirit of {SAG}, {SDCA}, {MISO} and {SVRG}, a set of recently proposed incremental gradient algorithms with fast linear convergence rates. {SAGA} improves on the theory behind {SAG} and {SVRG}, with better theoretical convergence rates, and has support for composite objectives where a proximal operator is used on the regulariser. Unlike {SDCA}, {SAGA} supports non-strongly convex problems directly, and is adaptive to any inherent strong convexity of the problem. We give experimental results showing the effectiveness of our method.},
	pages = {1--9},
	author = {Defazio, Aaron and Bach, Francis and Lacoste-Julien, Simon},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {1407.0202},
	note = {bibtex*[isbn=0631191666;arxivid=1407.0202] },
	file = {5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives:/usr/stud/brechet/Zotero/storage/BLIJUEZ3/5258-saga-a-fast-incremental-gradient-method-with-support-for-non-strongly-convex-composite-objectives.pdf:application/pdf}
}

@article{Sanjabi2018,
	title = {On the Convergence and Robustness of Training {GANs} with Regularized Optimal Transport},
	url = {http://arxiv.org/abs/1802.08249},
	abstract = {Generative Adversarial Networks ({GANs}) are one of the most practical methods for learning data distributions. A popular {GAN} formulation is based on the use of Wasserstein distance as a metric between probability distributions. Unfortunately, minimizing the Wasserstein distance between the data distribution and the generative model distribution is a computationally challenging problem as its objective is non-convex, non-smooth, and even hard to compute. In this work, we show that obtaining gradient information of the smoothed Wasserstein {GAN} formulation, which is based on regularized Optimal Transport ({OT}), is computationally effortless and hence one can apply first order optimization methods to minimize this objective. Consequently, we establish theoretical convergence guarantee to stationarity for a proposed class of {GAN} optimization algorithms. Unlike the original non-smooth formulation, our algorithm only requires solving the discriminator to approximate optimality. We apply our method to learning {MNIST} digits as well as {CIFAR}-10images. Our experiments show that our method is computationally efficient and generates images comparable to the state of the art algorithms given the same architecture and computational power.},
	author = {Sanjabi, Maziar and Ba, Jimmy and Razaviyayn, Meisam and Lee, Jason D.},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1802.08249},
	note = {bibtex*[arxivid=1802.08249] }
}

@article{Lindstrom2014,
	title = {Racial Bias Shapes Social Reinforcement Learning},
	volume = {25},
	issn = {14679280},
	url = {http://arxiv.org/abs/1602.01783},
	doi = {10.1177/0956797613514093},
	abstract = {Both emotional facial expressions and markers of racial-group belonging are ubiquitous signals in social interaction, but little is known about how these signals together affect future behavior through learning. To address this issue, we investigated how emotional (threatening or friendly) in-group and out-group faces reinforced behavior in a reinforcement-learning task. We asked whether reinforcement learning would be modulated by intergroup attitudes (i.e., racial bias). The results showed that individual differences in racial bias critically modulated reinforcement learning. As predicted, racial bias was associated with more efficiently learned avoidance of threatening out-group individuals. We used computational modeling analysis to quantitatively delimit the underlying processes affected by social reinforcement. These analyses showed that racial bias modulates the rate at which exposure to threatening out-group individuals is transformed into future avoidance behavior. In concert, these results shed new light on the learning processes underlying social interaction with racial-in-group and out-group individuals.},
	pages = {711--719},
	number = {3},
	journaltitle = {Psychological Science},
	author = {Lindström, Björn and Selbing, Ida and Molapour, Tanaz and Olsson, Andreas},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {1602.01783},
	note = {bibtex*[isbn=0956-7976;arxivid=1602.01783;pmid=24458270] },
	keywords = {emotions, learning, racial and ethnic attitudes and relations, social influences}
}

@article{Schmitzer2016,
	title = {Stabilized Sparse Scaling Algorithms for Entropy Regularized Transport Problems},
	url = {http://arxiv.org/abs/1610.06519},
	abstract = {Scaling algorithms for entropic transport-type problems have become a very popular numerical method, encompassing Wasserstein barycenters, multi-marginal problems, gradient flows and unbalanced transport. However, a standard implementation of the scaling algorithm has several numerical limitations: the scaling factors diverge and convergence becomes impractically slow as the entropy regularization approaches zero. Moreover, handling the dense kernel matrix becomes unfeasible for large problems. To address this, we propose several modifications: A log-domain stabilized formulation, the well-known epsilon-scaling heuristic, an adaptive truncation of the kernel and a coarse-to-fine scheme. This allows to solve larger problems with smaller regularization and negligible truncation error. A new convergence analysis of the Sinkhorn algorithm is developed, working towards a better understanding of epsilon-scaling. Numerical examples illustrate efficiency and versatility of the modified algorithm.},
	pages = {1--49},
	author = {Schmitzer, Bernhard},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1610.06519},
	note = {bibtex*[arxivid=1610.06519] }
}

@article{Samal2018,
	title = {Introduction and preview},
	issn = {21915318},
	url = {http://books.google.com.hk/books?id=VWq5GG6ycxMC{&}printsec=frontcover{&}dq=Elements+of+Information+Theory{&}hl={&}cd=1{&}source=gbs{_}api{%}5Cnpapers3://publication/uuid/E1320BD9-B443-434F-8D83-8198955806BD},
	doi = {10.1007/978-3-319-70733-4_1},
	abstract = {© Springer-Verlag Berlin Heidelberg 2006. The classical state-space model for a proper system, corresponding to a set of ordinary di.erential equations ({ODEs}), can be obtained by selecting a minimal set of variables (known as state variables). However, there are practical situations where physical variables (referred to as descriptor variables) cannot be chosen as state variables in a natural way to provide a mathematical model in a state-space form. Usually, descriptor variables chosen naturally are not minimal in the sense that they may be related algebraically. This may result in a singular system model since some of the relationships of these variables are dynamic while others purely static.},
	pages = {1--18},
	number = {9783319707327},
	journaltitle = {{SpringerBriefs} in Applied Sciences and Technology},
	author = {Samal, Sneha},
	date = {2018},
	note = {bibtex*[isbn=9780198275435]}
}

@article{Sriperumbudur2009,
	title = {On integral probability metrics, {\textbackslash}phi-divergences and binary classification},
	url = {http://arxiv.org/abs/0901.2698},
	abstract = {A class of distance measures on probabilities -- the integral probability metrics ({IPMs}) -- is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. {IPMs} have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures defined on a metric space. Practical applications of {IPMs} are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of {IPMs}, which should contribute to making {IPMs} more widely used in practice, for instance in areas where \${\textbackslash}phi\$-divergences are currently popular. First, to understand the relation between {IPMs} and \${\textbackslash}phi\$-divergences, the necessary and sufficient conditions under which these classes intersect are derived: the total variation distance is shown to be the only non-trivial \${\textbackslash}phi\$-divergence that is also an {IPM}. This shows that {IPMs} are essentially different from \${\textbackslash}phi\$-divergences. Second, empirical estimates of several {IPMs} from finite i.i.d. samples are obtained, and their consistency and convergence rates are analyzed. These estimators are shown to be easily computable, with better rates of convergence than estimators of \${\textbackslash}phi\$-divergences. Third, a novel interpretation is provided for {IPMs} by relating them to binary classification, where it is shown that the {IPM} between class-conditional distributions is the negative of the optimal risk associated with a binary classifier. In addition, the smoothness of an appropriate binary classifier is proved to be inversely related to the distance between the class-conditional distributions, measured in terms of an {IPM}.},
	pages = {1--18},
	number = {1},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	date = {2009},
	eprinttype = {arxiv},
	eprint = {0901.2698},
	note = {bibtex*[arxivid=0901.2698] },
	file = {sriperumbudur2009:/usr/stud/brechet/Zotero/storage/ERF8PXHV/sriperumbudur2009.pdf:application/pdf}
}

@article{Sriperumbudur2012,
	title = {On the empirical estimation of integral probability metrics},
	volume = {6},
	issn = {19357524},
	doi = {10.1214/12-EJS722},
	abstract = {In this paper, we develop and analyze a nonparametric method for estimating the class of integral probability metrics ({IPMs}), examples of which include the Wasserstein distance, Dudley metric, and maximum mean discrepancy ({MMD}). We show that these distances can be estimated efficiently by solving a linear program in the case of Wasserstein distance and Dudley metric, while {MMD} is computable in a closed form. All these estimators are shown to be strongly consistent and their convergence rates are analyzed. Based on these results, we show that {IPMs} are simple to estimate and the estimators exhibit good convergence behavior compared to \&\#x00F8;-divergence estimators.},
	pages = {1550--1599},
	journaltitle = {Electronic Journal of Statistics},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R.G.},
	date = {2012},
	note = {bibtex*[isbn=9781424469604]},
	keywords = {Dual-bounded Lipschitz distance (Dudley metric), Empirical estimation, Integral probability metrics, Kantorovich metric, Kernel distance, Lipschitz classifier, Rademacher average, Reproducing kernel Hilbert space, Support vector machine},
	file = {sriperumbudur2012:/usr/stud/brechet/Zotero/storage/E9J93V5S/sriperumbudur2012.pdf:application/pdf}
}

@article{Mattar2012,
	title = {Differential Entropy},
	url = {http://mathworld.wolfram.com/DifferentialEntropy.html},
	pages = {243--259},
	issue = {X},
	journaltitle = {{MathWorld}--A Wolfram Web Resource},
	author = {Mattar, Marwan and Rudary, Matthew Weisstein, Eric},
	date = {2012},
	note = {bibtex*[isbn=9781466583177]}
}

@book{Santambrogio2015b,
	title = {Optimal Transport for Applied Mathematicians},
	isbn = {978-3-319-20827-5},
	abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
	pagetotal = {1-376},
	author = {Santambrogio, F.},
	date = {2015},
	doi = {10.1007/978-3-319-20828-2},
	note = {bibtex*[booktitle=Springer International Publishing Switzerland] },
	file = {Santambrogio - 2015 - Optimal Transport for Applied Mathematicians:/usr/stud/brechet/Zotero/storage/IKNCMGFK/Santambrogio - 2015 - Optimal Transport for Applied Mathematicians.pdf:application/pdf}
}

@article{To2019,
	title = {E {NTROPIC} {GAN} S {MEET} {VAE} S : A S {TATISTICAL} A {PPROACH} {TO} C {OMPUTE} S {AMPLE}},
	author = {To, Pproach},
	date = {2019}
}

@article{Bellemare2017,
	title = {The Cramer Distance as a Solution to Biased Wasserstein Gradients},
	issn = {0022-0302},
	url = {http://arxiv.org/abs/1705.10743},
	doi = {10.1111/j.1574-0862.2012.00588.x},
	abstract = {The Wasserstein probability metric has received much attention from the machine learning community. Unlike the Kullback-Leibler divergence, which strictly measures change in probability, the Wasserstein metric reflects the underlying geometry between outcomes. The value of being sensitive to this geometry has been demonstrated, among others, in ordinal regression and generative modelling. In this paper we describe three natural properties of probability divergences that reflect requirements from machine learning: sum invariance, scale sensitivity, and unbiased sample gradients. The Wasserstein metric possesses the first two properties but, unlike the Kullback-Leibler divergence, does not possess the third. We provide empirical evidence suggesting that this is a serious issue in practice. Leveraging insights from probabilistic forecasting we propose an alternative to the Wasserstein metric, the Cram{\textbackslash}'er distance. We show that the Cram{\textbackslash}'er distance possesses all three desired properties, combining the best of the Wasserstein and Kullback-Leibler divergences. To illustrate the relevance of the Cram{\textbackslash}'er distance in practice we design a new algorithm, the Cram{\textbackslash}'er Generative Adversarial Network ({GAN}), and show that it performs significantly better than the related Wasserstein {GAN}.},
	pages = {1--20},
	author = {Bellemare, Marc G. and Danihelka, Ivo and Dabney, Will and Mohamed, Shakir and Lakshminarayanan, Balaji and Hoyer, Stephan and Munos, Rémi},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.10743},
	note = {bibtex*[isbn=1112300600;arxivid=1705.10743] }
}

@article{Heusel2017,
	title = {{GANs} Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium},
	issue = {Nips},
	author = {Heusel, Martin and Jan, L G and Hochreiter, Sepp},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {arXiv:1706.08500v6},
	note = {bibtex*[arxivid={arXiv}:1706.08500v6] }
}

@article{Nye2018,
	title = {Are Efficient Deep Representations Learnable?},
	issn = {0004-6361},
	url = {http://arxiv.org/abs/1807.06399},
	doi = {10.1051/0004-6361/201527329},
	abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
	pages = {1--16},
	author = {Nye, Maxwell and Saxe, Andrew},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1807.06399},
	note = {bibtex*[isbn=2004012439;arxivid=1807.06399;mendeley-tags={TOREAD};pmid=23459267] },
	keywords = {{TOREAD}}
}

@article{Chen2016,
	title = {{InfoGAN}: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets},
	issn = {978-3-319-16807-4},
	url = {http://arxiv.org/abs/1606.03657},
	doi = {10.1007/978-3-319-16817-3},
	abstract = {This paper describes {InfoGAN}, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. {InfoGAN} is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, {InfoGAN} successfully disentangles writing styles from digit shapes on the {MNIST} dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the {SVHN} dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the {CelebA} face dataset. Experiments show that {InfoGAN} learns interpretable representations that are competitive with representations learned by existing fully supervised methods.},
	author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1606.03657},
	note = {bibtex*[isbn=978-3-319-16816-6;arxivid=1606.03657;pmid=23459267] },
	file = {Full Text:/usr/stud/brechet/Zotero/storage/2CEWKHMB/Chen et al. - 2016 - InfoGAN Interpretable Representation Learning by .pdf:application/pdf;infogan_interpretable_representation_learning_by_information_maximizing_generative_adversarial_nets.pdf:/Users/Pierre/Documents/ss18/mt/literature/infogan_interpretable_representation_learning_by_information_maximizing_generative_adversarial_nets.pdf:application/pdf}
}

@book{Santambrogio2015a,
	title = {Optimal Transport for Applied Mathematicians},
	isbn = {978-3-319-20827-5},
	abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
	pagetotal = {1-376},
	author = {Santambrogio, F.},
	date = {2015},
	doi = {10.1007/978-3-319-20828-2},
	note = {bibtex*[booktitle=Springer International Publishing Switzerland] }
}

@article{Cover2006a,
	title = {Universal source coding},
	pages = {427--462},
	journaltitle = {Elements of Information Theory},
	author = {Cover, Thomas and Thomas, Joy},
	date = {2006}
}

@article{Levy2018,
	title = {Notions of optimal transport theory and how to implement them on a computer},
	volume = {72},
	issn = {00978493},
	doi = {10.1016/j.cag.2018.01.009},
	abstract = {This article gives an introduction to optimal transport, a mathematical theory that makes it possible to measure distances between functions (or distances between more general objects), to interpolate between objects or to enforce mass/volume conservation in certain computational physics simulations. Optimal transport is a rich scientific domain, with active research communities, both on its theoretical aspects and on more applicative considerations, such as geometry processing and machine learning. This article aims at explaining the main principles behind the theory of optimal transport, introduce the different involved notions, and more importantly, how they relate, to let the reader grasp an intuition of the elegant theory that structures them. Then we will consider a specific setting, called semi-discrete, where a continuous function is transported to a discrete sum of Dirac masses. Studying this specific setting naturally leads to an efficient computational algorithm, that uses classical notions of computational geometry, such as a generalization of Voronoi diagrams called Laguerre diagrams.},
	pages = {135--148},
	journaltitle = {Computers and Graphics (Pergamon)},
	author = {Lévy, Bruno and Schwindt, Erica L.},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1710.02634},
	note = {bibtex*[arxivid=1710.02634] },
	keywords = {Mathematics, Numerical optimization, Optimal transport, Physics},
	file = {Lévy, Schwindt - 2018 - Notions of optimal transport theory and how to implement them on a computer:/usr/stud/brechet/Zotero/storage/6WUL6TTE/Lévy, Schwindt - 2018 - Notions of optimal transport theory and how to implement them on a computer.pdf:application/pdf}
}

@article{Durrett2010,
	title = {Probability},
	issn = {0006341X},
	url = {http://ebooks.cambridge.org/ref/id/CBO9780511779398},
	doi = {10.1017/CBO9780511779398},
	abstract = {"This book is an introduction to probability theory covering laws of large numbers, central limit theorems, random walks, martingales, Markov chains, ergodic theorems, and Brownian motion. It is a comprehensive treatment concentrating on the results that are the most useful for applications. Its philosophy is that the best way to learn probability is to see it in action, so there are 200 examples and 450 problems"--},
	pages = {438},
	author = {Durrett, Rick},
	date = {2010},
	note = {bibtex*[isbn=9780511779398;pmid=16156020]}
}

@article{Courty2017b,
	title = {Joint Distribution Optimal Transportation for Domain Adaptation},
	issn = {0162-8828},
	doi = {10.1109/TPAMI.2016.2615921},
	abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \${\textbackslash}mathcal\{P\}\_s\$ and \${\textbackslash}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \${\textbackslash}mathcal\{P\}{\textasciicircum}f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
	author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.08848},
	note = {bibtex*[isbn=9782875870148;arxivid=1705.08848] }
}

@article{Aronszajn1950,
	title = {Theory of Reproducing Kernels},
	volume = {68},
	issn = {00029947},
	url = {http://www.jstor.org/stable/1990404?origin=crossref},
	doi = {10.2307/1990404},
	abstract = {Preface The present paper may be considered as a sequel to our previous paper in the Proceedings of the Cambridge Philosophical Society, Theorie générale de noyaux reproduisants—Première partie (vol. 39 (1944)) which was written in 1942-1943. In the introduction to this paper we ...},
	pages = {337},
	number = {3},
	journaltitle = {Transactions of the American Mathematical Society},
	author = {Aronszajn, N.},
	date = {1950},
	note = {bibtex*[isbn=047021211X;pmid=17746742]}
}

@article{Loshchilov2015,
	title = {Fixing Weight Decay Regularization in Adam},
	volume = {100},
	issn = {1567-7257},
	url = {http://arxiv.org/abs/1711.05101},
	doi = {10.1016/j.meegid.2012.02.004},
	abstract = {L\$\_2\$ regularization and weight decay regularization are equivalent for standard stochastic gradient descent (when rescaled by the learning rate), but as we demonstrate this is {\textbackslash}emph\{not\} the case for adaptive gradient algorithms, such as Adam. While common deep learning frameworks of these algorithms implement L\$\_2\$ regularization (often calling it "weight decay" in what may be misleading due to the inequivalence we expose), we propose a simple modification to recover the original formulation of weight decay regularization by decoupling the weight decay from the optimization steps taken w.r.t. the loss function. We provide empirical evidence that our proposed modification (i) decouples the optimal choice of weight decay factor from the setting of the learning rate for both standard {SGD} and Adam, and (ii) substantially improves Adam's generalization performance, allowing it to compete with {SGD} with momentum on image classification datasets (on which it was previously typically outperformed by the latter). We also propose a version of Adam with warm restarts ({AdamWR}) that has strong anytime performance while achieving state-of-the-art results on {CIFAR}-10 and {ImageNet}32x32. Our source code is available at https://github.com/loshchil/{AdamW}-and-{SGDW}},
	author = {Loshchilov, Ilya and Hutter, Frank},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1711.05101},
	note = {bibtex*[isbn=1711.05101v2;arxivid=1711.05101;pmid=22365971] }
}

@article{Oizumi,
	title = {Information Geometry Connecting Wasserstein Distance and Kullback-Leibler Divergence via the Entropy-Relaxed Transportation Problem},
	pages = {1--24},
	author = {Oizumi, Masafumi},
	eprinttype = {arxiv},
	eprint = {arXiv:1709.10219v1},
	note = {bibtex*[arxivid={arXiv}:1709.10219v1] },
	file = {Information-Geometry-Connecting-Wasserstein-Distance-and-Kullback-Leibler-Divergence-via-the-Entropy-Relaxed-Transportation-Problem:/usr/stud/brechet/Zotero/storage/HIG6UQEA/Information-Geometry-Connecting-Wasserstein-Distance-and-Kullback-Leibler-Divergence-via-the-Entropy-Relaxed-Transportation-Problem.pdf:application/pdf}
}

@article{Xiang2017,
	title = {On the Effects of Batch and Weight Normalization in Generative Adversarial Networks},
	issn = {0098-6445},
	url = {http://arxiv.org/abs/1704.03971},
	doi = {10.1080/00986449008911427},
	abstract = {Generative adversarial networks ({GANs}) are highly effective unsupervised learning frameworks that can generate very sharp data, even for data such as images with complex, highly multimodal distributions. However {GANs} are known to be very hard to train, suffering from problems such as mode collapse and disturbing visual artifacts. Batch normalization ({BN}) techniques have been introduced to address the training. Though {BN} accelerates the training in the beginning, our experiments show that the use of {BN} can be unstable and negatively impact the quality of the trained model. The evaluation of {BN} and numerous other recent schemes for improving {GAN} training is hindered by the lack of an effective objective quality measure for {GAN} models. To address these issues, we first introduce a weight normalization ({WN}) approach for {GAN} training that significantly improves the stability, efficiency and the quality of the generated samples. To allow a methodical evaluation, we introduce squared Euclidean reconstruction error on a test set as a new objective measure, to assess training performance in terms of speed, stability, and quality of generated samples. Our experiments with a standard {DCGAN} architecture on commonly used datasets ({CelebA}, {LSUN} bedroom, and {CIFAR}-10) indicate that training using {WN} is generally superior to {BN} for {GANs}, achieving 10\% lower mean squared loss for reconstruction and significantly better qualitative results than {BN}. We further demonstrate the stability of {WN} on a 21-layer {ResNet} trained with the {CelebA} data set. The code for this paper is available at https://github.com/stormraiser/gan-weightnorm-resnet},
	author = {Xiang, Sitao and Li, Hao},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1704.03971},
	note = {bibtex*[arxivid=1704.03971] }
}

@article{Nye2018a,
	title = {Are Efficient Deep Representations Learnable?},
	issn = {0004-6361},
	url = {http://arxiv.org/abs/1807.06399},
	doi = {10.1051/0004-6361/201527329},
	abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
	pages = {1--14},
	issue = {Ml},
	author = {Nye, Maxwell and Saxe, Andrew},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1807.06399},
	note = {bibtex*[isbn=2004012439;arxivid=1807.06399;pmid=23459267] },
	file = {Welling - Unknown - Auto-Encoding Variational Bayes arXiv 1312 . 6114v10 stat . ML 1 May 2014:/usr/stud/brechet/Zotero/storage/848MS7XW/Welling - Unknown - Auto-Encoding Variational Bayes arXiv 1312 . 6114v10 stat . ML 1 May 2014.pdf:application/pdf}
}

@article{Cover2005a,
	title = {Elements of Information Theory},
	issn = {15579654},
	doi = {10.1002/047174882X},
	abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, {AEP}, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
	pages = {1--748},
	issue = {x},
	journaltitle = {Elements of Information Theory},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	date = {2005},
	eprinttype = {arxiv},
	eprint = {ISBN 0-471-06259-6},
	note = {bibtex*[isbn=9780471241959;arxivid={ISBN} 0-471-06259-6;pmid=20660925] }
}

@article{Uilbart1979,
	title = {'i. h. p.,},
	volume = {4},
	pages = {333--354},
	author = {Uilbart, C G},
	date = {1979}
}

@article{Lecun2015,
	title = {Deep learning},
	volume = {521},
	issn = {14764687},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	pages = {436--444},
	number = {7553},
	author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	date = {2015},
	eprinttype = {arxiv},
	eprint = {arXiv:1312.6184v5},
	note = {bibtex*[booktitle=Nature;isbn=9780521835688;arxivid={arXiv}:1312.6184v5;pmid=10463930] }
}

@article{Cover2005,
	title = {Elements of Information Theory},
	issn = {15579654},
	doi = {10.1002/047174882X},
	abstract = {Following a brief introduction and overview, early chapters cover the basic algebraic relationships of entropy, relative entropy and mutual information, {AEP}, entropy rates of stochastics processes and data compression, duality of data compression and the growth rate of wealth. Later chapters explore Kolmogorov complexity, channel capacity, differential entropy, the capacity of the fundamental Gaussian channel, the relationship between information theory and statistics, rate distortion and network information theories. The final two chapters examine the stock market and inequalities in information theory. In many cases the authors actually describe the properties of the solutions before the presented problems.},
	pages = {1--748},
	journaltitle = {Elements of Information Theory},
	author = {Cover, Thomas M. and Thomas, Joy A.},
	date = {2005},
	eprinttype = {arxiv},
	eprint = {ISBN 0-471-06259-6},
	note = {bibtex*[isbn=9780471241959;arxivid={ISBN} 0-471-06259-6;pmid=20660925] }
}

@article{Blondel2018,
	title = {Smooth and Sparse Optimal Transport},
	volume = {84},
	author = {Blondel, Mathieu and Seguy, Vivien and Rolet, Antoine},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {arXiv:1710.06276v2},
	note = {bibtex*[arxivid={arXiv}:1710.06276v2] }
}

@article{Green2017,
	title = {Modeling contagion through social networks to explain and predict gunshot violence in Chicago, 2006 to 2014},
	volume = {177},
	issn = {21686106},
	url = {http://arxiv.org/abs/1406.2661},
	doi = {10.1001/jamainternmed.2016.8245},
	abstract = {Importance Every day in the United States, more than 200 people are murdered or assaulted with a firearm. Little research has considered the role of interpersonal ties in the pathways through which gun violence spreads. Objective To evaluate the extent to which the people who will become subjects of gun violence can be predicted by modeling gun violence as an epidemic that is transmitted between individuals through social interactions. Design, Setting, and Participants This study was an epidemiological analysis of a social network of individuals who were arrested during an 8-year period in Chicago, Illinois, with connections between people who were arrested together for the same offense. Modeling of the spread of gunshot violence over the network was assessed using a probabilistic contagion model that assumed individuals were subject to risks associated with being arrested together, in addition to demographic factors, such as age, sex, and neighborhood residence. Participants represented a network of 138 163 individuals who were arrested between January 1, 2006, and March 31, 2014 (29.9\% of all individuals arrested in Chicago during this period), 9773 of whom were subjects of gun violence. Individuals were on average 27 years old at the midpoint of the study, predominantly male (82.0\%) and black (75.6\%), and often members of a gang (26.2\%). Main Outcomes and Measures Explanation and prediction of becoming a subject of gun violence (fatal or nonfatal) using epidemic models based on person-to-person transmission through a social network. Results Social contagion accounted for 63.1\% of the 11 123 gunshot violence episodes; subjects of gun violence were shot on average 125 days after their infector (the person most responsible for exposing the subject to gunshot violence). Some subjects of gun violence were shot more than once. Models based on both social contagion and demographics performed best; when determining the 1.0\% of people (n = 1382) considered at highest risk to be shot each day, the combined model identified 728 subjects of gun violence (6.5\%) compared with 475 subjects of gun violence (4.3\%) for the demographics model (53.3\% increase) and 589 subjects of gun violence (5.3\%) for the social contagion model (23.6\% increase). Conclusions and Relevance Gunshot violence follows an epidemic-like process of social contagion that is transmitted through networks of people by social interactions. Violence prevention efforts that account for social contagion, in addition to demographics, have the potential to prevent more shootings than efforts that focus on only demographics.},
	pages = {326--333},
	number = {3},
	journaltitle = {{JAMA} Internal Medicine},
	author = {Green, Ben and Horel, Thibaut and Papachristos, Andrew V.},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1701.00160},
	note = {bibtex*[isbn=1581138285;arxivid=1701.00160;pmid=15040217] },
	file = {Goodfellow et al. - 2014 - Generative Adversarial Networks:/usr/stud/brechet/Zotero/storage/IIUJITFN/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf:application/pdf}
}

@article{Sriperumbudur2009a,
	title = {On integral probability metrics, {\textbackslash}phi-divergences and binary classification},
	url = {http://arxiv.org/abs/0901.2698},
	abstract = {A class of distance measures on probabilities -- the integral probability metrics ({IPMs}) -- is addressed: these include the Wasserstein distance, Dudley metric, and Maximum Mean Discrepancy. {IPMs} have thus far mostly been used in more abstract settings, for instance as theoretical tools in mass transportation problems, and in metrizing the weak topology on the set of all Borel probability measures defined on a metric space. Practical applications of {IPMs} are less common, with some exceptions in the kernel machines literature. The present work contributes a number of novel properties of {IPMs}, which should contribute to making {IPMs} more widely used in practice, for instance in areas where \${\textbackslash}phi\$-divergences are currently popular. First, to understand the relation between {IPMs} and \${\textbackslash}phi\$-divergences, the necessary and sufficient conditions under which these classes intersect are derived: the total variation distance is shown to be the only non-trivial \${\textbackslash}phi\$-divergence that is also an {IPM}. This shows that {IPMs} are essentially different from \${\textbackslash}phi\$-divergences. Second, empirical estimates of several {IPMs} from finite i.i.d. samples are obtained, and their consistency and convergence rates are analyzed. These estimators are shown to be easily computable, with better rates of convergence than estimators of \${\textbackslash}phi\$-divergences. Third, a novel interpretation is provided for {IPMs} by relating them to binary classification, where it is shown that the {IPM} between class-conditional distributions is the negative of the optimal risk associated with a binary classifier. In addition, the smoothness of an appropriate binary classifier is proved to be inversely related to the distance between the class-conditional distributions, measured in terms of an {IPM}.},
	pages = {1--18},
	number = {1},
	author = {Sriperumbudur, Bharath K. and Fukumizu, Kenji and Gretton, Arthur and Schölkopf, Bernhard and Lanckriet, Gert R. G.},
	date = {2009},
	eprinttype = {arxiv},
	eprint = {0901.2698},
	note = {bibtex*[arxivid=0901.2698] }
}

@article{Coppersmith2006,
	title = {Bibliography [1]},
	pages = {145--171},
	author = {Coppersmith, D},
	date = {2006}
}

@article{Wang,
	title = {Non-local Neural Networks},
	author = {Wang, Xiaolong and Girshick, Ross},
	eprinttype = {arxiv},
	eprint = {arXiv:1711.07971v3},
	note = {bibtex*[arxivid={arXiv}:1711.07971v3] }
}

@article{DaSilva2008,
	title = {Variational analysis of the phenyl + O2and phenoxy + O reactions},
	volume = {112},
	issn = {10895639},
	doi = {10.1021/jp7118845},
	abstract = {Variational transition state analysis was performed on the barrierless phenyl + O2 and phenoxy + O association reactions. In addition, we also calculated rate constants for the related vinyl radical (C2H3) + O2 and vinoxy radical (C2H3O) + O reactions and provided rate constant estimates for analogous reactions in substituted aromatic systems. Potential energy scans along the dissociating C-{OO} and {CO}-O bonds (with consideration of C-{OO} internal rotation) were obtained at the O3LYP/6-31G(d) density functional theory level. The {CO}-O and C-{OO} bond scission reactions were observed to be barrierless, in both phenyl and vinyl systems. Potential energy wells were scaled by G3B3 reaction enthalpies to obtain accurate activation enthalpies. Frequency calculations were performed for all reactants and products and at points along the potential energy surfaces, allowing us to evaluate thermochemical properties as a function of temperature according to the principles of statistical mechanics and the rigid rotor harmonic oscillator ({RRHO}) approximation. The low-frequency vibrational modes corresponding to R-{OO} internal rotation were omitted from the {RRHO} analysis and replaced with a hindered internal rotor analysis using O3LYP/6-31G(d) rotor potentials. Rate constants were calculated as a function of temperature (300-2000 K) and position from activation entropies and enthalpies, according to canonical transition state theory; these rate constants were minimized with respect to position to obtain variational rate constants as a function of temperature. For the phenyl + O2 reaction, we identified the transition state to be located at a C-{OO} bond length of between 2.56 and 2.16 A (300-2000 K), while for the phenoxy + O reaction, the transition state was located at a {CO}-O bond length of 2.00-1.90 A. Variational rate constants were fit to a three-parameter form of the Arrhenius equation, and for the phenyl + O2 association reaction, we found k(T) = 1.860 x 1013T-0.217 exp(0.358/T) (with k in cm3 mol-1 s-1 and T in K); this rate equation provides good agreement with low-temperature experimental measurements of the phenyl + O2 rate constant. Preliminary results were presented for a correlation between activation energy (or reaction enthalpy) and pre-exponential factor for heterolytic O-O bond scission reactions.},
	pages = {3566--3575},
	number = {16},
	journaltitle = {Journal of Physical Chemistry A},
	author = {Da Silva, Gabriel and Bozzelli, Joseph W.},
	date = {2008},
	note = {bibtex*[isbn=1089-5639;pmid=18348555]}
}

@article{Salimans,
	title = {Weight Normalization : A Simple Reparameterization to Accelerate Training of Deep Neural Networks},
	author = {Salimans, Tim},
	eprinttype = {arxiv},
	eprint = {arXiv:1602.07868v3},
	note = {bibtex*[arxivid={arXiv}:1602.07868v3] }
}

@article{Frogner2015,
	title = {Learning with a Wasserstein Loss},
	issn = {10495258},
	url = {http://arxiv.org/abs/1506.05439},
	doi = {10.1016/S0891-5849(98)00315-3},
	abstract = {Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.},
	pages = {1--17},
	author = {Frogner, Charlie and Zhang, Chiyuan and Mobahi, Hossein and Araya-Polo, Mauricio and Poggio, Tomaso},
	date = {2015},
	eprinttype = {arxiv},
	eprint = {1506.05439},
	note = {bibtex*[isbn=1506.05439;arxivid=1506.05439;pmid=10381194] },
	keywords = {confusing dogs with cats, for example, in what follows, lationships between the different, might, or semantic similarity, output dimensions, performance in a way, re-, relationships, similarity, space the ground metric, structure on the label, that is sensitive to, that likewise have semantic, using the ground metric, we can measure prediction, we will call the}
}

@article{Gregor2014,
	title = {{DRAW} : A Recurrent Neural Network For Image Generation {arXiv} : 1502 . 04623v2 [ cs . {CV} ] 20 May 2015},
	author = {Gregor, Karol and Graves, Alex and Com, Wierstra Google},
	date = {2014},
	eprinttype = {arxiv},
	eprint = {arXiv:1502.04623v2},
	note = {bibtex*[arxivid={arXiv}:1502.04623v2] }
}

@article{Gretton2008,
	title = {A Kernel Method for the Two-Sample Problem},
	volume = {1},
	issn = {1049-5258},
	url = {http://arxiv.org/abs/0805.2368},
	abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space ({RKHS}). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
	pages = {1--10},
	author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J. and Scholkopf, Bernhard and Smola, Alexander J.},
	date = {2008},
	eprinttype = {arxiv},
	eprint = {0805.2368},
	note = {bibtex*[isbn=0-262-19568-2;arxivid=0805.2368] },
	file = {gretton2008:/usr/stud/brechet/Zotero/storage/6H9T4VCU/gretton2008.pdf:application/pdf}
}

@article{Salomon2007,
	title = {Data Compression},
	issn = {2155-5222},
	url = {http://www.csa.com/partners/viewrecord.php?requester=gs{&}collection=TRD{&}recid=1892920CI},
	doi = {10.1002/9781118256053.ch13},
	pages = {1008},
	issue = {x},
	journaltitle = {Online},
	author = {Salomon, David},
	date = {2007},
	note = {bibtex*[isbn=1-84628-602-6;pmid=20556846]}
}

@article{Feizi,
	title = {Understanding {GANs} : the {LQG} Setting},
	pages = {1--22},
	author = {Feizi, Soheil and Farnia, Farzan and Ginart, Tony and Tse, David},
	eprinttype = {arxiv},
	eprint = {arXiv:1710.10793v2},
	note = {bibtex*[arxivid={arXiv}:1710.10793v2] }
}

@article{Type2006,
	title = {Maximum Entropy 모델을 이용한 나열 및 병렬형 인식},
	volume = {1},
	pages = {409--425},
	issue = {x},
	author = {Type, Parallel and Maximum, Using and Model, Entropy},
	date = {2006}
}

@book{Santambrogio2015,
	title = {Optimal Transport for Applied Mathematicians},
	isbn = {978-3-319-20827-5},
	abstract = {This monograph presents a rigorous mathematical introduction to optimal transport as a variational problem, its use in modeling various phenomena, and its connections with partial differential equations. Its main goal is to provide the reader with the techniques necessary to understand the current research in optimal transport and the tools which are most useful for its applications. Full proofs are used to illustrate mathematical concepts and each chapter includes a section that discusses applications of optimal transport to various areas, such as economics, finance, potential games, image processing and fluid dynamics. Several topics are covered that have never been previously in books on this subject, such as the Knothe transport, the properties of functionals on measures, the Dacorogna-Moser flow, the formulation through minimal flows with prescribed divergence formulation, the case of the supremal cost, and the most classical numerical methods. Graduate students and researchers in both pure and applied mathematics interested in the problems and applications of optimal transport will find this to be an invaluable resource.},
	pagetotal = {1-376},
	author = {Santambrogio, F.},
	date = {2015},
	doi = {10.1007/978-3-319-20828-2},
	note = {bibtex*[booktitle=Springer International Publishing Switzerland] }
}

@article{Courty2017,
	title = {Joint Distribution Optimal Transportation for Domain Adaptation},
	issn = {0162-8828},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123838360000187 http://arxiv.org/abs/1705.08848},
	doi = {10.1109/TPAMI.2016.2615921},
	abstract = {This paper deals with the unsupervised domain adaptation problem, where one wants to estimate a prediction function \$f\$ in a given target domain without any labeled sample by exploiting the knowledge available from a source domain where labels are known. Our work makes the following assumption: there exists a non-linear transformation between the joint feature/label space distributions of the two domain \${\textbackslash}mathcal\{P\}\_s\$ and \${\textbackslash}mathcal\{P\}\_t\$. We propose a solution of this problem with optimal transport, that allows to recover an estimated target \${\textbackslash}mathcal\{P\}{\textasciicircum}f\_t=(X,f(X))\$ by optimizing simultaneously the optimal coupling and \$f\$. We show that our method corresponds to the minimization of a bound on the target error, and provide an efficient algorithmic solution, for which convergence is proved. The versatility of our approach, both in terms of class of hypothesis or loss functions is demonstrated with real world classification and regression problems, for which we reach or surpass state-of-the-art results.},
	pages = {317--327},
	journaltitle = {{MATLAB} for Neuroscientists},
	author = {Courty, Nicolas and Flamary, Rémi and Habrard, Amaury and Rakotomamonjy, Alain},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.08848},
	note = {bibtex*[isbn=9782875870148;arxivid=1705.08848;pmid=21492013] }
}

@article{Restarts2018,
	title = {{TION} {LOOK} {AT} D {EEP} L {EARNING} {HEURISTICS} :},
	author = {Restarts, Rate},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {arXiv:1810.13243v1},
	note = {bibtex*[arxivid={arXiv}:1810.13243v1] }
}

@article{Cuturi2013a,
	title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transportation Distances},
	issn = {10495258},
	url = {http://arxiv.org/abs/1306.0895},
	abstract = {Optimal transportation distances are a fundamental family of parameterized distances for histograms. Despite their appealing theoretical properties, excellent performance in retrieval tasks and intuitive formulation, their computation involves the resolution of a linear program whose cost is prohibitive whenever the histograms' dimension exceeds a few hundreds. We propose in this work a new family of optimal transportation distances that look at transportation problems from a maximum-entropy perspective. We smooth the classical optimal transportation problem with an entropic regularization term, and show that the resulting optimum is also a distance which can be computed through Sinkhorn-Knopp's matrix scaling algorithm at a speed that is several orders of magnitude faster than that of transportation solvers. We also report improved performance over classical optimal transportation distances on the {MNIST} benchmark problem.},
	pages = {1--9},
	author = {Cuturi, Marco},
	date = {2013},
	eprinttype = {arxiv},
	eprint = {1306.0895},
	note = {bibtex*[isbn=9781510810587;arxivid=1306.0895;pmid=1714571] },
	file = {Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Optimal Transportation Distances:/usr/stud/brechet/Zotero/storage/5RZ26V8E/Cuturi - 2013 - Sinkhorn Distances Lightspeed Computation of Optimal Transportation Distances.pdf:application/pdf}
}

@book{Garreau1987,
	title = {Real and Complex Analysis.},
	volume = {36},
	isbn = {0-07-054234-1},
	url = {http://www.jstor.org/stable/2348852?origin=crossref},
	abstract = {This is an advanced text for the one- or two-semester course in analysis taught primarily to math, science, computer science, and electrical engineering majors at the junior, senior or graduate level. The basic techniques and theorems of analysis are presented in such a way that the intimate connections between its various branches are strongly emphasized. The traditionally separate subjects of 'real analysis' and 'complex analysis' are thus united in one volume. Some of the basic ideas from functional analysis are also included. This is the only book to take this unique approach. The third edition includes a new chapter on differentiation. Proofs of theorems presented in the book are concise and complete and many challenging exercises appear at the end of each chapter. The book is arranged so that each chapter builds upon the other, giving students a gradual understanding of the subject.},
	pagetotal = {423},
	number = {4},
	author = {Garreau, G. A. and Rudin, Walter},
	date = {1987},
	doi = {10.2307/2348852},
	note = {bibtex*[booktitle=The Statistician;pmid=21273021]
{ISSN}: 00390526}
}

@article{Bousquet2017,
	title = {From optimal transport to generative modeling: the {VEGAN} cookbook},
	url = {http://arxiv.org/abs/1705.07642},
	abstract = {We study unsupervised generative modeling in terms of the optimal transport ({OT}) problem between true (but unknown) data distribution \$P\_X\$ and the latent variable model distribution \$P\_G\$. We show that the {OT} problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport ({POT}) objective, which can be efficiently minimized using stochastic gradient descent by sampling from \$P\_X\$ and \$P\_G\$. We show that {POT} for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders ({AAE}) (Makhzani et al., 2016), which provides the first theoretical justification for {AAEs} known to the authors. We also compare {POT} to other popular techniques like variational auto-encoders ({VAE}) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by {VAEs}, and (b) establishing duality between Wasserstein {GAN} (Arjovsky and Bottou, 2017) and {POT} for the 1-Wasserstein distance.},
	pages = {1--15},
	author = {Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Simon-Gabriel, Carl-Johann and Schoelkopf, Bernhard},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.07642},
	note = {bibtex*[arxivid=1705.07642] },
	file = {Bousquet et al. - Unknown - From optimal transport to generative modeling the VEGAN cookbook:/usr/stud/brechet/Zotero/storage/YPCSVXSH/Bousquet et al. - Unknown - From optimal transport to generative modeling the VEGAN cookbook.pdf:application/pdf}
}

@article{Nye2018b,
	title = {Are Efficient Deep Representations Learnable?},
	issn = {0004-6361},
	url = {http://arxiv.org/abs/1807.06399},
	doi = {10.1051/0004-6361/201527329},
	abstract = {Many theories of deep learning have shown that a deep network can require dramatically fewer resources to represent a given function compared to a shallow network. But a question remains: can these efficient representations be learned using current deep learning techniques? In this work, we test whether standard deep learning methods can in fact find the efficient representations posited by several theories of deep representation. Specifically, we train deep neural networks to learn two simple functions with known efficient solutions: the parity function and the fast Fourier transform. We find that using gradient-based optimization, a deep network does not learn the parity function, unless initialized very close to a hand-coded exact solution. We also find that a deep linear neural network does not learn the fast Fourier transform, even in the best-case scenario of infinite training data, unless the weights are initialized very close to the exact hand-coded solution. Our results suggest that not every element of the class of compositional functions can be learned efficiently by a deep network, and further restrictions are necessary to understand what functions are both efficiently representable and learnable.},
	author = {Nye, Maxwell and Saxe, Andrew},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1807.06399},
	note = {bibtex*[isbn=2004012439;arxivid=1807.06399;pmid=23459267] }
}

@article{Spectroscopy1975,
	title = {List of Symbols},
	issn = {1473-6691},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780122120503500074},
	doi = {10.1016/B978-0-12-212050-3.50007-4},
	abstract = {00000},
	pages = {xv--xvi},
	issue = {x},
	journaltitle = {Feedback Systems: Input–Output Properties},
	author = {Spectroscopy, Electrochemical Impedance},
	date = {1975},
	note = {bibtex*[isbn=9780511615092;pmid=24170943]}
}

@article{Ba2016,
	title = {Layer Normalization},
	issn = {10450823},
	url = {http://arxiv.org/abs/1607.06450},
	doi = {10.1038/nature14236},
	abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
	author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1607.06450},
	note = {bibtex*[isbn=978-3-642-04273-7;arxivid=1607.06450] }
}

@article{Zeiler2012,
	title = {Visualizing and Understanding Convolutional Networks},
	author = {Zeiler, Matthew D and Fergus, Rob},
	date = {2012},
	eprinttype = {arxiv},
	eprint = {arXiv:1311.2901v3},
	note = {bibtex*[arxivid={arXiv}:1311.2901v3] }
}

@article{Li2017,
	title = {{MMD} {GAN}: Towards Deeper Understanding of Moment Matching Network},
	issn = {10495258},
	url = {http://arxiv.org/abs/1705.08584},
	abstract = {Generative moment matching network ({GMMN}) is a deep generative model that differs from Generative Adversarial Network ({GAN}) by replacing the discriminator in {GAN} with a two-sample test based on kernel maximum mean discrepancy ({MMD}). Although some theoretical guarantees of {MMD} have been studied, the empirical performance of {GMMN} is still not as competitive as that of {GAN} on challenging and large benchmark datasets. The computational efficiency of {GMMN} is also less desirable in comparison with {GAN}, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of {GMMN} and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original {GMMN}. The new approach combines the key ideas in both {GMMN} and {GAN}, hence we name it {MMD} {GAN}. The new distance measure in {MMD} {GAN} is a meaningful loss that enjoys the advantage of weak topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including {MNIST}, {CIFAR}- 10, {CelebA} and {LSUN}, the performance of {MMD}-{GAN} significantly outperforms {GMMN}, and is competitive with other representative {GAN} works.},
	pages = {1--14},
	issue = {{MMD}},
	author = {Li, Chun-Liang and Chang, Wei-Cheng and Cheng, Yu and Yang, Yiming and Póczos, Barnabás},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.08584},
	note = {bibtex*[arxivid=1705.08584] }
}

@article{Peyre2011,
	title = {Comparison between \$W\_2\$ distance and \${\textbackslash}dot\{H\}{\textasciicircum}\{-1\}\$ norm, and localisation of Wasserstein distance},
	volume = {2},
	url = {http://arxiv.org/abs/1104.4631},
	doi = {10.1051/cocv/2017050},
	abstract = {It is well known that the quadratic Wasserstein distance \$W\_2 ({\textbackslash}mathord\{{\textbackslash}boldsymbol\{{\textbackslash}cdot\}\}, {\textbackslash}mathord\{{\textbackslash}boldsymbol\{{\textbackslash}cdot\}\})\$ is formally equivalent, for infinitesimally small perturbations, to some weighted \$H{\textasciicircum}\{-1\}\$ homogeneous Sobolev norm. In this article I show that this equivalence can be integrated to get non-asymptotic comparison results between these distances. Then I give an application of these results to prove that the \$W\_2\$ distance exhibits some localisation phenomenon: if \${\textbackslash}mu\$ and \${\textbackslash}nu\$ are measures on \${\textbackslash}mathbf\{R\}{\textasciicircum}n\$ and \${\textbackslash}varphi {\textbackslash}colon {\textbackslash}mathbf\{R\}{\textasciicircum}n {\textbackslash}to {\textbackslash}mathbf\{R\}\_+\$ is some bump function with compact support, then under mild hypotheses, you can bound above the Wasserstein distance between \${\textbackslash}varphi {\textbackslash}cdot {\textbackslash}mu\$ and \${\textbackslash}varphi {\textbackslash}cdot {\textbackslash}nu\$ by an explicit multiple of \$W\_2 ({\textbackslash}mu, {\textbackslash}nu)\$.},
	number = {2},
	author = {Peyre, Rémi},
	date = {2011},
	eprinttype = {arxiv},
	eprint = {1104.4631},
	note = {bibtex*[arxivid=1104.4631] },
	keywords = {homogeneous sobolev norm, localisation, wasserstein distance},
	file = {peyre2011:/usr/stud/brechet/Zotero/storage/MEGAFLJF/peyre2011.pdf:application/pdf}
}

@article{Bottou2016,
	title = {Optimization Methods for Large-Scale Machine Learning},
	issn = {0036-1445},
	url = {http://arxiv.org/abs/1606.04838},
	doi = {10.1137/16M1080173},
	abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient ({SG}) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile {SG} algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
	author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {1606.04838},
	note = {bibtex*[isbn=9781538634288;arxivid=1606.04838] },
	file = {Bottou, Curtis, Nocedal - 2016 - Optimization Methods for Large-Scale Machine Learning:/usr/stud/brechet/Zotero/storage/8ZS9Q24I/Bottou, Curtis, Nocedal - 2016 - Optimization Methods for Large-Scale Machine Learning.pdf:application/pdf}
}

@article{Sommerfeld2018,
	title = {Inference for empirical Wasserstein distances on finite spaces},
	volume = {80},
	issn = {14679868},
	doi = {10.1111/rssb.12236},
	abstract = {The Wasserstein distance is an attractive tool for data analysis but statistical inference is hindered by the lack of distributional limits. To overcome this obstacle, for probability measures supported on finitely many points, we derive the asymptotic distribution of empirical Wasserstein distances as the optimal value of a linear program with random objective function. This facilitates statistical inference (e.g. confidence intervals for sample based Wasserstein distances) in large generality. Our proof is based on directional Hadamard differentiability. Failure of the classical bootstrap and alternatives are discussed. The utility of the distributional results is illustrated on two data sets.},
	pages = {219--238},
	number = {1},
	journaltitle = {Journal of the Royal Statistical Society. Series B: Statistical Methodology},
	author = {Sommerfeld, Max and Munk, Axel},
	date = {2018},
	eprinttype = {arxiv},
	eprint = {1610.03287},
	note = {bibtex*[arxivid=1610.03287] },
	keywords = {Wasserstein distance, Optimal transport, Bootstrap, Central limit theorem, Directional Hadamard derivative, Hypothesis testing}
}

@article{Gehring2016,
	title = {Convolutional Sequence to Sequence Learning},
	author = {Gehring, Jonas and Dauphin, Yann N},
	date = {2016},
	eprinttype = {arxiv},
	eprint = {arXiv:1705.03122v3},
	note = {bibtex*[arxivid={arXiv}:1705.03122v3] }
}

@article{Rockafellar1970,
	title = {Convex Analysis},
	issn = {0036-1445},
	url = {https://www.degruyter.com/view/books/9781400873173/9781400873173/9781400873173.xml},
	doi = {10.1515/9781400873173},
	abstract = {Available for the first time in paperback, R. Tyrrell Rockafellar's classic study presents readers with a coherent branch of nonlinear mathematical analysis that is especially suited to the study of optimization problems. Rockafellar's theory differs from classical analysis in that differentiability assumptions are replaced by convexity assumptions. The topics treated in this volume include: systems of inequalities, the minimum or maximum of a convex function over a convex set, Lagrange multipliers, minimax theorems and duality, as well as basic results about the structure of convex sets and the continuity and differentiability of convex functions and saddle- functions. This book has firmly established a new and vital area not only for pure mathematics but also for applications to economics and engineering. A sound knowledge of linear algebra and introductory real analysis should provide readers with sufficient background for this book. There is also a guide for the reader who may be using the book as an introduction, indicating which parts are essential and which may be skipped on a first reading.},
	author = {Rockafellar, Ralph Tyrell},
	date = {1970},
	note = {bibtex*[booktitle=Analysis;isbn=9781400873173]}
}

@article{Gotmare2018,
	title = {A Closer Look at Deep Learning Heuristics: Learning rate restarts, Warmup and Distillation},
	url = {https://arxiv.org/abs/1810.13243},
	author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
	date = {2018-10},
	eprinttype = {arxiv},
	eprint = {1810.13243},
	note = {bibtex*[arxivid=1810.13243] }
}

@article{Mescheder2017,
	title = {The Numerics of {GANs}},
	issn = {10495258},
	url = {http://arxiv.org/abs/1705.10461},
	abstract = {In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks ({GANs}). Using the formalism of smooth two-player games we analyze the associated gradient vector field of {GAN} training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common {GAN} architectures and show convergence on {GAN} architectures that are known to be notoriously hard to train.},
	issue = {Nips},
	author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
	date = {2017},
	eprinttype = {arxiv},
	eprint = {1705.10461},
	note = {bibtex*[arxivid=1705.10461] },
	file = {Full Text:/usr/stud/brechet/Zotero/storage/2ISNZ59G/Mescheder et al. - 2017 - The Numerics of GANs.pdf:application/pdf}
}

@book{Bertsimas1997a,
	title = {Introduction to Linear Optimization},
	volume = {30},
	isbn = {1-886529-19-1},
	abstract = {This book provides a unified, insightful, and modern treatment of linear optimization, that is, linear programming, network flow problems, and discrete optimization. It includes classical topics as well as the state of the art, in both theory and practice.},
	pagetotal = {15-21},
	author = {Bertsimas, Dimitris J. and Tsitsiklis, J.N. John N. J.N.},
	date = {1997},
	note = {bibtex*[booktitle=Introduction to Linear Optimization;pmid=3528304]}
}

@article{Salimans2018,
	title = {Improving {GANs} Using Optimal Transport},
	url = {http://arxiv.org/abs/1803.05573},
	abstract = {We present Optimal Transport {GAN} ({OT}-{GAN}), a variant of generative adversarial nets minimizing a new metric measuring the distance between the generator distribution and the data distribution. This metric, which we call mini-batch energy distance, combines optimal transport in primal form with an energy distance defined in an adversarially learned feature space, resulting in a highly discriminative distance function with unbiased mini-batch gradients. Experimentally we show {OT}-{GAN} to be highly stable when trained with large mini-batches, and we present state-of-the-art results on several popular benchmark problems for image generation.},
	journaltitle = {{arXiv}:1803.05573 [cs, stat]},
	author = {Salimans, Tim and Zhang, Han and Radford, Alec and Metaxas, Dimitris},
	urldate = {2019-03-21},
	date = {2018-03-14},
	eprinttype = {arxiv},
	eprint = {1803.05573},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1803.05573 PDF:/usr/stud/brechet/Zotero/storage/F75GS5TE/Salimans et al. - 2018 - Improving GANs Using Optimal Transport.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/SF9ZH5PB/1803.html:text/html}
}

@article{Salimans2016,
	title = {Improved Techniques for Training {GANs}},
	url = {http://arxiv.org/abs/1606.03498},
	abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks ({GANs}) framework. We focus on two applications of {GANs}: semi-supervised learning, and the generation of images that humans find visually realistic. Unlike most work on generative models, our primary goal is not to train a model that assigns high likelihood to test data, nor do we require the model to be able to learn well without using any labels. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on {MNIST}, {CIFAR}-10 and {SVHN}. The generated images are of high quality as confirmed by a visual Turing test: our model generates {MNIST} samples that humans cannot distinguish from real data, and {CIFAR}-10 samples that yield a human error rate of 21.3\%. We also present {ImageNet} samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of {ImageNet} classes.},
	journaltitle = {{arXiv}:1606.03498 [cs]},
	author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi},
	urldate = {2019-03-21},
	date = {2016-06-10},
	eprinttype = {arxiv},
	eprint = {1606.03498},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1606.03498 PDF:/usr/stud/brechet/Zotero/storage/A3JZM648/Salimans et al. - 2016 - Improved Techniques for Training GANs.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/DEWV8MVB/1606.html:text/html}
}

@article{Gulrajani2017,
	title = {Improved Training of Wasserstein {GANs}},
	url = {http://arxiv.org/abs/1704.00028},
	abstract = {Generative Adversarial Networks ({GANs}) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein {GAN} ({WGAN}) makes progress toward stable training of {GANs}, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in {WGAN} to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard {WGAN} and enables stable training of a wide variety of {GAN} architectures with almost no hyperparameter tuning, including 101-layer {ResNets} and language models over discrete data. We also achieve high quality generations on {CIFAR}-10 and {LSUN} bedrooms.},
	journaltitle = {{arXiv}:1704.00028 [cs, stat]},
	author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron},
	urldate = {2019-03-21},
	date = {2017-03-31},
	eprinttype = {arxiv},
	eprint = {1704.00028},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1704.00028 PDF:/usr/stud/brechet/Zotero/storage/XUFXPCWH/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/HI93EMPA/1704.html:text/html}
}

@article{Higgins2016,
	title = {beta-{VAE}: Learning Basic Visual Concepts with a Constrained Variational Framework},
	url = {https://openreview.net/forum?id=Sy2fzU9gl},
	shorttitle = {beta-{VAE}},
	abstract = {Learning an interpretable factorised representation of the independent data generative factors of the world without supervision is an important precursor for the development of artificial...},
	author = {Higgins, Irina and Matthey, Loic and Pal, Arka and Burgess, Christopher and Glorot, Xavier and Botvinick, Matthew and Mohamed, Shakir and Lerchner, Alexander},
	urldate = {2019-03-23},
	date = {2016-11-04},
	file = {Full Text PDF:/usr/stud/brechet/Zotero/storage/E6TI4BM5/Higgins et al. - 2016 - beta-VAE Learning Basic Visual Concepts with a Co.pdf:application/pdf;Snapshot:/usr/stud/brechet/Zotero/storage/GSMTT4ZK/forum.html:text/html}
}

@article{Karras2018,
	title = {A Style-Based Generator Architecture for Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1812.04948},
	abstract = {We propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-specific control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.},
	journaltitle = {{arXiv}:1812.04948 [cs, stat]},
	author = {Karras, Tero and Laine, Samuli and Aila, Timo},
	urldate = {2019-03-23},
	date = {2018-12-12},
	eprinttype = {arxiv},
	eprint = {1812.04948},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1812.04948 PDF:/usr/stud/brechet/Zotero/storage/PAYISA8S/Karras et al. - 2018 - A Style-Based Generator Architecture for Generativ.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/JNRUH8NN/1812.html:text/html}
}

@article{Ridgeway2016,
	title = {A Survey of Inductive Biases for Factorial Representation-Learning},
	url = {http://arxiv.org/abs/1612.05299},
	abstract = {With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning. This article surveys various constraints that encourage a learning algorithm to discover factorial representations. I dichotomize the constraints in terms of unsupervised and supervised inductive bias. Unsupervised inductive biases exploit assumptions about the environment, such as the statistical distribution of factor coefficients, assumptions about the perturbations a factor should be invariant to (e.g. a representation of an object can be invariant to rotation, translation or scaling), and assumptions about how factors are combined to synthesize an observation. Supervised inductive biases are constraints on the representations based on additional information connected to observations. Supervisory labels come in variety of types, which vary in how strongly they constrain the representation, how many factors are labeled, how many observations are labeled, and whether or not we know the associations between the constraints and the factors they are related to. This survey brings together a wide variety of models that all touch on the problem of learning factorial representations and lays out a framework for comparing these models based on the strengths of the underlying supervised and unsupervised inductive biases.},
	journaltitle = {{arXiv}:1612.05299 [cs]},
	author = {Ridgeway, Karl},
	urldate = {2019-03-23},
	date = {2016-12-15},
	eprinttype = {arxiv},
	eprint = {1612.05299},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1612.05299 PDF:/usr/stud/brechet/Zotero/storage/PC59NR2J/Ridgeway - 2016 - A Survey of Inductive Biases for Factorial Represe.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/3JJD59JD/1612.html:text/html}
}

@software{2019,
	title = {Dataset to assess the disentanglement properties of unsupervised learning methods: deepmind/dsprites-dataset},
	rights = {Apache-2.0},
	url = {https://github.com/deepmind/dsprites-dataset},
	shorttitle = {Dataset to assess the disentanglement properties of unsupervised learning methods},
	publisher = {{DeepMind}},
	urldate = {2019-03-23},
	date = {2019-03-20},
	note = {original-date: 2017-05-09T14:32:35Z}
}

@article{Houthooft2016,
	title = {{VIME}: Variational Information Maximizing Exploration},
	url = {http://arxiv.org/abs/1605.09674},
	shorttitle = {{VIME}},
	abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning ({RL}). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep {RL} scenarios. As such, most contemporary {RL} relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration ({VIME}), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. {VIME} modifies the {MDP} reward function, and can be applied with several different underlying {RL} algorithms. We demonstrate that {VIME} achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
	journaltitle = {{arXiv}:1605.09674 [cs, stat]},
	author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	urldate = {2019-03-23},
	date = {2016-05-31},
	eprinttype = {arxiv},
	eprint = {1605.09674},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv\:1605.09674 PDF:/usr/stud/brechet/Zotero/storage/DW8ZTSL2/Houthooft et al. - 2016 - VIME Variational Information Maximizing Explorati.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/ZPBR8Y44/1605.html:text/html}
}

@article{Arjovsky2017,
	title = {Wasserstein {GAN}},
	url = {http://arxiv.org/abs/1701.07875},
	abstract = {We introduce a new algorithm named {WGAN}, an alternative to traditional {GAN} training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions.},
	journaltitle = {{arXiv}:1701.07875 [cs, stat]},
	author = {Arjovsky, Martin and Chintala, Soumith and Bottou, Léon},
	urldate = {2019-03-25},
	date = {2017-01-26},
	eprinttype = {arxiv},
	eprint = {1701.07875},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1701.07875 PDF:/usr/stud/brechet/Zotero/storage/3EJBR6TS/Arjovsky et al. - 2017 - Wasserstein GAN.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/EGA28UQ7/1701.html:text/html}
}

@article{Kingma2014,
	title = {Adam: A Method for Stochastic Optimization},
	url = {http://arxiv.org/abs/1412.6980},
	shorttitle = {Adam},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss {AdaMax}, a variant of Adam based on the infinity norm.},
	journaltitle = {{arXiv}:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	urldate = {2019-03-27},
	date = {2014-12-22},
	eprinttype = {arxiv},
	eprint = {1412.6980},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1412.6980 PDF:/usr/stud/brechet/Zotero/storage/LJWIXXKQ/Kingma and Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/B9WVYY8L/1412.html:text/html}
}

@article{Balaji2018,
	title = {Entropic {GANs} meet {VAEs}: A Statistical Approach to Compute Sample Likelihoods in {GANs}},
	url = {https://openreview.net/forum?id=BygMAiRqK7},
	shorttitle = {Entropic {GANs} meet {VAEs}},
	abstract = {Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks ({GANs}) and Variational {AutoEncoders} ({VAEs})....},
	author = {Balaji, Yogesh and Hasani, Hamed and Chellappa, Rama and Feizi, Soheil},
	urldate = {2019-03-27},
	date = {2018-09-27},
	file = {Full Text PDF:/usr/stud/brechet/Zotero/storage/G4STG9BB/Balaji et al. - 2018 - Entropic GANs meet VAEs A Statistical Approach to.pdf:application/pdf;Snapshot:/usr/stud/brechet/Zotero/storage/C2JGQKP3/forum.html:text/html}
}

@book{Gelles2017,
	location = {Boston Delft},
	title = {Coding for interactive communication: a survey},
	isbn = {978-0-521-83378-3 978-1-68083-346-1},
	series = {Foundations and trends in theoretical computer science},
	shorttitle = {Coding for interactive communication},
	pagetotal = {168},
	number = {13:1-2},
	publisher = {now Publishers},
	author = {Gelles, Ran},
	date = {2017},
	langid = {english},
	note = {{OCLC}: on1035831877},
	file = {Gelles - 2017 - Coding for interactive communication a survey.pdf:/usr/stud/brechet/Zotero/storage/DBMRLPYB/Gelles - 2017 - Coding for interactive communication a survey.pdf:application/pdf}
}

@book{Gelles2017a,
	location = {Boston Delft},
	title = {Coding for interactive communication: a survey},
	isbn = {978-0-521-83378-3 978-1-68083-346-1},
	series = {Foundations and trends in theoretical computer science},
	shorttitle = {Coding for interactive communication},
	pagetotal = {168},
	number = {13:1-2},
	publisher = {now Publishers},
	author = {Gelles, Ran},
	date = {2017},
	langid = {english},
	note = {{OCLC}: on1035831877},
	file = {Gelles - 2017 - Coding for interactive communication a survey.pdf:/usr/stud/brechet/Zotero/storage/B2PDYB5L/Gelles - 2017 - Coding for interactive communication a survey.pdf:application/pdf}
}

@book{Boyd2010,
	location = {Cambridge [u.a.]},
	edition = {8. print.},
	title = {Convex optimization},
	isbn = {978-0-521-83378-3},
	pagetotal = {{XIII}, 716 S. :},
	publisher = {Cambridge Univ. Press},
	author = {Boyd, Stephen P.},
	date = {2010},
	file = {TUM OPAC:/usr/stud/brechet/Zotero/storage/4T5KU53A/singleHit.html:text/html}
}

@article{Burgess2018,
	title = {Understanding disentangling in \${\textbackslash}beta\$-{VAE}},
	url = {http://arxiv.org/abs/1804.03599},
	abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified {ELBO} bound in \${\textbackslash}beta\$-{VAE}, as training progresses. From these insights, we propose a modification to the training regime of \${\textbackslash}beta\$-{VAE}, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in \${\textbackslash}beta\$-{VAE}, without the previous trade-off in reconstruction accuracy.},
	journaltitle = {{arXiv}:1804.03599 [cs, stat]},
	author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
	urldate = {2019-04-05},
	date = {2018-04-10},
	eprinttype = {arxiv},
	eprint = {1804.03599},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence},
	file = {arXiv\:1804.03599 PDF:/usr/stud/brechet/Zotero/storage/H5LUGGVM/Burgess et al. - 2018 - Understanding disentangling in \$beta\$-VAE.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/YLLPC42G/1804.html:text/html}
}

@article{Esmaeili2018,
	title = {Structured Disentangled Representations},
	url = {http://arxiv.org/abs/1804.02086},
	abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, {KL} divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
	journaltitle = {{arXiv}:1804.02086 [cs, stat]},
	author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and van de Meent, Jan-Willem},
	urldate = {2019-04-05},
	date = {2018-04-05},
	eprinttype = {arxiv},
	eprint = {1804.02086},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1804.02086 PDF:/usr/stud/brechet/Zotero/storage/MIN84HFJ/Esmaeili et al. - 2018 - Structured Disentangled Representations.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/JB4WL7ZH/1804.html:text/html}
}

@article{Hong2019,
	title = {How Generative Adversarial Networks and Their Variants Work: An Overview},
	volume = {52},
	issn = {03600300},
	url = {http://arxiv.org/abs/1711.05914},
	doi = {10.1145/3301282},
	shorttitle = {How Generative Adversarial Networks and Their Variants Work},
	abstract = {Generative Adversarial Networks ({GAN}) have received wide attention in the machine learning field for their potential to learn high-dimensional, complex real data distribution. Specifically, they do not rely on any assumptions about the distribution and can generate real-like samples from latent space in a simple manner. This powerful property leads {GAN} to be applied to various applications such as image synthesis, image attribute editing, image translation, domain adaptation and other academic fields. In this paper, we aim to discuss the details of {GAN} for those readers who are familiar with, but do not comprehend {GAN} deeply or who wish to view {GAN} from various perspectives. In addition, we explain how {GAN} operates and the fundamental meaning of various objective functions that have been suggested recently. We then focus on how the {GAN} can be combined with an autoencoder framework. Finally, we enumerate the {GAN} variants that are applied to various tasks and other fields for those who are interested in exploiting {GAN} for their research.},
	pages = {1--43},
	number = {1},
	journaltitle = {{ACM} Computing Surveys},
	author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
	urldate = {2019-04-08},
	date = {2019-02-13},
	eprinttype = {arxiv},
	eprint = {1711.05914},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1711.05914 PDF:/usr/stud/brechet/Zotero/storage/T3FBC695/Hong et al. - 2019 - How Generative Adversarial Networks and Their Vari.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/UE7WE8I4/1711.html:text/html}
}

@book{Billingsley1995,
	location = {New York [u.a.]},
	edition = {3. ed.},
	title = {Probability and measure},
	isbn = {978-0-471-00710-4},
	series = {Wiley series in probability and mathematical statistics},
	pagetotal = {{XII}, 593 S. :},
	publisher = {Wiley},
	author = {Billingsley, Patrick},
	date = {1995},
	file = {TUM OPAC:/usr/stud/brechet/Zotero/storage/HFWPJB6G/singleHit.html:text/html}
}

@book{Hansen2009,
	location = {Copenhagen},
	edition = {Fourth edition},
	title = {Measure theory},
	isbn = {978-87-91927-44-7},
	pagetotal = {xiv, 600 Seiten :},
	publisher = {Department of Mathematical Sciences, University of Copenhagen},
	author = {Hansen, Ernst},
	date = {2009},
	file = {TUM OPAC:/usr/stud/brechet/Zotero/storage/ABCMQPGM/singleHit.html:text/html}
}

@article{Peyre2018,
	title = {Computational Optimal Transport},
	url = {http://arxiv.org/abs/1803.00567},
	abstract = {Optimal transport ({OT}) theory can be informally described using the words of the French mathematician Gaspard Monge (1746-1818): A worker with a shovel in hand has to move a large pile of sand lying on a construction site. The goal of the worker is to erect with all that sand a target pile with a prescribed shape (for example, that of a giant sand castle). Naturally, the worker wishes to minimize her total effort, quantified for instance as the total distance or time spent carrying shovelfuls of sand. Mathematicians interested in {OT} cast that problem as that of comparing two probability distributions, two different piles of sand of the same volume. They consider all of the many possible ways to morph, transport or reshape the first pile into the second, and associate a "global" cost to every such transport, using the "local" consideration of how much it costs to move a grain of sand from one place to another. Recent years have witnessed the spread of {OT} in several fields, thanks to the emergence of approximate solvers that can scale to sizes and dimensions that are relevant to data sciences. Thanks to this newfound scalability, {OT} is being increasingly used to unlock various problems in imaging sciences (such as color or texture processing), computer vision and graphics (for shape manipulation) or machine learning (for regression, classification and density fitting). This short book reviews {OT} with a bias toward numerical methods and their applications in data sciences, and sheds lights on the theoretical properties of {OT} that make it particularly useful for some of these applications.},
	journaltitle = {{arXiv}:1803.00567 [stat]},
	author = {Peyré, Gabriel and Cuturi, Marco},
	urldate = {2019-04-11},
	date = {2018-03-01},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {1803.00567},
	keywords = {Statistics - Machine Learning},
	file = {Peyré and Cuturi - 2018 - Computational Optimal Transport.pdf:/usr/stud/brechet/Zotero/storage/D2ARJUIW/Peyré and Cuturi - 2018 - Computational Optimal Transport.pdf:application/pdf}
}

@article{Balaji2018a,
	title = {Entropic {GANs} meet {VAEs}: A Statistical Approach to Compute Sample Likelihoods in {GANs}},
	url = {http://arxiv.org/abs/1810.04147},
	shorttitle = {Entropic {GANs} meet {VAEs}},
	abstract = {Building on the success of deep learning, two modern approaches to learn a probability model of the observed data are Generative Adversarial Networks ({GANs}) and Variational {AutoEncoders} ({VAEs}). {VAEs} consider an explicit probability model for the data and compute a generative distribution by maximizing a variational lower-bound on the log-likelihood function. {GANs}, however, compute a generative model by minimizing a distance between observed and generated probability distributions without considering an explicit model for the observed data. The lack of having explicit probability models in {GANs} prohibits computation of sample likelihoods in their frameworks and limits their use in statistical inference problems. In this work, we show that an optimal transport {GAN} with the entropy regularization can be viewed as a generative model that maximizes a lower-bound on average sample likelihoods, an approach that {VAEs} are based on. In particular, our proof constructs an explicit probability model for {GANs} that can be used to compute likelihood statistics within {GAN}'s framework. Our numerical results on several datasets demonstrate consistent trends with the proposed theory.},
	journaltitle = {{arXiv}:1810.04147 [cs, stat]},
	author = {Balaji, Yogesh and Hassani, Hamed and Chellappa, Rama and Feizi, Soheil},
	urldate = {2019-04-13},
	date = {2018-10-09},
	eprinttype = {arxiv},
	eprint = {1810.04147},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1810.04147 PDF:/usr/stud/brechet/Zotero/storage/HF242KW3/Balaji et al. - 2018 - Entropic GANs meet VAEs A Statistical Approach to.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/PKPNVDL4/1810.html:text/html}
}

@book{Villani2009,
	location = {Berlin [u.a.]},
	title = {Optimal transport},
	isbn = {978-3-540-71049-3},
	series = {Grundlehren der mathematischen Wissenschaften},
	pagetotal = {{XXII}, 973 S. :},
	publisher = {Springer},
	author = {Villani, Cédric},
	date = {2009},
	file = {TUM OPAC:/usr/stud/brechet/Zotero/storage/ESVBLR5V/singleHit.html:text/html}
}

@online{zotero-275,
	title = {{MNIST} handwritten digit database, Yann {LeCun}, Corinna Cortes and Chris Burges},
	url = {http://yann.lecun.com/exdb/mnist/},
	urldate = {2019-04-14},
	file = {MNIST handwritten digit database, Yann LeCun, Corinna Cortes and Chris Burges:/usr/stud/brechet/Zotero/storage/4C4P5Z72/mnist.html:text/html}
}

@article{Kingma2013,
	title = {Auto-Encoding Variational Bayes},
	url = {http://arxiv.org/abs/1312.6114},
	abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
	journaltitle = {{arXiv}:1312.6114 [cs, stat]},
	author = {Kingma, Diederik P. and Welling, Max},
	urldate = {2019-04-14},
	date = {2013-12-20},
	eprinttype = {arxiv},
	eprint = {1312.6114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv\:1312.6114 PDF:/usr/stud/brechet/Zotero/storage/ETHUBA6Z/Kingma and Welling - 2013 - Auto-Encoding Variational Bayes.pdf:application/pdf;arXiv.org Snapshot:/usr/stud/brechet/Zotero/storage/UILUFME6/1312.html:text/html}
}

@article{Radford2015,
	title = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
	url = {http://arxiv.org/abs/1511.06434},
	abstract = {In recent years, supervised learning with convolutional networks ({CNNs}) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with {CNNs} has received less attention. In this work we hope to help bridge the gap between the success of {CNNs} for supervised learning and unsupervised learning. We introduce a class of {CNNs} called deep convolutional generative adversarial networks ({DCGANs}), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
	journaltitle = {{arXiv}:1511.06434 [cs]},
	author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
	urldate = {2019-04-14},
	date = {2015-11-19},
	eprinttype = {arxiv},
	eprint = {1511.06434},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1511.06434 PDF:/usr/stud/brechet/Zotero/storage/MN4DJZDB/Radford et al. - 2015 - Unsupervised Representation Learning with Deep Con.pdf:application/pdf}
}
