\documentclass[11pt,a4paper]{article}

% Working date: The date this entry is describing. Not necessary the date of last edit.
\newcommand{\workingDate}{\textsc{2019 $|$ April $|$ 23}}

% Name and institution must preceed call to researchdiary.sty package
\newcommand{\userName}{Pierre BrÃ©chet}
\newcommand{\institution}{TUM}

% The guts and glory
\usepackage{research_diary}
\usepackage{usrcmd}
\bibliography{bibliography}

% Begin document.
% Use \logoPNG or \logoEPS. If compiling with PDFTeX, use \logoPNG
\begin{document}

{\Huge April 30}

\section{Scaling the distance}

Say the distance $\ct$ is scaled by a factor $k>0$ to produce $c(x, y) = k\cdot \ct(x,y)$.

Then, admitting the model learns $u(x) = k\cdot \ut(x),\, v(y) = k \cdot
\vt(y)$, the term $\frac{1}{\eps}(u(x) + v(Y) - c(x, y))$ is in fact
$\frac{k}{\eps}(\ut(x) + \vt(y) - \ct(x,y)) =
\frac{1}{\tilde{\eps}}(\ut(x) + \vt(y) - \ct(x,y))$. In other terms, scaling
the distance by $k$ has the effect to learn with an effective regularization
weight $\eps = \frac{\tilde{\eps}}{k}$,

\todo{find a way to normalize the distance}

\section{Implementation testing}
    Still testing the previous implementation of the anti-informative regularization, there wasn't batch normalization in the last layer of the W network on 02. 05.

    Different tests are made to figure out if the solving routine is still operational when no sinkhorn is used before understanding sinkhorn

    Results on branch master were using a \texttt{grad_mode} set to \texttt{FE}, meaning the samples were used in the computation of the weights for the modules \texttt{FE}, \texttt{shared}, \texttt{spec}, whereas the implementation from 02.05 uses only the spec layers for the \texttt{W} network ?

    Should the shared network be frozen (i.e. no grad enabled) on the samples coming from the generated distribution ? tests are in progress

\section{Grad (mode) propagation}
    The mode of the gradient dictates how the gradients are computed. If the back-propagation is not active, the inputs won't contribute to the parameters gradient.

    Since the anti-informative network \texttt{W} performs on real samples, one idea would be to leave its gradient propagation set to the shared layers of parameters \texttt{FE}, and leave the gradient mode of only \texttt{U} to \texttt{spec}.
    Results can be found in '/usr/stud/brechet/computational-ot/fig/thesis-b/0205_reg-w-infogan/grad-mode-spec'

\section{Use of batch normalization in the specialized layers}
    tests are in progress to check whether batch normalization is useful and necessary in the specialized layers of the dual networks

\end{document}
